# Settings Listing

## Overview

Chatbook settings control LLM behavior, prompt construction, tool usage, formatting, and UI behavior. Settings are stored in notebook tagging rules under `"ChatNotebookSettings"` and follow a hierarchical inheritance model.

### Accessing Settings

Use `CurrentChatSettings` to read and write settings:

```wl
(* Read global settings *)
CurrentChatSettings[]
CurrentChatSettings["Temperature"]

(* Read settings scoped to a notebook or cell *)
CurrentChatSettings[notebookObj]
CurrentChatSettings[cellObj, "Model"]

(* Write settings *)
CurrentChatSettings[$FrontEnd, "Temperature"] = 0.5
CurrentChatSettings[notebookObj, "AutoFormat"] = False

(* Reset to inherited value *)
CurrentChatSettings[notebookObj, "Temperature"] =.
```

### Inheritance Model

Settings resolve through a hierarchy, with more specific scopes overriding broader ones:

| Scope              | Description                      |
| ------------------ | -------------------------------- |
| `CellObject`       | Per-cell override                |
| `NotebookObject`   | Per-notebook settings            |
| `$FrontEndSession` | Session-wide (non-persistent)    |
| `$FrontEnd`        | Global persistent settings       |

If a setting is not defined at a given scope, it inherits from the next broader scope. A value of `Inherited` explicitly defers to the parent scope.

### Automatic Values

Many settings default to `Automatic`, meaning they are resolved at runtime based on the current model, service, and other settings. The resolution pipeline is defined in `Settings.wl` via `resolveAutoSettings`, which evaluates `Automatic` values in topologically sorted dependency order. Model-specific defaults are looked up from `$modelAutoSettings`.

---

## Model & Service

| Setting | Default | Description |
| ------- | ------- | ----------- |
| `"Model"` | `$DefaultModel` | The LLM model specification. The default is defined via `RuleDelayed` (`:>`) in `$defaultChatSettings` (`Settings.wl`), so `$DefaultModel` is evaluated lazily each time the setting is accessed. `$DefaultModel` (`Settings.wl`) returns `<\|"Service" -> "LLMKit", "Name" -> Automatic\|>` for Wolfram Engine 14.1+ and `<\|"Service" -> "OpenAI", "Name" -> "gpt-4o"\|>` for older versions. The value can be: (1) an `Association` with `"Service"` and `"Name"` keys; (2) a plain string (interpreted as an OpenAI model name by `serviceName` in `Models.wl`, and converted to `{"OpenAI", model}` by `makeLLMConfiguration` in `SendChat.wl`); (3) a `{service, name}` list (converted to an Association by `resolveFullModelSpec`). During `resolveAutoSettings` (`Settings.wl`), the model is resolved via `resolveFullModelSpec` (`Models.wl`), which: returns already-resolved models unchanged (checked via `"ResolvedModel" -> True`); converts `{service, name}` lists and plain strings to Associations; for LLMKit service with unspecified name, substitutes the actual backing service and model from `$defaultLLMKitService`/`$defaultLLMKitModelName` and sets `"Authentication" -> "LLMKit"`; for other services with `"Name" -> Automatic`, calls `chooseDefaultModelName` (`Models.wl`), which tries in order: the `$DefaultModel` name if the service matches, the service's registered `"DefaultModel"` property, the first model from the cached model list, or `Automatic` as fallback — if no string name can be resolved, it queries `getServiceModelList` and throws `$Canceled` if not connected. The resolved model is then passed through `standardizeModelData` (`Models.wl`), which enriches the Association with computed metadata: `"BaseID"`, `"BaseName"`, `"Family"` (from `modelNameData`), `"Date"`, `"DisplayName"`, `"FineTuned"`, `"Icon"`, `"Multimodal"`, `"Name"` (normalized via `toModelName`), `"Snapshot"`, and `"ResolvedModel" -> True`. The `toModelName` function normalizes model name strings (e.g., CamelCase to lowercase with hyphens, "ChatGPT" to "gpt-3.5-turbo"). Model is resolved first in the topological sort order (`$autoSettingKeyPriority` explicitly prepends `"Model"`). Many settings depend on the resolved Model via `$autoSettingKeyDependencies`: `"Authentication"`, `"ForceSynchronous"`, `"HybridToolMethod"`, `"MaxCellStringLength"`, `"MaxContextTokens"`, `"MaxTokens"`, `"Multimodal"`, `"TokenizerName"`, `"ToolCallExamplePromptStyle"`, `"ToolCallRetryMessage"`, `"ToolExamplePrompt"`, and `"ToolsEnabled"`. The service name is extracted from the model via `serviceName` (`Models.wl`), which checks for a `"Service"` key in the model Association, with `"OpenAI"` as the default for plain strings. In `$llmConfigPassedKeys` (`SendChat.wl`), so it is passed through `LLMConfiguration` to the LLM service. Listed in `$popOutSettings` (`ConvertChatNotebook.wl`) as one of four settings shown during chat notebook conversion. Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. The `$modelAutoSettings` table (`Settings.wl`) does not contain overrides for the Model setting itself; rather, the resolved model's service and name/family are used as lookup keys to resolve other settings. Exposed in the preferences UI (`PreferencesContent.wl`) in both the "Notebooks" tab and the "Services" tab via `makeModelSelector`, which provides a service selector popup menu and a model name selector popup menu (or text input). The UI reads and writes `CurrentChatSettings[$preferencesScope, "Model"]` as an Association. The `"ServiceDefaultModel"` setting is used alongside to remember the last-selected model per service across service switches. Also accessible via `SetModel` (`Models.wl`), which accepts a string model name or Association and writes to the notebook's `TaggingRules`, optionally updating `System`$LLMEvaluator` via `LLMConfiguration`. |
| `"Authentication"` | `Automatic` | Authentication method for the LLM service. When `Automatic`, resolves based on the model specification: if the model has an explicit `"Authentication"` field, that value is used; if the model's `"Service"` is `"LLMKit"`, resolves to `"LLMKit"`; otherwise remains `Automatic` (uses the service's default authentication). Depends on `"Model"`. Passed directly to `LLMServices`Chat` and `LLMServices`ChatSubmit` (not via `LLMConfiguration`). |
| `"EnableLLMServices"` | `Automatic` | Whether Chatbook uses the `LLMServices` framework for LLM communication. When `Automatic`, resolves to the internal `$useLLMServices` flag, which evaluates to `True` only if `$enableLLMServices` is `Automatic` or `True` AND the `Wolfram/LLMFunctions` paclet (version 1.2.2+) is installed (`Services.wl`). When `True`, Chatbook routes all chat requests through `LLMServices`Chat`/`LLMServices`ChatSubmit`, the OpenAI completion URL input is hidden from the preferences UI (`PreferencesContent.wl`), and available services are discovered dynamically. When `False`, Chatbook falls back to direct API calls using legacy service configuration with `$fallBackServices`, and the OpenAI completion URL input is shown in the preferences UI. The setting value is read from `CurrentChatSettings` and assigned to the `$enableLLMServices` variable in `Actions.wl` before each `sendChat` call. The `sendChat` function in `SendChat.wl` has a condition `/; $useLLMServices` that gates the primary chat execution path. Other settings depend on this: `HandlerFunctionsKeys` depends on `EnableLLMServices` for resolution order, and `Multimodal` depends on both `EnableLLMServices` and `Model` (when LLM Services are disabled but the model supports multimodal, multimodal is enabled directly; when enabled, it additionally checks for multimodal paclet availability). This is a non-inherited persona value (listed in `$nonInheritedPersonaValues` in `Settings.wl`), meaning it retains its value from the notebook/cell scope rather than inheriting from the persona. No model-specific overrides exist. Not exposed directly in the preferences UI. |
| `"Multimodal"` | `Automatic` | Whether multimodal (image) input is supported, controlling whether graphics and images in notebook cells are encoded and included in messages sent to the LLM. When `Automatic`, resolved by `multimodalQ` (`Settings.wl`), which evaluates three factors: (1) whether the model supports multimodal input (via `multimodalModelQ` in `Models.wl`), (2) whether LLMServices is enabled (`EnableLLMServices`), and (3) whether required paclets are available. The resolution logic: if the model does not support multimodal, returns `False`; if the model supports multimodal and `EnableLLMServices` is `False`, returns `True` (direct API path needs no extra paclets); if the model supports multimodal and `EnableLLMServices` is `True`, returns `multimodalPacletsAvailable[]`, which checks that `Wolfram/LLMFunctions` version 1.2.4+ and `ServiceConnection_OpenAI` version 13.3.18+ (with multimodal support) are installed. The `multimodalModelQ` function (`Models.wl`) determines model capability by: checking for an explicit `"Multimodal"` key in the resolved model Association (set during `standardizeModelData`, which adds `"Multimodal" -> multimodalModelQ @ model` to every resolved model); matching known model name patterns (Claude 3+, GPT-4o/GPT-4o-mini/ChatGPT-4o, GPT-4-turbo with date suffix); or detecting "vision" in the normalized model name. Model-specific overrides in `$modelAutoSettings`: `True` for Claude 4, Gemini 2, Gemini 3, GPT-4.1, GPT-5, O1, O3, and O4-Mini; `False` for O1-Mini and O3-Mini. Depends on `"EnableLLMServices"` and `"Model"` (declared in `$autoSettingKeyDependencies`). The resolved value is stored in the `$multimodalMessages` global variable (`CommonSymbols.wl`) at three points in `SendChat.wl` (lines 90, 176, 298) and in `makeChatMessages` (`ChatMessages.wl`), and is preserved across handler evaluation via `ChatState.wl`. When `True`, `makeMessageContent` (`ChatMessages.wl`) processes cell content through `expandMultimodalString`, which splits strings on expression URI patterns, calls `inferMultimodalTypes` to classify content as `"Text"` or `"Image"`, and produces multimodal message content (with image data). Image inclusion is further gated by `allowedMultimodalRoles` (`ChatMessages.wl`), which restricts multimodal content to `"User"` role messages for GPT-4o models and allows `All` roles for other models. Images are resized via `resizeMultimodalImage` (`ChatMessages.wl`) to fit within `$maxMMImageSize` dimensions before encoding. In `Serialization.wl`, the related `$multimodalImages` variable (derived from `$contentTypes`) controls whether graphics boxes are encoded as image URIs (`toMarkdownImageBox`) or replaced with `"[GRAPHIC]"` placeholders, and whether `"Picture"` style cells are serialized as image URIs. Graphics exceeding `$maxBoxSizeForImages` bytes fall back to non-multimodal serialization. Chat modes override multimodal behavior: `ContentSuggestions` mode sets its own `$wlSuggestionsMultimodal`, `$textSuggestionsMultimodal`, and `$notebookSuggestionsMultimodal` flags (all `False`); `ChatTitle.wl` uses `$multimodalTitleContext = False` for title generation. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook for message content processing). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Exposed in the preferences UI (`PreferencesContent.wl`) under the "Features" section as a PopupMenu with three options: `Automatic` ("Enabled by Model"), `True` ("Enabled Always"), and `False` ("Enabled Never"), reading and writing `CurrentChatSettings[$preferencesScope, "Multimodal"]`. |
| `"Reasoning"` | `Automatic` | Whether model reasoning/chain-of-thought is enabled. Model-specific; only supported by certain models (e.g., O-series, GPT-5). Models that don't support it return `Missing["NotSupported"]`. |

## LLM Parameters

| Setting | Default | Description |
| ------- | ------- | ----------- |
| `"Temperature"` | `Automatic` | Sampling temperature for the LLM. Higher values increase randomness. Model default: `0.7`. Some models (e.g., O4-Mini, GPT-5) return `Missing["NotSupported"]`. |
| `"FrequencyPenalty"` | `0.1` | Penalty applied to tokens based on their frequency in the text so far. Reduces repetition. Default is a fixed numeric value (`0.1`), not `Automatic`. Only used in the legacy (non-LLMServices) HTTP request path: in `makeHTTPRequest` (`SendChat.wl`), the value is looked up from settings via `Lookup[settings, "FrequencyPenalty", 0.1]` and passed to the OpenAI API as the `"frequency_penalty"` field in the request body. Values of `Automatic` or `Missing` are stripped from the request via `DeleteCases`. Not included in `$llmConfigPassedKeys` (`SendChat.wl`), so it is NOT passed through `LLMConfiguration` when using the LLMServices framework (`LLMServices`Chat`/`LLMServices`ChatSubmit`). No model-specific overrides exist in `$modelAutoSettings`. No dependencies on other settings. Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not exposed in the preferences UI. |
| `"PresencePenalty"` | `Automatic` | Penalty applied to tokens based on whether they have appeared in the text so far. Model default: `0.1`. Some models (e.g., Google Gemini, O4-Mini, GPT-5) return `Missing["NotSupported"]`. |
| `"TopP"` | `1` | Top-p (nucleus) sampling parameter. A value of `1` considers all tokens. |
| `"MaxTokens"` | `Automatic` | Maximum number of tokens the LLM may generate in its response (output token limit). When `Automatic`, resolved by `autoMaxTokens` (`Settings.wl`), which looks up the model name in `$maxTokensTable`. Only two legacy models have explicit entries: `"gpt-4-vision-preview"` and `"gpt-4-1106-preview"` (both `4096`); all other models resolve to `Automatic`, deferring to the LLM service's own default output limit. No model-specific overrides exist in `$modelAutoSettings`. Depends on `"Model"` (declared in `$autoSettingKeyDependencies`). In the LLMServices path (the primary/modern code path), the resolved value is passed through `LLMConfiguration` via `$llmConfigPassedKeys` (`SendChat.wl`); when `Automatic`, the underlying `LLMConfiguration`/service determines the appropriate limit. In the legacy HTTP request path (`makeHTTPRequest` in `SendChat.wl`), the value is placed in the JSON request body as `"max_tokens"`; if `Automatic`, it is stripped by `DeleteCases[..., Automatic\|_Missing]`, allowing the API's own default to apply. This setting controls the **output** token limit and is distinct from `"MaxContextTokens"`, which controls the **input** context window size for token budgeting. Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. No persona-level overrides exist in any built-in persona. Not exposed in the preferences UI. |
| `"MaxContextTokens"` | `Automatic` | Maximum token capacity of the context window, used internally for token budgeting and context management. When `Automatic`, resolved by `autoMaxContextTokens` (`Settings.wl`), which dispatches based on the model's service and name. For Ollama services, the context length is queried dynamically via `ServiceExecute[service, "ModelContextLength", ...]` and cached; for LLMKit services, the value is capped at `2^16` (65,536). For other services, resolution first checks `$modelAutoSettings` for an exact model match, then falls back to `autoMaxContextTokens0`, which pattern-matches against tokenized model name components (e.g., `{"claude", "3", ...}` -> `200000`, `{"gpt", "4o", ...}` -> `131072`, `{"gemini", ..., "pro", ...}` -> `30720`). The ultimate fallback for unrecognized models is `2^12` (4,096). Model-specific values in `$modelAutoSettings`: `200000` for Claude 3/4, `128000` for GPT-4o, `1047576` for Gemini 2+/GPT-4.1, `400000` for GPT-5, `64000` for O1-Mini (halved for reasoning token headroom), `100000` for O1/O3/O3-Mini/O4-Mini. Depends on `"Authentication"` and `"Model"` (declared in `$autoSettingKeyDependencies`). The resolved value is used to initialize the token budget in `makeTokenBudget` (`ChatMessages.wl`), which multiplies it by `TokenBudgetMultiplier` to produce the working budget. During message construction, token pressure is tracked as `1.0 - ($tokenBudget / MaxContextTokens)`, and the cell string budget (`$cellStringBudget`) is dynamically reduced as pressure increases, dropping to `0` when fewer than `$reservedTokens` (500) remain. The `MaxCellStringLength` setting depends on `MaxContextTokens`: `chooseMaxCellStringLength` (`Settings.wl`) scales the character limit proportionally to the context window via `Min[Ceiling[$defaultMaxCellStringLength * tokens / 2^14], 2^14]`. `MaxOutputCellStringLength` in turn depends on the resolved `MaxCellStringLength`. Chat modes override this value: Context mode scales it to 25% via `$notebookContextLimitScale` (`ChatModes/Context.wl`); NotebookAssistance mode fixes it at `2^15` (32,768); ContentSuggestions mode selects per content type (`2^12` for WL/Text, `2^15` for Notebook). Also used by `modelContextStringLimit` (`LLMUtilities.wl`), which converts to a character limit via `tokens * 2` (with fallback to 8,000). Listed in `$popOutSettings` (`ConvertChatNotebook.wl`) as one of four settings shown during chat notebook conversion. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook for context management). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not exposed in the preferences UI. |
| `"StopTokens"` | `Automatic` | Stop sequences that signal the LLM to stop generating. Some models return `Missing["NotSupported"]`. The resolved stop tokens include the `"EndToken"` value when applicable. |
| `"TokenBudgetMultiplier"` | `Automatic` | Multiplier for the token budget calculation. Default: `1`. |

## Chat Behavior

| Setting | Default | Description |
| ------- | ------- | ----------- |
| `"IncludeHistory"` | `Automatic` | Whether to include chat history (preceding cells) in the context sent to the LLM. When `Automatic`, resolves to `Automatic` (no further resolution via `resolveAutoSetting0`). In practice, `Automatic` behaves the same as `True`: the `If[ ! settings["IncludeHistory"], cells = { evalCell } ]` check in `sendChat` (`SendChat.wl`) does not trigger because `! Automatic` evaluates to `Not[Automatic]` (not `True`), so the full cell list is passed to `constructMessages`. When explicitly `True`, the same behavior applies. When `False`, only the evaluation cell itself is included — no preceding chat history. The `selectChatCells` function (`SendChat.wl`) first selects candidate cells (up to `ChatHistoryLength` cells, bounded by chat delimiter cells), and then the `IncludeHistory` check decides whether to keep those cells or replace them with just the current cell. In `ChatHistory.wl`, `selectChatHistoryCells` dispatches on the setting value: `False` returns only the current cell; any other value (including `Automatic`) applies the `ChatHistoryLength` limit to the full cell list. Cell style overrides: cells with the `"SideChat"` style automatically set `IncludeHistory` to `False` via `addCellStyleSettings` in `Actions.wl`. Exposed in the notebook preferences UI as an "Include chat history" checkbox under the "Chat Notebook Cells" section (`PreferencesContent.wl`), where both `True` and `Automatic` display as checked. No dependencies in `$autoSettingKeyDependencies`. No model-specific overrides exist in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed to the LLM service). Not in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. |
| `"ChatHistoryLength"` | `1000` | Maximum number of chat cells to include in the context. When sending a chat message, the system selects cells starting from the current cell and looking backwards, limited to this count. Used in two paths: `selectChatCells` in `SendChat.wl` (which sets `$maxChatCells` from this value and applies `Take[..., UpTo @ $maxChatCells]` on the filtered cell list) and `selectChatHistoryCells` in `ChatHistory.wl` (which applies the same kind of limit on cell information entries). When the value is not a positive integer, falls back to the default `$maxChatCells`. Exposed in the notebook preferences UI as a numeric input field under the "Notebooks" section. Note: this is a cell count limit, not a token limit; token budgeting is handled separately by `"MaxContextTokens"` and `"TokenBudgetMultiplier"`. |
| `"MergeMessages"` | `True` | Whether to merge consecutive messages with the same role into a single message. When `True`, the `mergeMessageData` function in `ChatMessages.wl` groups consecutive non-system messages by role (via `SplitBy[messages, Lookup["Role"]]`) and concatenates their text content into a single message per group. System messages at the start of the message list are always kept separate (not merged). The merge process also applies `mergeCodeBlocks`, which combines adjacent code blocks of the same language (e.g., two consecutive `` ```wl `` blocks) into a single code block, preventing fragmentation when multiple cells are combined. The check uses `TrueQ`, so only an explicit `True` triggers merging; `False`, `Automatic`, or any other value leaves messages unmerged. Merging is applied in `augmentChatMessages` (`ChatMessages.wl`) after message construction but before prompt generator augmentation. No dependencies in `$autoSettingKeyDependencies`. No model-specific overrides exist in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook for message preprocessing). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Exposed in the notebook preferences UI as a "Merge chat messages" checkbox under the "Notebooks" section (`PreferencesContent.wl`), with tooltip: "If enabled, adjacent cells with the same author will be merged into a single chat message." Also exposed via the ChatPreferences tool as a boolean parameter. |
| `"MaxCellStringLength"` | `Automatic` | Maximum string length for cell content included in the LLM context. When `Automatic`, resolved by `chooseMaxCellStringLength` (`Settings.wl`): if `MaxContextTokens` is `Infinity`, returns `Infinity`; otherwise computes `Min[Ceiling[$defaultMaxCellStringLength * tokens / 2^14], 2^14]`, where `$defaultMaxCellStringLength` is `10000` (`Serialization.wl`) and `tokens` is the resolved `MaxContextTokens` value. This scales the character limit proportionally to the model's context window, capping at 16,384 characters. Depends on `"Model"` and `"MaxContextTokens"` (declared in `$autoSettingKeyDependencies`). The resolved value is used as the initial `$cellStringBudget` in `makeChatMessages` (`ChatMessages.wl`), which dynamically decreases the budget as messages are added based on token pressure: `$cellStringBudget = Ceiling[(1 - $tokenPressure) * $initialCellStringBudget]`, dropping to `0` when the remaining token budget falls below `$reservedTokens` (500). Each cell is serialized via `CellToString` with `"MaxCellStringLength" -> $cellStringBudget` (`ChatMessages.wl`), which truncates cell content exceeding this limit. Chat modes override this value: the Context chat mode lists it in `$downScaledSettings` (scaled down for context queries), and ContentSuggestions overrides it to `1000`. The `"MaxOutputCellStringLength"` setting depends on the resolved `MaxCellStringLength` value. Exposed in the ChatPreferences tool as an integer parameter (where `0` means determine automatically). No model-specific overrides exist in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed to the LLM service). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not exposed in the preferences UI. |
| `"MaxOutputCellStringLength"` | `Automatic` | Maximum string length for output cell content included in the LLM context. Controls how aggressively output cells (styles matching `"Output"`, `"Print"`, `"Echo"`) are truncated when serialized via `CellToString` (`Serialization.wl`). When `Automatic`, resolved by `chooseMaxOutputCellStringLength` (`Settings.wl`): computes `Min[Ceiling[MaxCellStringLength / 10], 1000]`, producing a value that is one-tenth of the resolved `MaxCellStringLength`, capped at 1,000 characters. The fallback constant `$defaultMaxOutputCellStringLength` (`Serialization.wl`) is `500`, used when the option value is `Automatic` at the `CellToString` level. Depends on `"MaxCellStringLength"` (declared in `$autoSettingKeyDependencies`). The resolved value flows through to `CellToString` as the `"MaxOutputCellStringLength"` option; within `CellToString`, it is assigned to the `$maxOutputCellStringLength` dynamic variable (`Serialization.wl`), which controls two behaviors: (1) **Output cell truncation**: cells matching `$$outputStyle` (`"Output"`, `"Print"`, `"Echo"`) are wrapped through `truncateString` after serialization, which calls `stringTrimMiddle[str, $maxOutputCellStringLength]` to trim strings exceeding the limit by replacing the middle with an elision marker. (2) **Graphics serialization threshold**: for graphics boxes, `ByteCount @ box < $maxOutputCellStringLength` determines whether a small graphics expression is serialized as an `InputForm` string (below threshold) or replaced with a placeholder like `"[GRAPHIC]"` (above threshold). Additionally, `truncateString` is used throughout `Serialization.wl` for other content (e.g., `InputForm` strings, compressed data, export packets), always defaulting to `$maxOutputCellStringLength` when no explicit size is given. Special local overrides within `Serialization.wl`: `GridBox` table rendering temporarily sets `$maxOutputCellStringLength = 2*$cellPageWidth` for table cell content; `getExamplesString` (documentation example extraction) temporarily sets it to `100` for compact example summaries; cells matching `$$noTruncateStyle` (`"AlphabeticalListing"`) temporarily set it to `Infinity` to disable truncation entirely. Chat mode overrides: Context mode scales this to 25% via `$notebookContextLimitScale` (applied by `downScaledSettings` in `Context.wl`); ContentSuggestions mode fixes it at `200` via `$contentSuggestionsOverrides`. The `NotebookChunking` prompt generator uses its own independent variable `$maxChunkOutputCellStringLength = 500` (passed directly to `CellToString` as the `"MaxOutputCellStringLength"` option in `NotebookChunking.wl`). No model-specific overrides exist in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook for serialization). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not exposed in the preferences UI. |
| `"ForceSynchronous"` | `Automatic` | Whether to force synchronous (non-streaming) chat requests. When `Automatic`, resolves via `forceSynchronousQ` (`Settings.wl`), which returns `True` if the model's service is `"GoogleGemini"` (since Google Gemini uses a non-streaming API by default) and `False` otherwise. Model-specific overrides in `$modelAutoSettings` set this to `True` for O1, O3, and O4-Mini (OpenAI reasoning models that do not support streaming), and explicitly to `False` for O1-Mini, Gemini 2, and Gemini 3 (which do support streaming, overriding the service-level default for Gemini). When `True`, `chatSubmit0` in `SendChat.wl` uses the synchronous `LLMServices`Chat` function instead of the streaming `LLMServices`ChatSubmit`, waits for the complete response before writing output, sets progress display to `"WaitingForResponse"`, and returns `None` instead of a `TaskObject`. Additionally, when `ForceSynchronous` is `True`, `$showProgressText` is forced to `True` in `resolveAutoSettings` (`Settings.wl`) regardless of the `ShowProgressText` setting. Depends on `"Model"`. The `"BypassResponseChecking"` setting depends on this: when `ForceSynchronous` is `True`, `BypassResponseChecking` also resolves to `True`, skipping HTTP status code validation, empty response detection, and JSON error parsing. Not exposed in the preferences UI. |
| `"TimeConstraint"` | `Automatic` | Time limit (in seconds) for chat evaluation. |
| `"ConvertSystemRoleToUser"` | `Automatic` | Whether to convert system-role messages to user-role messages. Model default: `False`. Required for some models (e.g., O1-Mini). |
| `"ReplaceUnicodeCharacters"` | `Automatic` | Whether to replace Unicode characters with ASCII equivalents before sending to the LLM. Model default: `False`. Enabled for Anthropic models and some OpenAI models (e.g., GPT-5.2). |
| `"BypassResponseChecking"` | `Automatic` | Whether to bypass response validation after receiving an LLM response. When `True`, the response is immediately written as a formatted output cell without validating the HTTP status code, checking for empty responses, or extracting error data from the response body. When `False`, the response goes through full validation: the debug log is processed to extract body chunks, status codes are checked (non-200 responses trigger error cells), empty responses are detected, and JSON error data is parsed before writing output. Resolves to `True` when `ForceSynchronous` is `True`, `False` otherwise. Depends on `"ForceSynchronous"`. |
| `"Assistance"` | `Automatic` | Whether automatic assistance mode is enabled. When `Automatic`, resolves to `False`. When `True`, LLM responses are processed immediately rather than being queued for user approval, output cells use `"AssistantOutput"` styles instead of `"ChatOutput"`, and certain tools are disabled (WolframLanguageEvaluator, CreateNotebook, WolframAlpha). Controlled in the notebook preferences UI as "Enable automatic assistance". |

## Prompting

| Setting | Default | Description |
| ------- | ------- | ----------- |
| `"BasePrompt"` | `Automatic` | Specifies which base prompt components to include in the system prompt. Can be `Automatic` (inherited from persona/model settings), `None` (disables all base prompting, as used by the RawModel persona), a single component name string, or a list of component names. Lists can include `ParentList` to inherit from the parent scope while adding additional components (e.g., `{ParentList, "Notebooks", "WolframLanguageStyle"}`). Available components are defined in `Prompting.wl` via `$basePromptComponents` and `$basePromptOrder`, and include individual components (e.g., `"Markdown"`, `"CodeBlocks"`, `"MathExpressions"`, `"EscapedCharacters"`, `"WolframLanguageStyle"`, `"EndTurnToken"`) as well as class names that expand to groups of components (e.g., `"Notebooks"`, `"WolframLanguage"`, `"Math"`, `"Formatting"`, `"All"`). Dependencies between components are automatically resolved via `$basePromptDependencies`. Part of `$modelInheritedLists`, which enables special list-merging behavior with `ParentList`. Interacts with `ExcludedBasePrompts`, which removes specified components from the resolved list. Personas typically set this to include `ParentList` plus persona-specific components (e.g., CodeAssistant uses `{ParentList, "Notebooks", "WolframLanguageStyle"}`). |
| `"ExcludedBasePrompts"` | `Automatic` | List of base prompt component names to exclude from the system prompt. When `Automatic`, resolves to `{ParentList}` via the global model auto default, meaning it inherits exclusions from the parent model settings. Can be a list containing strings (component names or class names) and/or `ParentList` for inheritance. Valid values must match `{ (_String\|ParentList)... }`; invalid values trigger an `"InvalidExcludedBasePrompts"` failure. Applied after the `"BasePrompt"` list is resolved: in `augmentChatMessages` (`ChatMessages.wl`), the resolved `BasePrompt` list is filtered via `DeleteCases` to remove any components matching the exclusion list. Additionally, the excluded components are removed from the collected prompt components via `removeBasePrompt` in `Prompting.wl`, which drops matching keys from `$collectedPromptComponents` and strips the corresponding text from the system message content. The resolved value (with `ParentList` entries removed) is stored in the `$excludedBasePrompts` global variable (`Settings.wl`), which is also checked by `needsBasePrompt` (`Prompting.wl`) to prevent excluded components from being collected during prompt construction. Part of `$modelInheritedLists` (along with `"BasePrompt"`), which enables special list-merging behavior in `inheritModelSettings`: when the value contains `ParentList`, it is merged with the model-specific default via `mergeChatSettings`, allowing syntax like `{ParentList, "EscapedCharacters"}` to mean "inherit parent exclusions and also exclude EscapedCharacters." Model-specific overrides: GPT-5.2 uses `{ParentList, "EscapedCharacters"}` (because it has improved Unicode handling). The `$defaultConfigSettings` in `LLMUtilities.wl` sets this to `{"Notebooks", "NotebooksPreamble"}` for LLM configuration generation (not the regular chat notebook flow). No explicit dependencies in `$autoSettingKeyDependencies`. Not exposed in the preferences UI. |
| `"ChatContextPreprompt"` | `Automatic` | **Deprecated.** Legacy preprompt text used as the "Pre" section of the system prompt sent to the LLM. Resolved via `getPrePrompt` in `ChatMessages.wl`, which checks the following in priority order: persona-level `"ChatContextPreprompt"`, persona-level `"Pre"` / `"PromptTemplate"` / `"Prompts"`, then global `"ChatContextPreprompt"`, then global `"Pre"` / `"PromptTemplate"` / `"Prompts"`. The value must be a `String`, `TemplateObject`, or list thereof. Exposed in the chat context settings dialog (`Actions.wl`) as a text input field with a default of `"You are a helpful Wolfram Language programming assistant. Your job is to offer Wolfram Language code suggestions based on previous inputs and offer code suggestions to fix errors."`. Automatic value resolution is not implemented (noted as TODO in `Settings.wl`). Superseded by persona-based prompting via `"LLMEvaluator"` and the `"BasePrompt"` component system. |
| `"UserInstructions"` | `Automatic` | User-provided instructions to include in the system prompt. |
| `"Prompts"` | `{}` | Additional prompt messages to include in the conversation. |
| `"PromptGenerators"` | `Automatic` | [TODO] List of prompt generators to use for augmenting prompts with additional context (e.g., related documentation, Wolfram Alpha results). Behavior defined in the `PromptGenerators/` directory. Default when `Automatic`: `{}`. |
| `"PromptGeneratorsEnabled"` | `Automatic` | [TODO] Which prompt generators are enabled. Behavior defined in the `PromptGenerators/` directory. |
| `"PromptGeneratorMessagePosition"` | `2` | Position in the message list where prompt generator messages are inserted. |
| `"PromptGeneratorMessageRole"` | `"System"` | Message role used for prompt generator messages. |
| `"DiscourageExtraToolCalls"` | `Automatic` | Whether to include a base prompt component discouraging unnecessary tool calls. When enabled, adds the `"DiscourageExtraToolCalls"` base prompt component to the system prompt, which appends the text: `"Don't make more tool calls than is needed. Tool calls cost tokens, so be efficient!"`. The setting is evaluated via `discourageExtraToolCallsQ` in `ChatMessages.wl`, which returns `False` if `ToolsEnabled` is `False` or `Tools` is empty (i.e., the prompt is only included when tools are actually available). Has no dependencies on other base prompt components. Model-specific: currently only enabled (`True`) for Anthropic Claude 3.7 Sonnet via `$modelAutoSettings`. No global auto default exists, so `Automatic` effectively resolves to `False` for all other models. Not exposed in the preferences UI. |

## Tools

| Setting | Default | Description |
| ------- | ------- | ----------- |
| `"Tools"` | `Automatic` | [TODO] Tool definitions available to the LLM. Behavior defined in the `Tools/` directory. When `Automatic`, tools are resolved based on the persona and `ToolsEnabled` setting. |
| `"ToolsEnabled"` | `Automatic` | Whether tools are enabled for the current chat. Resolved per model (e.g., disabled for Gemini Pro/Pro Vision). |
| `"ToolMethod"` | `Automatic` | Method for tool calling. `"Service"` uses the LLM service's native tool calling API. Other values use prompt-based tool calling. Model-specific defaults. |
| `"HybridToolMethod"` | `Automatic` | Whether to use hybrid tool calling, combining service-level and prompt-based tool calling. When `Automatic`, resolved by `hybridToolMethodQ` (`Settings.wl`): returns `False` if `ToolsEnabled` is `False`; returns `False` if `ToolMethod` is `"Service"` (since service-level calling is already in use, hybrid mode is unnecessary); otherwise returns `True` if the model matches `$$hybridToolModel`, which requires the service to be `"OpenAI"`, `"AzureOpenAI"`, or `"LLMKit"` (or the model to be a plain string); returns `False` for all other models. When `True`, `makeLLMConfiguration` in `SendChat.wl` builds the `LLMConfiguration` with `"ToolMethod" -> "Service"` and includes `LLMTool` definitions in the `"Tools"` parameter, enabling the LLM service's native tool calling API alongside Chatbook's prompt-based tool calling. This means the model receives both prompt-based tool instructions (from the resolved `ToolMethod`, e.g., `"Simple"`) and service-level tool definitions, allowing it to use either mechanism. When `False`, the `LLMConfiguration` omits tool definitions and relies solely on the prompt-based tool method. Depends on `"Model"`, `"ToolsEnabled"`, and `"ToolMethod"` (declared in `$autoSettingKeyDependencies`). Model-specific overrides in `$modelAutoSettings`: `True` for GPT-4o, GPT-4.1, and O3-Mini (which use `ToolMethod -> Verbatim @ Automatic` to keep prompt-based tools alongside service tools); `False` for DeepSeek Reasoner, GPT-5, O1, O3, and O4-Mini (which either use pure `"Service"` tool method or have limited tool support). Not in `$llmConfigPassedKeys` (not passed directly through `LLMConfiguration`), but indirectly controls whether `LLMConfiguration` includes `"ToolMethod" -> "Service"` and `"Tools"`. Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not exposed in the preferences UI. |
| `"ToolOptions"` | `$DefaultToolOptions` | Per-tool option overrides. Default provides options for `"WolframAlpha"`, `"WolframLanguageEvaluator"`, `"WebFetcher"`, `"WebSearcher"`, and `"WebImageSearcher"`. See `Tools/Common.wl` for the full default. |
| `"ToolSelectionType"` | `<\|\|>` | Tool selection configuration. An empty association by default. |
| `"ToolCallFrequency"` | `Automatic` | How often the LLM is allowed to make tool calls. Default: `Automatic`. |
| `"ToolCallRetryMessage"` | `Automatic` | Whether to send retry messages when a tool call fails. Resolves to `True` for LLMKit-authenticated sessions, `False` for some models (e.g., GPT-4.1, GPT-5). |
| `"ToolExamplePrompt"` | `Automatic` | Tool example prompt specification included in the system prompt to guide tool usage. Resolved per model (e.g., `None` for Claude 3). |
| `"ToolCallExamplePromptStyle"` | `Automatic` | Style of tool call example prompts (`"Basic"` or `Automatic`). Model-specific. |
| `"ToolResponseRole"` | `Automatic` | Message role used for tool response messages. Model default: `"System"`. Some models use `"User"` (e.g., Claude 2, MistralAI, DeepSeek Reasoner, local models like Qwen/Nemotron/Mistral). |
| `"ToolResponseStyle"` | `Automatic` | Style used for formatting tool responses. MistralAI uses `"SystemTags"`. |
| `"SplitToolResponseMessages"` | `Automatic` | Whether to split tool responses into separate messages. Model default: `False`. Enabled for Anthropic models as a workaround. |
| `"MaxToolResponses"` | `5` | Maximum number of tool responses allowed per chat turn before the tool-calling loop is stopped. During each chat turn, the `$toolCallCount` global variable (`CommonSymbols.wl`) is incremented each time the LLM makes a tool call (`SendChat.wl`), initialized to `0` via `ChatState.wl`. The `sendToolResponseQ` function (`SendChat.wl`) checks whether to continue the tool-calling loop: if `$toolCallCount > n` (where `n` is the `MaxToolResponses` value), it returns `False`, causing the chat to stop via `throwTop @ StopChat @ cell` — ending the current turn without sending the tool response back to the LLM. Note that the comparison uses `>` (not `>=`), so the LLM can make up to `n + 1` tool calls before being stopped (the count is incremented before the check). There is a TODO comment in the source noting that this should allow one final response when reaching the limit and disable tools for the next chat submit. Model-specific overrides in `$modelAutoSettings`: `3` for O1, O3, and O4-Mini (OpenAI reasoning models). Chat mode overrides: NotebookAssistance mode explicitly sets this to `5` in `$notebookAssistanceBaseSettings` (`ShowNotebookAssistance.wl`). Listed in `$popOutSettings` (`ConvertChatNotebook.wl`) as one of four settings shown during chat notebook conversion. No dependencies in `$autoSettingKeyDependencies`. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook to control the tool-calling loop). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not exposed in the preferences UI. |
| `"SendToolResponse"` | `Automatic` | Whether to send tool responses back to the LLM for further processing. |
| `"EndToken"` | `Automatic` | End-of-turn token that signals the LLM has finished its response. Model default: `"/end"`. Some models use `None` (e.g., GPT-4.1). The resolved value is stored in the `$endToken` global variable (`CommonSymbols.wl`) during `resolveAutoSettings` (`Settings.wl`). Used in three ways: (1) **Base prompt instruction**: The `"EndTurnToken"` base prompt component (`Prompting.wl`) adds `"* Always end your turn by writing /end."` to the system prompt when `$endToken` is a non-empty string. The related `"EndTurnToolCall"` component (which depends on `"EndTurnToken"`) adds `"* If you are going to make a tool call, you must do so BEFORE ending your turn."`. (2) **Stop tokens**: `$endToken` is included in the stop sequences sent to the LLM API via `methodStopTokens` (`Settings.wl`), which builds tool-method-specific stop token lists (e.g., `{"ENDTOOLCALL", $endToken}` for `"Textual"`/`"JSON"` methods, `{$endToken}` for `"Service"` method, `{"\n/exec", $endToken}` for `"Simple"` method). These are composed into the `"StopTokens"` setting by `autoStopTokens` and passed to `LLMConfiguration` and the HTTP request body. (3) **Tool call example templates**: `Tools/Examples.wl` uses `$endTokenString` (which prepends `"\n"` to `$endToken` when non-empty) in assistant message templates across all example styles (Basic, Instruct, Zephyr, Phi, Boxed, ChatML, XML, DeepSeekCoder, Llama, Gemma, Nemotron) to show the LLM how to end its turn in example conversations. When `None` or empty, the end token is omitted from prompts, stop sequences, and example templates. After receiving a response, `trimStopTokens` in `SendChat.wl` removes stop tokens (including the end token) from the end of the LLM's output. No model-specific overrides exist beyond GPT-4.1 (`None`). Not exposed in the preferences UI. |

## Formatting & Output

| Setting | Default | Description |
| ------- | ------- | ----------- |
| `"AutoFormat"` | `True` | Whether to auto-format LLM output by parsing Markdown syntax and converting it to structured notebook cells. When enabled, the LLM response is processed to convert Markdown elements (code blocks with language detection, headings, bold/italic text, inline code, LaTeX math, images, bullet lists, block quotes, and tables) into properly formatted Wolfram notebook cells. Also includes the `"Formatting"` base prompt component in the system prompt, which instructs the LLM that its output will be parsed as Markdown. When disabled, output is displayed as plain text. Works in conjunction with `"DynamicAutoFormat"` to control whether formatting is applied during streaming. |
| `"DynamicAutoFormat"` | `Automatic` | Whether to apply formatting during streaming, providing live-formatted output as the LLM response streams in. When `True`, the streaming content is processed by the formatting function in real-time, converting Markdown to formatted notebook expressions as they arrive. When `False`, streaming content is displayed as raw text (via `RawBoxes @ Cell @ TextData`) without live formatting. When `Automatic`, resolves to `TrueQ` of the `"AutoFormat"` setting, so dynamic formatting is enabled whenever auto-formatting is enabled. Resolution is handled by `dynamicAutoFormatQ` in `SendChat.wl`, which first checks for an explicit `True`/`False` value, then falls back to `"AutoFormat"`. The resolved value is captured as the `reformat` variable in `activeAIAssistantCell` and passed to `dynamicTextDisplay`, which dispatches between formatted and raw text display. No model-specific overrides exist. Not exposed in the preferences UI; controlled indirectly via the `"AutoFormat"` checkbox. |
| `"StreamingOutputMethod"` | `Automatic` | Method for streaming output display. Default: `"PartialDynamic"`. |
| `"NotebookWriteMethod"` | `Automatic` | Method for writing content to the notebook, controlling whether FrontEnd task batching is used for notebook write operations during chat. When `Automatic`, resolves to `"PreemptiveLink"` via `resolveAutoSetting0` (`Settings.wl`). Two valid values: `"PreemptiveLink"` enables FrontEnd task optimization, where notebook write operations (e.g., `NotebookWrite`, formatting toggles, error cell placement) are queued via `createFETask` (`FrontEnd.wl`) into the `$feTasks` list and executed in batches via `runFETasks`, reducing MathLink roundtrips between the kernel and FrontEnd for better responsiveness during streaming; `"ServiceLink"` disables this optimization by locally redefining `createFETask` to be the identity function (`#1 &`), causing all notebook writes to execute inline immediately. The setting is evaluated by the `feTaskQ` function (`SendChat.wl`), which returns `True` for `"PreemptiveLink"` (or unspecified values) and `False` for `"ServiceLink"`. Invalid values trigger an `"InvalidWriteMethod"` warning message (`Common.wl`) and fall back to `True` (task optimization enabled). The `feTaskQ` result is passed to `withFETasks` (`SendChat.wl`), which wraps both the `"BodyChunkReceived"` and `"TaskFinished"` handler functions in `chatHandlers`: when `True`, handlers execute normally with `createFETask` queueing enabled; when `False`, handlers execute inside `Block[{createFETask = #1 &}, ...]`, disabling task queueing. In headless mode (`$headlessChat`), `feTaskQ` always returns `False` regardless of the setting value, since there is no FrontEnd to process tasks. In cloud notebooks (`$cloudNotebooks`), `createFETask` evaluates operations immediately rather than queueing them. No model-specific overrides exist in `$modelAutoSettings`. No dependencies in `$autoSettingKeyDependencies`. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook for notebook write optimization). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not exposed in the preferences UI. |
| `"TabbedOutput"` | `True` | Whether to use tabbed output for organizing long or multi-part responses. |
| `"ShowMinimized"` | `Automatic` | Whether to show output in a minimized/collapsed state. |
| `"ShowProgressText"` | `Automatic` | Whether to show progress text while the LLM is generating a response. Model default: `True`. |
| `"OpenToolCallBoxes"` | `Automatic` | Whether tool call display boxes are open by default. Resolves to `True` when `SendToolResponse` is `False`, otherwise `Automatic`. |
| `"TrackScrollingWhenPlaced"` | `Automatic` | Whether to auto-scroll the notebook to follow new output as it is placed. |
| `"AppendCitations"` | `Automatic` | Whether to automatically append formatted source citations to the LLM response. When enabled, citations are generated from sources gathered by prompt generators (e.g., documentation, web searches, WolframAlpha results) and appended as a markdown section. When disabled, the WolframAlpha prompt generator instead includes a hint asking the LLM to cite sources inline. Model default: `False`. The WolframAlpha persona overrides this to `True`. |

## Personas & UI

| Setting | Default | Description |
| ------- | ------- | ----------- |
| `"LLMEvaluator"` | `"CodeAssistant"` | The persona (LLM evaluator) to use. Determines the system prompt, available tools, and other settings. Can be a persona name string (e.g., `"CodeAssistant"`, `"PlainChat"`, `"RawModel"`) or a full persona `Association` with keys like `"Prompts"`, `"Tools"`, `"Icon"`, `"BasePrompt"`, etc. Built-in personas are defined in `LLMConfiguration/Personas/`. During `resolveAutoSettings` (`Settings.wl`), the string value is resolved via `getLLMEvaluator`, which calls `getNamedLLMEvaluator` to look up persona data from `GetCachedPersonaData`. If the name is not found as a built-in persona, `tryPromptRepositoryPersona` attempts to load it from the Wolfram Prompt Repository via `ResourceObject["Prompt" -> name]`. The resolved persona `Association` is merged with the current settings via `mergeChatSettings`, with `$nonInheritedPersonaValues` keys (including `"LLMEvaluator"` itself) dropped from the persona data before merging, preventing circular inheritance. After resolution, the `"LLMEvaluator"` key in the settings is replaced with the full persona `Association` (or the original string/`None` if unresolvable). When writing settings back to notebook `TaggingRules`, `toSmallSettings` in `SendChat.wl` converts the resolved `Association` back to just the persona name string (via the `"LLMEvaluatorName"` key) to save space. The persona determines: the system prompt (via `"Prompts"`, `"Pre"`, `"PromptTemplate"`, and `"BasePrompt"` keys), available tools (the `"Tools"` setting depends on `"LLMEvaluator"` and `"ToolsEnabled"` in `$autoSettingKeyDependencies`; `selectTools` in `Tools/Common.wl` uses the persona name to look up per-persona tool selections), and the output cell dingbat icon (`makeOutputDingbat` and `makeActiveOutputDingbat` in `SendChat.wl` extract the persona's `"PersonaIcon"` or `"Icon"` key). When `"RawOutput"` is enabled during chat evaluation, the persona is overridden with `GetCachedPersonaData["RawModel"]` in `sendChat` (`SendChat.wl`). `CreateChatDrivenNotebook` defaults this to `"PlainChat"`. Exposed in the notebook preferences UI as a persona PopupMenu selector under the "Notebooks" section (`PreferencesContent.wl`). Also central to the chat action menu in `UI.wl`, where each persona appears as a selectable menu item that writes to `{TaggingRules, "ChatNotebookSettings", "LLMEvaluator"}`, with a "Reset" option that sets the value to `Inherited`. Listed in `$nonInheritedPersonaValues`, so it retains its value from the notebook/cell scope rather than inheriting from persona configurations. No dependencies in `$autoSettingKeyDependencies` (but `"Tools"` depends on it). No model-specific overrides exist in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed to the LLM service). |
| `"VisiblePersonas"` | `$corePersonaNames` | List of persona names visible in the persona selector UI. |
| `"ChatDrivenNotebook"` | `False` | **Deprecated.** Whether the entire notebook operates in "chat-driven" mode rather than the default "chat-enabled" mode. When `True`, new cells default to `"ChatInput"` style, the persona selector prioritizes PlainChat/RawModel/CodeWriter/CodeAssistant at the top of the list (`UI.wl` `filterPersonas`), and the cloud toolbar displays "Chat-Driven Notebook" instead of "Chat Notebook" (`CloudToolbar.wl`). Used by `CreateChatDrivenNotebook[]`, which wraps `CreateChatNotebook` with `"ChatDrivenNotebook" -> True`, `"LLMEvaluator" -> "PlainChat"`, and `DefaultNewCellStyle -> "ChatInput"`. This is a non-inherited persona value (listed in `$nonInheritedPersonaValues`). |
| `"InitialChatCell"` | `True` | Whether to create an initial empty chat input cell when opening a new chat notebook. When `True`, `CreateChatNotebook` inserts an empty `"ChatInput"` cell (via `initialChatCells` in `CreateChatNotebook.wl`); for cloud notebooks (`$cloudNotebooks`), an additional selection-mover cell is appended to position the cursor. When `False`, the notebook is created with no initial cells. The value is evaluated via `TrueQ`, so only an explicit `True` creates the cell; `Automatic` or other non-boolean values behave as `False`. This is an **unsaved setting** (listed in `$unsavedSettings` in `CreateChatNotebook.wl`): it is used only at notebook creation time and is explicitly dropped from the notebook's `TaggingRules` by `makeChatNotebookSettings`, so it does not persist in the saved notebook. Defined as an option of `CreateChatNotebook` (inherited from `$defaultChatSettings`). When creating a notebook from a `ChatObject`, `initialChatCells` is locally overridden to return the converted message cells instead. No dependencies on other settings. No model-specific overrides exist in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed to the LLM service). Listed in `$nonInheritedPersonaValues`, so it retains its value from notebook/cell scope rather than inheriting from persona configurations. Not exposed in the preferences UI. |
| `"ChatInputIndicator"` | `Automatic` | Text prefix prepended to `"ChatInput"` cells when serializing notebook content for the LLM. When `Automatic`, resolves to `"\|01f4ac"` (speech balloon emoji). Can be any string (e.g., `"[USER]"`), or `None`/`""` to disable the indicator entirely. Only applied when the content is mixed (i.e., when `mixedContentQ` returns `True` in `ChatMessages.wl`, indicating the conversation includes both chat input cells and other cell types). When the indicator is used, the `"ChatInputIndicator"` base prompt component (`Prompting.wl`) is automatically included in the system prompt to explain the indicator's meaning to the LLM: it tells the model that cells prefixed with this symbol are actual user messages, while other cells are context. The indicator text is distinct from cell dingbats controlled by `"SetCellDingbat"`, which are visual notebook icons. The indicator symbol is set per chat evaluation via `chatIndicatorSymbol` in `SendChat.wl` and stored in the global `$chatIndicatorSymbol` variable (`Common.wl`). No model-specific overrides exist. |
| `"SetCellDingbat"` | `True` | Whether to set cell dingbats (icons) on chat cells. |
| `"EnableChatGroupSettings"` | `False` | Whether chat group-level settings are enabled. When `True`, during chat evaluation (`SendChat.wl`), `getChatGroupSettings` is called on the evaluation cell to retrieve prompt text from parent group header cells. The feature walks backward through notebook cells to find parent group headers using cell grouping rules (`"TitleGrouping"` and `"SectionGrouping"`) and collects `"Prompt"` values stored in their `TaggingRules` at the path `"ChatNotebookSettings"` → `"ChatGroupSettings"` → `"Prompt"`. Multiple prompts from different grouping levels are joined with `"\n\n"`. The collected group prompt is stored in the settings as `"ChatGroupSettings"` and incorporated into the system prompt via `buildSystemPrompt` in `ChatMessages.wl`, where it appears as the `"Group"` section in the prompt template (between `"Pre"` and `"Base"` sections). When `False`, no group settings are resolved and the `"Group"` section of the system prompt is omitted. Implementation is in `ChatGroups.wl`. Not exposed in the preferences UI. No model-specific overrides exist. |
| `"AllowSelectionContext"` | `Automatic` | Whether to allow the current selection to be used as context. Resolves to `True` when using workspace chat, inline chat, or sidebar chat. |
| `"CurrentPreferencesTab"` | `"Services"` | Persists the user's last-selected tab in the Chatbook preferences dialog. When the preferences dialog opens, the tab is initialized from this setting (defaulting to `"Services"` if unset); when the dialog closes, the current tab selection is saved back. The `openPreferencesPage` function in `PreferencesContent.wl` also writes to this setting at `$FrontEnd` scope to navigate directly to a specific preferences page. This is a non-inherited persona value (listed in `$nonInheritedPersonaValues`) and is excluded from debug/diagnostic data (listed in `$droppedSettingsKeys` in `Common.wl`). Not included in `$defaultChatSettings`. No model-specific overrides exist. |

## Storage & Conversations

| Setting | Default | Description |
| ------- | ------- | ----------- |
| `"ConversationUUID"` | `None` | UUID identifying the current conversation. `None` means no conversation tracking. When a conversation is saved (via `SaveChat`), `ensureConversationUUID` in `Storage.wl` checks whether the current setting is a valid string; if not, it generates a new UUID via `CreateUUID[]` and writes it back to the notebook or cell's `CurrentChatSettings`. Used as the primary key for persistent conversation storage (`Storage.wl`), chat search indexing (`Search.wl`), and chat history listings (`ChatModes/UI.wl`). The `AutoSaveConversations` setting depends on `ConversationUUID` being a valid string (along with `AppName`). Chat modes may override this: workspace chat and sidebar chat automatically generate a new UUID when starting a new conversation (`ChatModes/UI.wl`) or when initialized via NotebookAssistance settings (`ChatModes/ShowNotebookAssistance.wl`); inline chat does not set a UUID (falls back to `None`). When loading a saved conversation, the stored UUID is restored to `CurrentChatSettings` for the target notebook or cell. |
| `"AutoSaveConversations"` | `Automatic` | Whether to automatically save conversations to persistent storage after chat evaluations. When `Automatic`, resolves to `True` if both `AppName` is a valid string and `ConversationUUID` is a valid string; otherwise resolves to `False`. Depends on `"AppName"` and `"ConversationUUID"`. When `True`, conversations are saved after each chat evaluation, subject to the `"MinimumResponsesToSave"` threshold. Chat modes may override this: workspace chat and sidebar chat set it to `True` (with a new `ConversationUUID`), while inline chat sets it to `False`. |
| `"AppName"` | `Automatic` | Application name used to namespace saved conversations, search indexes, and chat history listings. When `Automatic`, resolves to `$defaultAppName` (`"Default"`). When set to a non-default string value, also establishes a service caller context via `setServiceCaller`. Chat modes may override this (e.g., NotebookAssistance uses `"NotebookAssistance"`). The `AutoSaveConversations` setting depends on `AppName` being a valid string. |
| `"MinimumResponsesToSave"` | `1` | Minimum number of assistant responses required before a conversation is automatically saved. Used by `autoSaveQ` in `Storage.wl` to gate auto-saving: after each chat evaluation, the function counts messages with `"Role" -> "Assistant"` in the conversation and only proceeds with saving if the count is greater than or equal to this value. The value must be a positive integer (`_Integer? Positive`); invalid values trigger an `"MinResponses"` confirmation failure. The default of `1` means that auto-saving occurs as soon as the first assistant response is present. Chat mode overrides: the `"GettingStarted"` alias in `ShowNotebookAssistance.wl` sets this to `2`, requiring at least two assistant responses before saving (preventing the initial getting-started prompt from being saved as a conversation). This setting only takes effect when `"AutoSaveConversations"` resolves to `True` (which itself requires valid `"AppName"` and `"ConversationUUID"` values). No dependencies in `$autoSettingKeyDependencies`. No model-specific overrides exist in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook for conversation storage). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not exposed in the preferences UI. |
| `"TargetCloudObject"` | `Automatic` | Target cloud object for cloud-based conversation storage. |

## Advanced / Internal

| Setting | Default | Description |
| ------- | ------- | ----------- |
| `"Tokenizer"` | `Automatic` | Tokenizer used for token counting. Resolved based on the model's tokenizer name. |
| `"HandlerFunctions"` | `$DefaultChatHandlerFunctions` | Callback functions invoked at various stages of chat processing. The value is an `Association` mapping event name strings to handler functions (or `None` to skip). The default value `$DefaultChatHandlerFunctions` (`Settings.wl`) defines 9 event keys, all defaulting to `None`: `"AppendCitationsStart"`, `"AppendCitationsEnd"`, `"ChatAbort"`, `"ChatPost"`, `"ChatPre"`, `"PromptGeneratorEnd"`, `"PromptGeneratorStart"`, `"ToolRequestReceived"`, and `"ToolResponseGenerated"`. The `"ChatAbort"`, `"ChatPost"`, and `"ChatPre"` entries use `RuleDelayed` (`:>`) pointing to global variables `$ChatAbort`, `$ChatPost`, and `$ChatPre` (all initially `None`), allowing runtime reassignment without modifying the association. A 10th event, `"ToolResponseReceived"`, is also dispatched via `applyHandlerFunction` (`SendChat.wl`) but is not included in the default association (falls back to `None` via `getHandlerFunction`). Custom handler values are merged with defaults during resolution: `resolveHandlers` in `Handlers.wl` creates a new association with `$DefaultChatHandlerFunctions` as the base, overlaid with user-provided handlers (after `replaceCellContext` processing), plus a `"Resolved" -> True` marker to prevent re-resolution. Resolution occurs during `resolveAutoSettings` (`Settings.wl`), where `getHandlerFunctions` is called on the settings and the result replaces the `"HandlerFunctions"` key. Each handler function is invoked via `applyHandlerFunction` (`Handlers.wl`), which constructs an argument association containing: `"EventName"` (the event type string), `"ChatNotebookSettings"` (current settings with `"Data"` and `"OpenAIKey"` keys dropped), and event-specific data. This argument association is accumulated in the `$ChatHandlerData` global variable (publicly exported) via `addHandlerArguments`, which merges new data with existing handler state (supporting nested association merging). The handler receives `$ChatHandlerData` with `"DefaultProcessingFunction"` dropped. Event dispatch locations: `"ChatPre"` is called in `sendChat` (`SendChat.wl`) before chat submission, with `"EvaluationCell"` and `"Messages"` in the arguments; `"ChatPost"` and `"ChatAbort"` are called in `applyChatPost` (`Actions.wl`) after chat completion or abort, with `"ChatObject"` and `"NotebookObject"` in the arguments; `"ToolRequestReceived"` is called after parsing a tool call (`SendChat.wl`), with `"ToolRequest"` in the arguments; `"ToolResponseGenerated"` is called after generating a tool response (`SendChat.wl`), with `"ToolResponse"` and `"ToolResponseString"` in the arguments; `"ToolResponseReceived"` is called after the tool response is formatted and ready to send back (`SendChat.wl`), with `"ToolResponse"` in the arguments; `"PromptGeneratorStart"` and `"PromptGeneratorEnd"` are called in `DefaultPromptGenerators.wl` around each prompt generator execution, with `"PromptGenerator"` in the arguments (and `"PromptGeneratorResult"` added for the end event); `"AppendCitationsStart"` and `"AppendCitationsEnd"` are called in `Citations.wl` around citation generation, with `"Sources"` and `"CitationString"` respectively. In the streaming chat submission path (`chatHandlers` in `SendChat.wl`), the resolved handlers are passed to `LLMServices`ChatSubmit` via the `HandlerFunctions` parameter, but `"ChatPost"`, `"ChatPre"`, and `"Resolved"` keys are dropped (listed in `$chatSubmitDroppedHandlers`). The `chatHandlers` function also wraps custom `"BodyChunkReceived"` and `"TaskFinished"` handlers (if provided) inside Chatbook's own streaming logic, calling the user's handler before Chatbook's processing for each body chunk and after task completion. Not passed through `LLMConfiguration` (not in `$llmConfigPassedKeys`). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. No dependencies in `$autoSettingKeyDependencies`. No model-specific overrides exist in `$modelAutoSettings`. Not exposed in the preferences UI. |
| `"HandlerFunctionsKeys"` | `Automatic` | Keys to include in the handler functions callback data passed to `LLMServices`ChatSubmit` and `URLSubmit`. Controls which fields from the streaming response are available to handler functions. When `Automatic` or unspecified, resolves to `$defaultHandlerKeys` (`SendChat.wl`): `{"Body", "BodyChunk", "BodyChunkProcessed", "StatusCode", "TaskStatus", "EventName"}`. When a list of strings, the user-provided keys are merged with `$defaultHandlerKeys` via `Union` (so the default keys are always included). When a single string, it is treated as a one-element list. Invalid values trigger an `"InvalidHandlerKeys"` warning (`Common.wl`) and fall back to `$defaultHandlerKeys`. Resolution is performed by `chatHandlerFunctionsKeys` (`SendChat.wl`), which is called from `resolveAutoSetting0` (`Settings.wl`). The resolved value is passed directly as the `HandlerFunctionsKeys` parameter to `LLMServices`ChatSubmit` (for LLMServices-based chat) and `URLSubmit` (for legacy HTTP-based chat) in `chatSubmit0` (`SendChat.wl`). Also used in other `URLSubmit` calls outside of chat: `VectorDatabases.wl` uses `{"ByteCountDownloaded", "StatusCode"}` for vector database downloads, and `RelatedWolframAlphaResults.wl` uses `{"StatusCode", "BodyByteArray"}` for Wolfram Alpha result fetching. Not passed through `LLMConfiguration` (not in `$llmConfigPassedKeys`). Depends on `"EnableLLMServices"` for resolution ordering in `$autoSettingKeyDependencies`. Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. No model-specific overrides exist in `$modelAutoSettings`. Not exposed in the preferences UI. |
| `"InheritanceTest"` | N/A | Internal diagnostic flag used by the settings inheritance verification system. Not a user-configurable setting and not included in `$defaultChatSettings`. During `verifyInheritance0` (`Settings.wl`), this flag is set to `True` at the `$FrontEnd` scope via `setCurrentValue[fe, {TaggingRules, "ChatNotebookSettings", "InheritanceTest"}, True]` to mark that the inheritance chain for tagging rules has been properly initialized. Subsequent calls to `verifyInheritance` check this flag via `inheritingQ`, which reads `AbsoluteCurrentValue[obj, {TaggingRules, "ChatNotebookSettings", "InheritanceTest"}]` — if `True` (or if the read fails), the object is considered to have valid inheritance and initialization is skipped. During `repairTaggingRules`, the flag is explicitly removed from child objects (notebooks, cells) so that it only persists at the top-level `FrontEndObject`, preventing it from appearing as an explicit override in child scopes. The `verifyInheritance` function is called by `currentChatSettings0` before reading or writing settings, ensuring the inheritance chain is intact. Listed in `$nonInheritedPersonaValues`, so it retains its value from notebook/cell scope rather than inheriting from persona configurations. No model-specific overrides exist. Not exposed in the preferences UI. |
| `"ProcessingFunctions"` | `$DefaultChatProcessingFunctions` | [TODO] Functions that control the chat processing pipeline (e.g., `"CellToChatMessage"`, `"ChatMessages"`, `"ChatSubmit"`, `"FormatChatOutput"`, `"FormatToolCall"`, `"WriteChatOutputCell"`). Spans multiple files. |
| `"ConversionRules"` | `None` | Custom transformation rules applied to notebook cells before they are serialized to chat message strings. When `None` (default), cells are passed through unmodified. When set to a list of replacement rules, the rules are compiled into a `Dispatch` table (cached for reuse) and applied to each cell via `ReplaceRepeated` inside `CellToString` (`Serialization.wl`) before string conversion occurs. This enables custom box-level or expression-level transformations of cell content prior to sending it to the LLM. The setting value is read from `CurrentChatSettings` in `makeChatMessages` (`ChatMessages.wl`) and stored in the dynamic variable `$conversionRules`, which `CellToString` picks up as its default `"ConversionRules"` option. Invalid values (neither a rule list nor a valid `Dispatch` table) trigger an `"InvalidConversionRules"` warning and fall back to `None`. |
| `"ExperimentalFeatures"` | `Automatic` | List of enabled experimental feature names, resolved dynamically from other settings. When `Automatic`, the `autoExperimentalFeatures` function (`Settings.wl`) builds a list based on two conditions: `"RelatedWolframAlphaResults"` is included if `"WolframAlphaCAGEnabled"` is `True` or `"RelatedWolframAlphaResults"` is in the `"PromptGenerators"` list; `"RelatedWebSearchResults"` is included if `"WebSearchRAGMethod"` is `"Tavily"` or `"WebSearch"` is in the `"PromptGenerators"` list. The resolved list is stored in the `$experimentalFeatures` global variable (`Settings.wl`) and preserved across handler evaluation via `ChatState.wl`. Individual features are checked at runtime via `featureEnabledQ` (`Settings.wl`), which tests membership in the resolved list. The primary consumer is `resolvePromptGenerators` in `PromptGenerators/Common.wl`, which appends `"RelatedWolframAlphaResults"` and/or `"WebSearch"` to the active prompt generators list based on feature flags. Depends on `"WolframAlphaCAGEnabled"`, `"WebSearchRAGMethod"`, and `"PromptGenerators"` (declared in `$autoSettingKeyDependencies`). No model-specific overrides exist. Not exposed in the preferences UI. |
| `"OpenAIKey"` | `Automatic` | OpenAI API key. Legacy setting for direct OpenAI API authentication. |
| `"OpenAIAPICompletionURL"` | `"https://api.openai.com/v1/chat/completions"` | OpenAI API completion endpoint URL. Legacy setting for direct OpenAI API access. |

---

## Model-Specific Auto Settings

When a setting has a value of `Automatic`, the resolution pipeline checks `$modelAutoSettings` for a model-specific default. Settings are looked up in order of specificity:

1. Service + model name (e.g., `$modelAutoSettings["Anthropic", "Claude4"]`)
2. Service + model ID
3. Service + model family
4. Any service + model name (e.g., `$modelAutoSettings[Automatic, "GPT4Omni"]`)
5. Any service + model ID
6. Any service + model family
7. Service-level default (e.g., `$modelAutoSettings["Anthropic", Automatic]`)
8. Global default (`$modelAutoSettings[Automatic, Automatic]`)

The first match wins. For details on how to add support for new models, see [TODO: How to Add Support for New Models].

### Global Auto Setting Defaults

These are the fallback values from `$modelAutoSettings[Automatic, Automatic]` when no model-specific override exists:

| Setting | Default |
| ------- | ------- |
| `"AppendCitations"` | `False` |
| `"ConvertSystemRoleToUser"` | `False` |
| `"EndToken"` | `"/end"` |
| `"ExcludedBasePrompts"` | `{ParentList}` |
| `"PresencePenalty"` | `0.1` |
| `"ReplaceUnicodeCharacters"` | `False` |
| `"ShowProgressText"` | `True` |
| `"SplitToolResponseMessages"` | `False` |
| `"Temperature"` | `0.7` |
| `"ToolResponseRole"` | `"System"` |

### Non-Inherited Persona Values

The following settings are not inherited from the persona configuration when resolving settings. They retain their value from the notebook/cell scope:

- `"ChatDrivenNotebook"`
- `"CurrentPreferencesTab"`
- `"EnableLLMServices"`
- `"Icon"`
- `"InheritanceTest"`
- `"InitialChatCell"`
- `"LLMEvaluator"`
- `"PersonaFavorites"`
- `"ServiceDefaultModel"`
