# Settings Listing

## Overview

Chatbook settings control LLM behavior, prompt construction, tool usage, formatting, and UI behavior. Settings are stored in notebook tagging rules under `"ChatNotebookSettings"` and follow a hierarchical inheritance model.

### Accessing Settings

Use `CurrentChatSettings` to read and write settings:

```wl
(* Read global settings *)
CurrentChatSettings[]
CurrentChatSettings["Temperature"]

(* Read settings scoped to a notebook or cell *)
CurrentChatSettings[notebookObj]
CurrentChatSettings[cellObj, "Model"]

(* Write settings *)
CurrentChatSettings[$FrontEnd, "Temperature"] = 0.5
CurrentChatSettings[notebookObj, "AutoFormat"] = False

(* Reset to inherited value *)
CurrentChatSettings[notebookObj, "Temperature"] =.
```

### Inheritance Model

Settings resolve through a hierarchy, with more specific scopes overriding broader ones:

| Scope              | Description                      |
| ------------------ | -------------------------------- |
| `CellObject`       | Per-cell override                |
| `NotebookObject`   | Per-notebook settings            |
| `$FrontEndSession` | Session-wide (non-persistent)    |
| `$FrontEnd`        | Global persistent settings       |

If a setting is not defined at a given scope, it inherits from the next broader scope. A value of `Inherited` explicitly defers to the parent scope.

### Automatic Values

Many settings default to `Automatic`, meaning they are resolved at runtime based on the current model, service, and other settings. The resolution pipeline is defined in `Settings.wl` via `resolveAutoSettings`, which evaluates `Automatic` values in topologically sorted dependency order. Model-specific defaults are looked up from `$modelAutoSettings`.

---

## Model & Service

| Setting | Default | Description |
| ------- | ------- | ----------- |
| `"Model"` | `$DefaultModel` | The LLM model specification. |
| `"Authentication"` | `Automatic` | Authentication method for the LLM service. |
| `"EnableLLMServices"` | `Automatic` | Whether Chatbook uses the `LLMServices` framework for LLM communication. |
| `"Multimodal"` | `Automatic` | Whether multimodal (image) input is supported, controlling whether graphics and images in notebook cells are encoded and included in messages sent to the LLM. |
| `"Reasoning"` | `Automatic` | Whether model reasoning/chain-of-thought is enabled. |

### Additional Details

#### `"Model"`

The default is defined via `RuleDelayed` (`:>`) in `$defaultChatSettings` (`Settings.wl`), so `$DefaultModel` is evaluated lazily each time the setting is accessed. `$DefaultModel` (`Settings.wl`) returns `<\|"Service" -> "LLMKit", "Name" -> Automatic\|>` for Wolfram Engine 14.1+ and `<\|"Service" -> "OpenAI", "Name" -> "gpt-4o"\|>` for older versions. The value can be: (1) an `Association` with `"Service"` and `"Name"` keys; (2) a plain string (interpreted as an OpenAI model name by `serviceName` in `Models.wl`, and converted to `{"OpenAI", model}` by `makeLLMConfiguration` in `SendChat.wl`); (3) a `{service, name}` list (converted to an Association by `resolveFullModelSpec`). During `resolveAutoSettings` (`Settings.wl`), the model is resolved via `resolveFullModelSpec` (`Models.wl`), which: returns already-resolved models unchanged (checked via `"ResolvedModel" -> True`); converts `{service, name}` lists and plain strings to Associations; for LLMKit service with unspecified name, substitutes the actual backing service and model from `$defaultLLMKitService`/`$defaultLLMKitModelName` and sets `"Authentication" -> "LLMKit"`; for other services with `"Name" -> Automatic`, calls `chooseDefaultModelName` (`Models.wl`), which tries in order: the `$DefaultModel` name if the service matches, the service's registered `"DefaultModel"` property, the first model from the cached model list, or `Automatic` as fallback — if no string name can be resolved, it queries `getServiceModelList` and throws `$Canceled` if not connected. The resolved model is then passed through `standardizeModelData` (`Models.wl`), which enriches the Association with computed metadata: `"BaseID"`, `"BaseName"`, `"Family"` (from `modelNameData`), `"Date"`, `"DisplayName"`, `"FineTuned"`, `"Icon"`, `"Multimodal"`, `"Name"` (normalized via `toModelName`), `"Snapshot"`, and `"ResolvedModel" -> True`. The `toModelName` function normalizes model name strings (e.g., CamelCase to lowercase with hyphens, "ChatGPT" to "gpt-3.5-turbo"). Model is resolved first in the topological sort order (`$autoSettingKeyPriority` explicitly prepends `"Model"`). Many settings depend on the resolved Model via `$autoSettingKeyDependencies`: `"Authentication"`, `"ForceSynchronous"`, `"HybridToolMethod"`, `"MaxCellStringLength"`, `"MaxContextTokens"`, `"MaxTokens"`, `"Multimodal"`, `"TokenizerName"`, `"ToolCallExamplePromptStyle"`, `"ToolCallRetryMessage"`, `"ToolExamplePrompt"`, and `"ToolsEnabled"`. The service name is extracted from the model via `serviceName` (`Models.wl`), which checks for a `"Service"` key in the model Association, with `"OpenAI"` as the default for plain strings. In `$llmConfigPassedKeys` (`SendChat.wl`), so it is passed through `LLMConfiguration` to the LLM service. Listed in `$popOutSettings` (`ConvertChatNotebook.wl`) as one of four settings shown during chat notebook conversion. Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. The `$modelAutoSettings` table (`Settings.wl`) does not contain overrides for the Model setting itself; rather, the resolved model's service and name/family are used as lookup keys to resolve other settings. Exposed in the preferences UI (`PreferencesContent.wl`) in both the "Notebooks" tab and the "Services" tab via `makeModelSelector`, which provides a service selector popup menu and a model name selector popup menu (or text input). The UI reads and writes `CurrentChatSettings[$preferencesScope, "Model"]` as an Association. The `"ServiceDefaultModel"` setting is used alongside to remember the last-selected model per service across service switches. Also accessible via `SetModel` (`Models.wl`), which accepts a string model name or Association and writes to the notebook's `TaggingRules`, optionally updating `System`$LLMEvaluator` via `LLMConfiguration`.

#### `"Authentication"`

When `Automatic`, resolves based on the model specification: if the model has an explicit `"Authentication"` field, that value is used; if the model's `"Service"` is `"LLMKit"`, resolves to `"LLMKit"`; otherwise remains `Automatic` (uses the service's default authentication). Depends on `"Model"`. Passed directly to `LLMServices`Chat` and `LLMServices`ChatSubmit` (not via `LLMConfiguration`).

#### `"EnableLLMServices"`

When `Automatic`, resolves to the internal `$useLLMServices` flag, which evaluates to `True` only if `$enableLLMServices` is `Automatic` or `True` AND the `Wolfram/LLMFunctions` paclet (version 1.2.2+) is installed (`Services.wl`). When `True`, Chatbook routes all chat requests through `LLMServices`Chat`/`LLMServices`ChatSubmit`, the OpenAI completion URL input is hidden from the preferences UI (`PreferencesContent.wl`), and available services are discovered dynamically. When `False`, Chatbook falls back to direct API calls using legacy service configuration with `$fallBackServices`, and the OpenAI completion URL input is shown in the preferences UI. The setting value is read from `CurrentChatSettings` and assigned to the `$enableLLMServices` variable in `Actions.wl` before each `sendChat` call. The `sendChat` function in `SendChat.wl` has a condition `/; $useLLMServices` that gates the primary chat execution path. Other settings depend on this: `HandlerFunctionsKeys` depends on `EnableLLMServices` for resolution order, and `Multimodal` depends on both `EnableLLMServices` and `Model` (when LLM Services are disabled but the model supports multimodal, multimodal is enabled directly; when enabled, it additionally checks for multimodal paclet availability). This is a non-inherited persona value (listed in `$nonInheritedPersonaValues` in `Settings.wl`), meaning it retains its value from the notebook/cell scope rather than inheriting from the persona. No model-specific overrides exist. Not exposed directly in the preferences UI.

#### `"Multimodal"`

When `Automatic`, resolved by `multimodalQ` (`Settings.wl`), which evaluates three factors: (1) whether the model supports multimodal input (via `multimodalModelQ` in `Models.wl`), (2) whether LLMServices is enabled (`EnableLLMServices`), and (3) whether required paclets are available. The resolution logic: if the model does not support multimodal, returns `False`; if the model supports multimodal and `EnableLLMServices` is `False`, returns `True` (direct API path needs no extra paclets); if the model supports multimodal and `EnableLLMServices` is `True`, returns `multimodalPacletsAvailable[]`, which checks that `Wolfram/LLMFunctions` version 1.2.4+ and `ServiceConnection_OpenAI` version 13.3.18+ (with multimodal support) are installed. The `multimodalModelQ` function (`Models.wl`) determines model capability by: checking for an explicit `"Multimodal"` key in the resolved model Association (set during `standardizeModelData`, which adds `"Multimodal" -> multimodalModelQ @ model` to every resolved model); matching known model name patterns (Claude 3+, GPT-4o/GPT-4o-mini/ChatGPT-4o, GPT-4-turbo with date suffix); or detecting "vision" in the normalized model name. Model-specific overrides in `$modelAutoSettings`: `True` for Claude 4, Gemini 2, Gemini 3, GPT-4.1, GPT-5, O1, O3, and O4-Mini; `False` for O1-Mini and O3-Mini. Depends on `"EnableLLMServices"` and `"Model"` (declared in `$autoSettingKeyDependencies`). The resolved value is stored in the `$multimodalMessages` global variable (`CommonSymbols.wl`) at three points in `SendChat.wl` (lines 90, 176, 298) and in `makeChatMessages` (`ChatMessages.wl`), and is preserved across handler evaluation via `ChatState.wl`. When `True`, `makeMessageContent` (`ChatMessages.wl`) processes cell content through `expandMultimodalString`, which splits strings on expression URI patterns, calls `inferMultimodalTypes` to classify content as `"Text"` or `"Image"`, and produces multimodal message content (with image data). Image inclusion is further gated by `allowedMultimodalRoles` (`ChatMessages.wl`), which restricts multimodal content to `"User"` role messages for GPT-4o models and allows `All` roles for other models. Images are resized via `resizeMultimodalImage` (`ChatMessages.wl`) to fit within `$maxMMImageSize` dimensions before encoding. In `Serialization.wl`, the related `$multimodalImages` variable (derived from `$contentTypes`) controls whether graphics boxes are encoded as image URIs (`toMarkdownImageBox`) or replaced with `"[GRAPHIC]"` placeholders, and whether `"Picture"` style cells are serialized as image URIs. Graphics exceeding `$maxBoxSizeForImages` bytes fall back to non-multimodal serialization. Chat modes override multimodal behavior: `ContentSuggestions` mode sets its own `$wlSuggestionsMultimodal`, `$textSuggestionsMultimodal`, and `$notebookSuggestionsMultimodal` flags (all `False`); `ChatTitle.wl` uses `$multimodalTitleContext = False` for title generation. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook for message content processing). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Exposed in the preferences UI (`PreferencesContent.wl`) under the "Features" section as a PopupMenu with three options: `Automatic` ("Enabled by Model"), `True` ("Enabled Always"), and `False` ("Enabled Never"), reading and writing `CurrentChatSettings[$preferencesScope, "Multimodal"]`.

#### `"Reasoning"`

Model-specific; only supported by certain models (e.g., O-series, GPT-5). Models that don't support it return `Missing["NotSupported"]`.

## LLM Parameters

| Setting | Default | Description |
| ------- | ------- | ----------- |
| `"Temperature"` | `Automatic` | Sampling temperature for the LLM, controlling randomness in token selection. Higher values produce more varied output; lower values produce more deterministic output. When `Automatic`, resolved via `$modelAutoSettings` lookup (no custom `resolveAutoSetting0` handler exists). The global auto default is `0.7` (`$modelAutoSettings[Automatic, Automatic]`). Model-specific overrides in `$modelAutoSettings`: `Missing["NotSupported"]` for GPT-5 (family-level via `$modelAutoSettings[Automatic, "GPT5"]`, inherited by GPT-5.1/GPT-5.2) and O4-Mini (`$modelAutoSettings[Automatic, "O4Mini"]`). All other models inherit the global default of `0.7`. Accepts numeric values in the range `0` to `2` (declared as `Restricted["Number", {0, 2}]` in `ChatPreferences.wl`). Included in `$llmConfigPassedKeys` (`SendChat.wl`), so it IS passed through `LLMConfiguration` to the LLM service when using the LLMServices framework (`LLMServices`Chat`/`LLMServices`ChatSubmit`); when `Automatic`, the resolved value (either `0.7` or `Missing["NotSupported"]`) is what gets passed. `Missing` values are stripped by `DeleteMissing` in `makeLLMConfiguration`, and unsupported parameters are dropped by `dropModelUnsupportedParameters` (which checks `autoModelSetting` for `Missing["NotSupported"]`). Also used in the legacy (non-LLMServices) HTTP request path: in `makeHTTPRequest` (`SendChat.wl`), the value is looked up from settings via `Lookup[settings, "Temperature", 0.7]` and passed to the OpenAI API as the `"temperature"` field in the request body; values of `Automatic` or `Missing` are stripped from the request via `DeleteCases`. No dependencies in `$autoSettingKeyDependencies`. Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not listed in `$popOutSettings`. Exposed in the preferences UI via `makeTemperatureInput` (`PreferencesContent.wl`), which provides a numeric input field under the "Notebooks" section, and via `makeTemperatureSlider` (`UI.wl`), which provides a slider (range `0` to `2`, step `0.01`) with a "Choose automatically" checkbox option in the advanced settings UI; when auto is selected, the value is set to `Inherited`. |
| `"FrequencyPenalty"` | `0.1` | Penalty applied to tokens based on their frequency in the text so far. Reduces repetition. Default is a fixed numeric value (`0.1`), not `Automatic`. Only used in the legacy (non-LLMServices) HTTP request path: in `makeHTTPRequest` (`SendChat.wl`), the value is looked up from settings via `Lookup[settings, "FrequencyPenalty", 0.1]` and passed to the OpenAI API as the `"frequency_penalty"` field in the request body. Values of `Automatic` or `Missing` are stripped from the request via `DeleteCases`. Not included in `$llmConfigPassedKeys` (`SendChat.wl`), so it is NOT passed through `LLMConfiguration` when using the LLMServices framework (`LLMServices`Chat`/`LLMServices`ChatSubmit`). No model-specific overrides exist in `$modelAutoSettings`. No dependencies on other settings. Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not exposed in the preferences UI. |
| `"PresencePenalty"` | `Automatic` | Penalty applied to tokens based on whether they have appeared in the text so far, encouraging the model to introduce new topics. When `Automatic`, resolved via `$modelAutoSettings` lookup (no custom `resolveAutoSetting0` handler exists). The global auto default is `0.1` (`$modelAutoSettings[Automatic, Automatic]`). Model-specific overrides in `$modelAutoSettings`: `Missing["NotSupported"]` for Google Gemini (service-level default via `$modelAutoSettings["GoogleGemini", Automatic]`), GPT-5 (family-level via `$modelAutoSettings[Automatic, "GPT5"]`), and O4-Mini (`$modelAutoSettings[Automatic, "O4Mini"]`). All other models inherit the global default of `0.1`. Included in `$llmConfigPassedKeys` (`SendChat.wl`), so it IS passed through `LLMConfiguration` to the LLM service when using the LLMServices framework (`LLMServices`Chat`/`LLMServices`ChatSubmit`); when `Automatic`, the resolved value (either `0.1` or `Missing["NotSupported"]`) is what gets passed, and `Missing` values are stripped by `DeleteCases` in `makeLLMConfiguration`. Also used in the legacy (non-LLMServices) HTTP request path: in `makeHTTPRequest` (`SendChat.wl`), the value is looked up from settings via `Lookup[settings, "PresencePenalty", 0.1]` and passed to the OpenAI API as the `"presence_penalty"` field in the request body; values of `Automatic` or `Missing` are stripped from the request via `DeleteCases`. No dependencies in `$autoSettingKeyDependencies`. Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not listed in `$popOutSettings`. Not exposed in the preferences UI. |
| `"TopP"` | `1` | Top-p (nucleus) sampling parameter, controlling the cumulative probability threshold for token selection. A value of `1` considers all tokens; lower values restrict sampling to the smallest set of tokens whose cumulative probability exceeds the threshold, reducing diversity. Default is a fixed numeric value (`1`), not `Automatic`. Only used in the legacy (non-LLMServices) HTTP request path: in `makeHTTPRequest` (`SendChat.wl`), the value is looked up from settings via `Lookup[settings, "TopP", 1]` and passed to the OpenAI API as the `"top_p"` field in the request body. Values of `Automatic` or `Missing` are stripped from the request via `DeleteCases`. Not included in `$llmConfigPassedKeys` (`SendChat.wl`), so it is NOT passed through `LLMConfiguration` when using the LLMServices framework (`LLMServices`Chat`/`LLMServices`ChatSubmit`). No model-specific overrides exist in `$modelAutoSettings`. No dependencies on other settings. Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not listed in `$popOutSettings`. Not exposed in the preferences UI or the ChatPreferences tool. |
| `"MaxTokens"` | `Automatic` | Maximum number of tokens the LLM may generate in its response (output token limit). When `Automatic`, resolved by `autoMaxTokens` (`Settings.wl`), which looks up the model name in `$maxTokensTable`. Only two legacy models have explicit entries: `"gpt-4-vision-preview"` and `"gpt-4-1106-preview"` (both `4096`); all other models resolve to `Automatic`, deferring to the LLM service's own default output limit. No model-specific overrides exist in `$modelAutoSettings`. Depends on `"Model"` (declared in `$autoSettingKeyDependencies`). In the LLMServices path (the primary/modern code path), the resolved value is passed through `LLMConfiguration` via `$llmConfigPassedKeys` (`SendChat.wl`); when `Automatic`, the underlying `LLMConfiguration`/service determines the appropriate limit. In the legacy HTTP request path (`makeHTTPRequest` in `SendChat.wl`), the value is placed in the JSON request body as `"max_tokens"`; if `Automatic`, it is stripped by `DeleteCases[..., Automatic\|_Missing]`, allowing the API's own default to apply. This setting controls the **output** token limit and is distinct from `"MaxContextTokens"`, which controls the **input** context window size for token budgeting. Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. No persona-level overrides exist in any built-in persona. Not exposed in the preferences UI. |
| `"MaxContextTokens"` | `Automatic` | Maximum token capacity of the context window, used internally for token budgeting and context management. When `Automatic`, resolved by `autoMaxContextTokens` (`Settings.wl`), which dispatches based on the model's service and name. For Ollama services, the context length is queried dynamically via `ServiceExecute[service, "ModelContextLength", ...]` and cached; for LLMKit services, the value is capped at `2^16` (65,536). For other services, resolution first checks `$modelAutoSettings` for an exact model match, then falls back to `autoMaxContextTokens0`, which pattern-matches against tokenized model name components (e.g., `{"claude", "3", ...}` -> `200000`, `{"gpt", "4o", ...}` -> `131072`, `{"gemini", ..., "pro", ...}` -> `30720`). The ultimate fallback for unrecognized models is `2^12` (4,096). Model-specific values in `$modelAutoSettings`: `200000` for Claude 3/4, `128000` for GPT-4o, `1047576` for Gemini 2+/GPT-4.1, `400000` for GPT-5, `64000` for O1-Mini (halved for reasoning token headroom), `100000` for O1/O3/O3-Mini/O4-Mini. Depends on `"Authentication"` and `"Model"` (declared in `$autoSettingKeyDependencies`). The resolved value is used to initialize the token budget in `makeTokenBudget` (`ChatMessages.wl`), which multiplies it by `TokenBudgetMultiplier` to produce the working budget. During message construction, token pressure is tracked as `1.0 - ($tokenBudget / MaxContextTokens)`, and the cell string budget (`$cellStringBudget`) is dynamically reduced as pressure increases, dropping to `0` when fewer than `$reservedTokens` (500) remain. The `MaxCellStringLength` setting depends on `MaxContextTokens`: `chooseMaxCellStringLength` (`Settings.wl`) scales the character limit proportionally to the context window via `Min[Ceiling[$defaultMaxCellStringLength * tokens / 2^14], 2^14]`. `MaxOutputCellStringLength` in turn depends on the resolved `MaxCellStringLength`. Chat modes override this value: Context mode scales it to 25% via `$notebookContextLimitScale` (`ChatModes/Context.wl`); NotebookAssistance mode fixes it at `2^15` (32,768); ContentSuggestions mode selects per content type (`2^12` for WL/Text, `2^15` for Notebook). Also used by `modelContextStringLimit` (`LLMUtilities.wl`), which converts to a character limit via `tokens * 2` (with fallback to 8,000). Listed in `$popOutSettings` (`ConvertChatNotebook.wl`) as one of four settings shown during chat notebook conversion. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook for context management). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not exposed in the preferences UI. |
| `"Reasoning"` | `Automatic` | Controls the reasoning/thinking effort level for models that support extended thinking. When `Automatic`, resolved via `$modelAutoSettings` lookup (no custom `resolveAutoSetting0` handler exists). No global auto default exists in `$modelAutoSettings[Automatic, Automatic]`, so for most models `Automatic` is left as-is and stripped by `DeleteMissing` in `makeLLMConfiguration`. Model-specific overrides in `$modelAutoSettings`: for GPT-5, resolves to `"Minimal"` (if `$gpt5Reasoning` is `True`, i.e., the `Wolfram/LLMFunctions` paclet is newer than version `2.2.4`) or `Missing["NotSupported"]` otherwise; for GPT-5.1, resolves to `"None"` (under the same paclet version condition) or `Missing["NotSupported"]`. No other model families define a `"Reasoning"` override. Included in `$llmConfigPassedKeys` (`SendChat.wl`), so it IS passed through `LLMConfiguration` to the LLM service when using the LLMServices framework; `Missing` values are stripped by `DeleteMissing` in `makeLLMConfiguration`, and unsupported parameters are dropped by `dropModelUnsupportedParameters` (which checks `autoModelSetting` for `Missing["NotSupported"]`). Not used in the legacy HTTP request path. No dependencies in `$autoSettingKeyDependencies`. Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not listed in `$popOutSettings`. Not exposed in the preferences UI. |
| `"StopTokens"` | `Automatic` | Stop sequences that signal the LLM to stop generating. When `Automatic`, resolved by `autoStopTokens` (`Settings.wl`), which computes the stop token list based on the current `"ToolMethod"`, `"ToolCallExamplePromptStyle"`, and whether `$AutomaticAssistance` is enabled. Resolution has three branches: (1) if already set to `Missing["NotSupported"]` (from model-specific settings), returns `Missing["NotSupported"]`; (2) if `"ToolsEnabled"` is `False`, returns `{ "[INFO]" }` when `$AutomaticAssistance` is `True`, otherwise `None`; (3) otherwise, combines results from `methodStopTokens` and `styleStopTokens`, plus `"[INFO]"` if `$AutomaticAssistance` is enabled, deduplicating and returning `None` if empty. `methodStopTokens` dispatches on `"ToolMethod"`: `"Simple"` -> `{ "\n/exec", <EndToken> }`, `"Service"` -> `{ <EndToken> }`, `"Textual"\|"JSON"` -> `{ "ENDTOOLCALL", <EndToken> }`, other -> `{ "ENDTOOLCALL", "\n/exec", <EndToken> }` (non-string values like `None` are filtered out via `Select[..., StringQ]`). `styleStopTokens` dispatches on `"ToolCallExamplePromptStyle"`: `"Phi"` -> `{ "<\|user\|>", "<\|assistant\|>" }`, `"Llama"` -> `{ "<\|start_header_id\|>" }`, `"Gemma"` -> `{ "<start_of_turn>" }`, `"Nemotron"` -> `{ "<extra_id_0>", "<extra_id_1>" }`, `"DeepSeekCoder"` -> `{ "<\:ff5cbegin\:2581of\:2581sentence\:ff5c>" }`, other strings or `None` -> `{ }`. Model-specific overrides in `$modelAutoSettings`: `Missing["NotSupported"]` for GPT-5/GPT-5.1/GPT-5.2 (family-level via `$modelAutoSettings[Automatic, "GPT5"]`, inherited by GPT-5.1/GPT-5.2), O3 (`$modelAutoSettings[Automatic, "O3"]`), and O4-Mini (`$modelAutoSettings[Automatic, "O4Mini"]`). Accepts `Automatic`, `None`, `{ }`, `{ __String }` (list of strings), or `_Missing`. In the legacy HTTP request path (`makeHTTPRequest` in `SendChat.wl`), `makeStopTokens` converts the resolved value to an API-ready format: `None\|{ }\|_Missing` -> `Missing[]` (stripped from request), `{ __String }` -> passed as-is to the `"stop"` field. In the LLMServices path (`makeLLMConfiguration` in `SendChat.wl`), `makeStopTokens` is called explicitly and passed as `"StopTokens"` in the `LLMConfiguration`; `Missing` values are removed by `DeleteMissing`. Note: `"StopTokens"` is NOT in `$llmConfigPassedKeys` — it is handled separately via explicit `makeStopTokens` calls in both `makeLLMConfiguration` branches. After response generation, `trimStopTokens` (`SendChat.wl`) removes any stop token found at the end of the response text from both `"FullContent"` and `"DynamicContent"` via `StringDelete[..., stop ~~ EndOfString]`. Not in `$autoSettingKeyDependencies` (but implicitly depends on `"ToolMethod"`, `"ToolCallExamplePromptStyle"`, `"ToolsEnabled"`, and `"EndToken"` — resolved after `"ToolMethod"` in `resolveAutoSettings0`). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not listed in `$popOutSettings`. Not exposed in the preferences UI. |
| `"TokenBudgetMultiplier"` | `Automatic` | Multiplier applied to `MaxContextTokens` to produce the working token budget for chat message construction. When `Automatic`, resolved by `resolveAutoSetting0` (`Settings.wl`) to `1` (no scaling). The token budget is computed by `makeTokenBudget` (`ChatMessages.wl`): `Ceiling[MaxContextTokens * TokenBudgetMultiplier]`. When set to an unspecified value (`Missing`, `Automatic`, or `Inherited`), the token budget equals `MaxContextTokens` unchanged. This budget is assigned to `$tokenBudget` in `makeChatMessages` (`ChatMessages.wl`) and is decremented as each message is added to the context; when a message's token count exceeds the remaining budget, it is truncated via `cutMessageContent`. Token pressure is tracked as `1.0 - ($tokenBudget / MaxContextTokens)`, and the cell string budget (`$cellStringBudget`) dynamically decreases as pressure rises, dropping to `0` when fewer than `$reservedTokens` (500) tokens remain. Accepts any non-negative numeric value (`$$size` pattern: non-negative `_Integer` or `_Real`, or `Infinity`), as well as `Automatic`. Setting a value less than `1` (e.g., `0.8`) restricts the effective context to a fraction of the model's full window; values greater than `1` allow exceeding the nominal context limit. No model-specific overrides exist in `$modelAutoSettings`. No dependencies in `$autoSettingKeyDependencies`. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook for token budget management). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not listed in `$popOutSettings`. Not exposed in the preferences UI. |

## Chat Behavior

| Setting | Default | Description |
| ------- | ------- | ----------- |
| `"IncludeHistory"` | `Automatic` | Whether to include chat history (preceding cells) in the context sent to the LLM. When `Automatic`, resolves to `Automatic` (no further resolution via `resolveAutoSetting0`). In practice, `Automatic` behaves the same as `True`: the `If[ ! settings["IncludeHistory"], cells = { evalCell } ]` check in `sendChat` (`SendChat.wl`) does not trigger because `! Automatic` evaluates to `Not[Automatic]` (not `True`), so the full cell list is passed to `constructMessages`. When explicitly `True`, the same behavior applies. When `False`, only the evaluation cell itself is included — no preceding chat history. The `selectChatCells` function (`SendChat.wl`) first selects candidate cells (up to `ChatHistoryLength` cells, bounded by chat delimiter cells), and then the `IncludeHistory` check decides whether to keep those cells or replace them with just the current cell. In `ChatHistory.wl`, `selectChatHistoryCells` dispatches on the setting value: `False` returns only the current cell; any other value (including `Automatic`) applies the `ChatHistoryLength` limit to the full cell list. Cell style overrides: cells with the `"SideChat"` style automatically set `IncludeHistory` to `False` via `addCellStyleSettings` in `Actions.wl`. Exposed in the notebook preferences UI as an "Include chat history" checkbox under the "Chat Notebook Cells" section (`PreferencesContent.wl`), where both `True` and `Automatic` display as checked. No dependencies in `$autoSettingKeyDependencies`. No model-specific overrides exist in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed to the LLM service). Not in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. |
| `"ChatHistoryLength"` | `1000` | Maximum number of chat cells to include in the context. When sending a chat message, the system selects cells starting from the current cell and looking backwards, limited to this count. Used in two paths: `selectChatCells` in `SendChat.wl` (which sets `$maxChatCells` from this value and applies `Take[..., UpTo @ $maxChatCells]` on the filtered cell list) and `selectChatHistoryCells` in `ChatHistory.wl` (which applies the same kind of limit on cell information entries). When the value is not a positive integer, falls back to the default `$maxChatCells`. Exposed in the notebook preferences UI as a numeric input field under the "Notebooks" section. Note: this is a cell count limit, not a token limit; token budgeting is handled separately by `"MaxContextTokens"` and `"TokenBudgetMultiplier"`. |
| `"MergeMessages"` | `True` | Whether to merge consecutive messages with the same role into a single message. When `True`, the `mergeMessageData` function in `ChatMessages.wl` groups consecutive non-system messages by role (via `SplitBy[messages, Lookup["Role"]]`) and concatenates their text content into a single message per group. System messages at the start of the message list are always kept separate (not merged). The merge process also applies `mergeCodeBlocks`, which combines adjacent code blocks of the same language (e.g., two consecutive `` ```wl `` blocks) into a single code block, preventing fragmentation when multiple cells are combined. The check uses `TrueQ`, so only an explicit `True` triggers merging; `False`, `Automatic`, or any other value leaves messages unmerged. Merging is applied in `augmentChatMessages` (`ChatMessages.wl`) after message construction but before prompt generator augmentation. No dependencies in `$autoSettingKeyDependencies`. No model-specific overrides exist in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook for message preprocessing). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Exposed in the notebook preferences UI as a "Merge chat messages" checkbox under the "Notebooks" section (`PreferencesContent.wl`), with tooltip: "If enabled, adjacent cells with the same author will be merged into a single chat message." Also exposed via the ChatPreferences tool as a boolean parameter. |
| `"MaxCellStringLength"` | `Automatic` | Maximum string length for cell content included in the LLM context. When `Automatic`, resolved by `chooseMaxCellStringLength` (`Settings.wl`): if `MaxContextTokens` is `Infinity`, returns `Infinity`; otherwise computes `Min[Ceiling[$defaultMaxCellStringLength * tokens / 2^14], 2^14]`, where `$defaultMaxCellStringLength` is `10000` (`Serialization.wl`) and `tokens` is the resolved `MaxContextTokens` value. This scales the character limit proportionally to the model's context window, capping at 16,384 characters. Depends on `"Model"` and `"MaxContextTokens"` (declared in `$autoSettingKeyDependencies`). The resolved value is used as the initial `$cellStringBudget` in `makeChatMessages` (`ChatMessages.wl`), which dynamically decreases the budget as messages are added based on token pressure: `$cellStringBudget = Ceiling[(1 - $tokenPressure) * $initialCellStringBudget]`, dropping to `0` when the remaining token budget falls below `$reservedTokens` (500). Each cell is serialized via `CellToString` with `"MaxCellStringLength" -> $cellStringBudget` (`ChatMessages.wl`), which truncates cell content exceeding this limit. Chat modes override this value: the Context chat mode lists it in `$downScaledSettings` (scaled down for context queries), and ContentSuggestions overrides it to `1000`. The `"MaxOutputCellStringLength"` setting depends on the resolved `MaxCellStringLength` value. Exposed in the ChatPreferences tool as an integer parameter (where `0` means determine automatically). No model-specific overrides exist in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed to the LLM service). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not exposed in the preferences UI. |
| `"MaxOutputCellStringLength"` | `Automatic` | Maximum string length for output cell content included in the LLM context. Controls how aggressively output cells (styles matching `"Output"`, `"Print"`, `"Echo"`) are truncated when serialized via `CellToString` (`Serialization.wl`). When `Automatic`, resolved by `chooseMaxOutputCellStringLength` (`Settings.wl`): computes `Min[Ceiling[MaxCellStringLength / 10], 1000]`, producing a value that is one-tenth of the resolved `MaxCellStringLength`, capped at 1,000 characters. The fallback constant `$defaultMaxOutputCellStringLength` (`Serialization.wl`) is `500`, used when the option value is `Automatic` at the `CellToString` level. Depends on `"MaxCellStringLength"` (declared in `$autoSettingKeyDependencies`). The resolved value flows through to `CellToString` as the `"MaxOutputCellStringLength"` option; within `CellToString`, it is assigned to the `$maxOutputCellStringLength` dynamic variable (`Serialization.wl`), which controls two behaviors: (1) **Output cell truncation**: cells matching `$$outputStyle` (`"Output"`, `"Print"`, `"Echo"`) are wrapped through `truncateString` after serialization, which calls `stringTrimMiddle[str, $maxOutputCellStringLength]` to trim strings exceeding the limit by replacing the middle with an elision marker. (2) **Graphics serialization threshold**: for graphics boxes, `ByteCount @ box < $maxOutputCellStringLength` determines whether a small graphics expression is serialized as an `InputForm` string (below threshold) or replaced with a placeholder like `"[GRAPHIC]"` (above threshold). Additionally, `truncateString` is used throughout `Serialization.wl` for other content (e.g., `InputForm` strings, compressed data, export packets), always defaulting to `$maxOutputCellStringLength` when no explicit size is given. Special local overrides within `Serialization.wl`: `GridBox` table rendering temporarily sets `$maxOutputCellStringLength = 2*$cellPageWidth` for table cell content; `getExamplesString` (documentation example extraction) temporarily sets it to `100` for compact example summaries; cells matching `$$noTruncateStyle` (`"AlphabeticalListing"`) temporarily set it to `Infinity` to disable truncation entirely. Chat mode overrides: Context mode scales this to 25% via `$notebookContextLimitScale` (applied by `downScaledSettings` in `Context.wl`); ContentSuggestions mode fixes it at `200` via `$contentSuggestionsOverrides`. The `NotebookChunking` prompt generator uses its own independent variable `$maxChunkOutputCellStringLength = 500` (passed directly to `CellToString` as the `"MaxOutputCellStringLength"` option in `NotebookChunking.wl`). No model-specific overrides exist in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook for serialization). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not exposed in the preferences UI. |
| `"ForceSynchronous"` | `Automatic` | Whether to force synchronous (non-streaming) chat requests. When `Automatic`, resolves via `forceSynchronousQ` (`Settings.wl`), which returns `True` if the model's service is `"GoogleGemini"` (since Google Gemini uses a non-streaming API by default) and `False` otherwise. Model-specific overrides in `$modelAutoSettings` set this to `True` for O1, O3, and O4-Mini (OpenAI reasoning models that do not support streaming), and explicitly to `False` for O1-Mini, Gemini 2, and Gemini 3 (which do support streaming, overriding the service-level default for Gemini). When `True`, `chatSubmit0` in `SendChat.wl` uses the synchronous `LLMServices`Chat` function instead of the streaming `LLMServices`ChatSubmit`, waits for the complete response before writing output, sets progress display to `"WaitingForResponse"`, and returns `None` instead of a `TaskObject`. Additionally, when `ForceSynchronous` is `True`, `$showProgressText` is forced to `True` in `resolveAutoSettings` (`Settings.wl`) regardless of the `ShowProgressText` setting. Depends on `"Model"`. The `"BypassResponseChecking"` setting depends on this: when `ForceSynchronous` is `True`, `BypassResponseChecking` also resolves to `True`, skipping HTTP status code validation, empty response detection, and JSON error parsing. Not exposed in the preferences UI. |
| `"TimeConstraint"` | `Automatic` | Time limit (in seconds) for the overall chat task evaluation. When a positive numeric value is given, `waitForLastTask` in `Actions.wl` wraps the task-waiting step in `TimeConstrained[waitForLastTask[$lastTask], timeConstraint, StopChat[]]`, so if the chat task exceeds the specified number of seconds, `StopChat[]` is called to abort the evaluation. When `Automatic` (or any non-positive value), no time constraint is applied and the chat task runs until completion. The check uses `TrueQ @ Positive @ timeConstraint`, so only explicitly positive numeric values (e.g., `30`, `60`, `120`) activate the constraint; `Automatic`, `Infinity`, `False`, and other non-positive values all result in no time limit. Note: this setting controls the overall chat task timeout, not individual tool evaluation timeouts (those are controlled separately by tool-specific options such as `"EvaluationTimeConstraint"` and `"PingTimeConstraint"` in `Tools/Common.wl`). No dependencies in `$autoSettingKeyDependencies`. No model-specific overrides exist in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook for task management). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not exposed in the notebook preferences UI or the ChatPreferences tool. |
| `"ConvertSystemRoleToUser"` | `Automatic` | Whether to convert system-role messages to user-role messages. Model default: `False`. Required for some models (e.g., O1-Mini). |
| `"ReplaceUnicodeCharacters"` | `Automatic` | Whether to replace Wolfram Language special characters (Unicode private use area codepoints such as `\[FreeformPrompt]`) with ASCII equivalents before sending messages to the LLM. When `True`, the `replaceUnicodeCharacters` function in `SendChat.wl` performs string replacements on all message content, including strings inside `LLMTool`, `LLMToolRequest`, and `LLMToolResponse` expressions. The replacement is applied in the message preparation pipeline (after role rewriting, before tool response splitting). When `Automatic`, resolves via `autoModelSetting` to the model-specific default. Model default: `False` (from `$modelAutoSettings[Automatic, Automatic]`). Model-specific overrides: `True` for all Anthropic models (service-level default in `$modelAutoSettings["Anthropic", Automatic]`) and GPT-5.2 (`$modelAutoSettings[Automatic, "GPT52"]`). The check uses `TrueQ`, so only an explicit `True` triggers replacement; `False`, `Automatic`, or any other value leaves messages unchanged. No dependencies in `$autoSettingKeyDependencies`. Not in `$llmConfigPassedKeys` (used internally by Chatbook for message preprocessing, not passed to the LLM service). Not exposed in the preferences UI. |
| `"BypassResponseChecking"` | `Automatic` | Whether to bypass response validation after receiving an LLM response. When `True`, the response is immediately written as a formatted output cell without validating the HTTP status code, checking for empty responses, or extracting error data from the response body. When `False`, the response goes through full validation: the debug log is processed to extract body chunks, status codes are checked (non-200 responses trigger error cells), empty responses are detected, and JSON error data is parsed before writing output. Resolves to `True` when `ForceSynchronous` is `True`, `False` otherwise. Depends on `"ForceSynchronous"`. |
| `"Assistance"` | `Automatic` | Whether automatic assistance mode is enabled. When `Automatic`, resolves to `False`. When `True`, LLM responses are processed immediately rather than being queued for user approval, output cells use `"AssistantOutput"` styles instead of `"ChatOutput"`, and certain tools are disabled (WolframLanguageEvaluator, CreateNotebook, WolframAlpha). Controlled in the notebook preferences UI as "Enable automatic assistance". |

## Prompting

| Setting | Default | Description |
| ------- | ------- | ----------- |
| `"BasePrompt"` | `Automatic` | Specifies which base prompt components to include in the system prompt. Can be `Automatic` (inherited from persona/model settings), `None` (disables all base prompting, as used by the RawModel persona), a single component name string, or a list of component names. Lists can include `ParentList` to inherit from the parent scope while adding additional components (e.g., `{ParentList, "Notebooks", "WolframLanguageStyle"}`). Available components are defined in `Prompting.wl` via `$basePromptComponents` and `$basePromptOrder`, and include individual components (e.g., `"Markdown"`, `"CodeBlocks"`, `"MathExpressions"`, `"EscapedCharacters"`, `"WolframLanguageStyle"`, `"EndTurnToken"`) as well as class names that expand to groups of components (e.g., `"Notebooks"`, `"WolframLanguage"`, `"Math"`, `"Formatting"`, `"All"`). Dependencies between components are automatically resolved via `$basePromptDependencies`. Part of `$modelInheritedLists`, which enables special list-merging behavior with `ParentList`. Interacts with `ExcludedBasePrompts`, which removes specified components from the resolved list. Personas typically set this to include `ParentList` plus persona-specific components (e.g., CodeAssistant uses `{ParentList, "Notebooks", "WolframLanguageStyle"}`). |
| `"ExcludedBasePrompts"` | `Automatic` | List of base prompt component names to exclude from the system prompt. When `Automatic`, resolves to `{ParentList}` via the global model auto default, meaning it inherits exclusions from the parent model settings. Can be a list containing strings (component names or class names) and/or `ParentList` for inheritance. Valid values must match `{ (_String\|ParentList)... }`; invalid values trigger an `"InvalidExcludedBasePrompts"` failure. Applied after the `"BasePrompt"` list is resolved: in `augmentChatMessages` (`ChatMessages.wl`), the resolved `BasePrompt` list is filtered via `DeleteCases` to remove any components matching the exclusion list. Additionally, the excluded components are removed from the collected prompt components via `removeBasePrompt` in `Prompting.wl`, which drops matching keys from `$collectedPromptComponents` and strips the corresponding text from the system message content. The resolved value (with `ParentList` entries removed) is stored in the `$excludedBasePrompts` global variable (`Settings.wl`), which is also checked by `needsBasePrompt` (`Prompting.wl`) to prevent excluded components from being collected during prompt construction. Part of `$modelInheritedLists` (along with `"BasePrompt"`), which enables special list-merging behavior in `inheritModelSettings`: when the value contains `ParentList`, it is merged with the model-specific default via `mergeChatSettings`, allowing syntax like `{ParentList, "EscapedCharacters"}` to mean "inherit parent exclusions and also exclude EscapedCharacters." Model-specific overrides: GPT-5.2 uses `{ParentList, "EscapedCharacters"}` (because it has improved Unicode handling). The `$defaultConfigSettings` in `LLMUtilities.wl` sets this to `{"Notebooks", "NotebooksPreamble"}` for LLM configuration generation (not the regular chat notebook flow). No explicit dependencies in `$autoSettingKeyDependencies`. Not exposed in the preferences UI. |
| `"ChatContextPreprompt"` | `Automatic` | **Deprecated.** Legacy preprompt text used as the "Pre" section of the system prompt sent to the LLM. Resolved via `getPrePrompt` in `ChatMessages.wl`, which checks the following in priority order: persona-level `"ChatContextPreprompt"`, persona-level `"Pre"` / `"PromptTemplate"` / `"Prompts"`, then global `"ChatContextPreprompt"`, then global `"Pre"` / `"PromptTemplate"` / `"Prompts"`. The value must be a `String`, `TemplateObject`, or list thereof. Exposed in the chat context settings dialog (`Actions.wl`) as a text input field with a default of `"You are a helpful Wolfram Language programming assistant. Your job is to offer Wolfram Language code suggestions based on previous inputs and offer code suggestions to fix errors."`. Automatic value resolution is not implemented (noted as TODO in `Settings.wl`). Superseded by persona-based prompting via `"LLMEvaluator"` and the `"BasePrompt"` component system. |
| `"UserInstructions"` | `Automatic` | User-provided instructions to include in the system prompt. The value must be a `String` or `None`; when `Automatic`, it resolves to `$$unspecified` (treated identically to `None`, i.e., no instructions are added). There is no explicit `resolveAutoSetting0` rule for this setting. When the resolved value is a non-empty string, it is formatted by `addUserInstructions` (`ChatMessages.wl`) using `$customInstructionsTemplate`, which wraps the text in a structured template: a `"# User Instructions"` heading, a precedence directive (`"IMPORTANT: The following user instructions take precedence over ALL other instructions."`), and XML-style `<user-instructions>` tags around the user's text. The formatted instructions are then appended to the assembled custom prompt (from the `"Prompts"` setting) within `assembleCustomPrompt` (`ChatMessages.wl`): if `"Prompts"` produced a string, the user instructions are appended with a `"\n\n"` separator; if `"Prompts"` produced `None`, the user instructions template is used alone. The combined result flows into `addPrompts` (`ChatMessages.wl`), where it is joined with workspace and inline chat context prompts via `StringRiffle` (with `"\n\n"` separators), then appended to the existing system message's `"Content"` (or used to create a new system message if none exists). Exposed in the preferences UI (`PreferencesContent.wl`) via `makeInstructionsContent` as a multi-line text input field under the "Instructions" subsection, with the description "Instructions specified here will be applied to all conversations with Notebook Assistant and other chats" and a hint placeholder (e.g., "Before making any tool calls, briefly explain what you are doing and why."). The save action writes the value via `CurrentChatSettings[$preferencesScope, "UserInstructions"]`; if the text is empty or not a string, it is set to `Inherited`. Localization strings for the preferences UI exist in all supported languages (`FrontEnd/TextResources/`). Persona overrides: the RawModel persona explicitly sets `"UserInstructions" -> None` (`LLMConfiguration/Personas/RawModel/LLMConfiguration.wl`), which suppresses user instructions even if set globally in preferences. No other built-in persona sets this value. No model-specific overrides in `$modelAutoSettings`. No chat mode overrides. No dependencies in `$autoSettingKeyDependencies`. Not in `$llmConfigPassedKeys` (not passed directly to the LLM service; used internally by Chatbook for system prompt construction). Not in `$usableChatSettingsKeys` (not settable via the ChatPreferences LLM tool). Not in `$modelInheritedLists`. Not in `$popOutSettings`. Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. |
| `"Prompts"` | `{}` | A list of custom prompt strings to append to the system prompt. When non-empty, the prompt strings are assembled by `assembleCustomPrompt0` (`ChatMessages.wl`) and appended to the system message content via `addPrompts`. The value is looked up from `settings["Prompts"]` and processed according to its type: a single string is used directly; a list of strings is joined with `"\n\n"` separators via `StringRiffle`; a list containing `TemplateObject` elements is resolved by applying each template with `applyPromptTemplate` (using the current settings and `$ChatHandlerData` as template parameters), then each result is validated by `checkPromptComponent` (which accepts strings or lists of strings, and throws `"InvalidPromptComponent"` for anything else). After assembly, the custom prompt is combined with `"UserInstructions"` via `addUserInstructions`, then with workspace and inline chat context prompts, and the combined result is appended to the existing system message's `"Content"` (or creates a new system message if none exists). Additionally, `"Prompts"` appears in the `getPrePrompt` fallback chain (`ChatMessages.wl`), which constructs the `"Pre"` section of the system prompt template used by `buildSystemPrompt`. The fallback priority is: persona-level `"ChatContextPreprompt"` > persona `"Pre"` > persona `"PromptTemplate"` > persona `"Prompts"` > settings-level `"ChatContextPreprompt"` > settings `"Pre"` > settings `"PromptTemplate"` > settings `"Prompts"`. Values in this chain must match `_String | _TemplateObject | { (_String|_TemplateObject)... }`. In `tryPromptRepositoryPersona` (`Settings.wl`), when loading personas from the Wolfram Prompt Repository, the `"Prompts"` field from the `LLMConfiguration` data is extracted and normalized: if it matches `_List | _String | _TemplateObject | _LLMPromptGenerator`, it is wrapped with `Flatten @ { prompts }` to ensure a flat list. There is a TODO comment in `Settings.wl` (line 1925) noting that special merging/inheritance logic for `"Prompts"` is planned but not yet implemented; currently, standard association merging rules apply via `mergeChatSettings0`. Exposed in the ChatPreferences tool (`ChatPreferences.wl`) as a settable key with type `[string]` (list of strings), described as "A list of instructions to append to the system prompt," and validated via `Interpreter[RepeatingElement["String"]]`. Listed in `$usableChatSettingsKeys`. No dependencies in `$autoSettingKeyDependencies`. No model-specific overrides in `$modelAutoSettings`. No built-in personas set this value. Not in `$llmConfigPassedKeys` (not passed directly to the LLM service; used internally by Chatbook for system prompt construction). Not in `$modelInheritedLists`. Not in `$popOutSettings`. Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. |
| `"PromptGenerators"` | `Automatic` | List of prompt generators used to augment the conversation with additional context (e.g., related documentation, Wolfram Alpha results, web search results) before sending messages to the LLM. When `Automatic`, resolves to `{}` (no generators) via `resolveAutoSetting0` (`Settings.wl`). The value can be a list containing: string names of built-in generators (e.g., `"RelatedDocumentation"`, `"RelatedWolframAlphaResults"`, `"WebSearch"`), `LLMPromptGenerator` objects directly, or `ParentList` for inheriting from the parent scope. Three built-in generators are registered in `$defaultPromptGenerators` (`DefaultPromptGenerators.wl`): `"RelatedDocumentation"` (searches documentation via vector databases, returning formatted markdown with citations; uses `RelatedDocumentation` from `RelatedDocumentation.wl`), `"RelatedWolframAlphaResults"` (queries Wolfram Alpha for relevant results via `RelatedWolframAlphaResults` from `RelatedWolframAlphaResults.wl`), and `"WebSearch"` (performs web searches using the Tavily API, requiring a `TAVILY_API_KEY` system credential). Requires Wolfram Engine 14.1+ (`LLMPromptGenerator` support); on older versions, `toPromptGenerator` returns `Nothing` for all generators. Resolution occurs in two stages: (1) During `resolveAutoSettings0` (`Settings.wl`, line 692), `resolvePromptGenerators` (`Common.wl`) is called on the full settings association. This function reads the `"PromptGenerators"` list, conditionally appends `"RelatedWolframAlphaResults"` if `featureEnabledQ["RelatedWolframAlphaResults", settings]` is `True` (triggered by `"WolframAlphaCAGEnabled" -> True`), conditionally appends `"WebSearch"` if `featureEnabledQ["RelatedWebSearchResults", settings]` is `True` (triggered by `"WebSearchRAGMethod" -> "Tavily"`), then resolves each string name to an `LLMPromptGenerator` object via `resolvePromptGenerator` (which looks up the name in `$defaultPromptGenerators`; invalid names throw `"InvalidPromptGenerator"` failure). `ParentList` and `$$unspecified` entries are removed via `Nothing`. The resolved list is deduplicated and stored back as `{ ___LLMPromptGenerator }`. (2) During message construction in `augmentChatMessages` (`ChatMessages.wl`, line 276), `applyPromptGenerators` (`DefaultPromptGenerators.wl`) is called with the resolved settings and merged messages. This function converts each generator to an `LLMPromptGenerator` via `toPromptGenerator`, creates generator data via `makePromptGeneratorData` (which extracts `"Input"` from the last message's content and `"Messages"` from the full message list), then applies each generator. Each generator invocation triggers `"PromptGeneratorStart"` and `"PromptGeneratorEnd"` handler function events (via `applyHandlerFunction`), with `"PromptGenerator"`, `"PromptGeneratorData"`, and `"PromptGeneratorName"` in the handler arguments (and `"PromptGeneratorResult"` added for the end event). Generator results are processed by `formatGeneratedPrompt`, which handles strings, lists, text/image content types, and `Missing`/`None` (returning `""`). Empty strings are removed from the final list. The non-empty generated prompt strings are then wrapped as message Associations with `"Role"` set to `PromptGeneratorMessageRole`, `"Content"` set to the generated text, and `"Temporary" -> True`, and inserted into the merged message list at the position specified by `PromptGeneratorMessagePosition`. Interacts with `"ExperimentalFeatures"` bidirectionally: `ExperimentalFeatures` depends on `PromptGenerators` in `$autoSettingKeyDependencies` — `autoExperimentalFeatures` (`Settings.wl`) checks if `"RelatedWolframAlphaResults"` or `"WebSearch"` are in the PromptGenerators list and adds the corresponding experimental feature flags; conversely, setting `"WolframAlphaCAGEnabled" -> True` or `"WebSearchRAGMethod" -> "Tavily"` causes `resolvePromptGenerators` to auto-append the corresponding generator. Persona overrides: AgentOne uses `{"RelatedDocumentation", "RelatedWolframAlphaResults"}`, AgentOneCoder uses `{"RelatedDocumentation"}`, CodeAssistant/CodeWriter/Wolfie/Birdnardo/NotebookAssistant use `{"RelatedDocumentation", ParentList}`, WolframAlpha uses `{"RelatedWolframAlphaResults", ParentList}`, RawModel uses `{}` (no generators), and PlainChat does not set this (inherits default). `ParentList` in persona values enables merging with the parent scope's generators via `mergeChatSettings`. Chat mode overrides: NotebookAssistance mode (`ShowNotebookAssistance.wl`) overrides to `{"RelatedDocumentation"}` in `$notebookAssistanceBaseSettings`. In `LLMUtilities.wl`, the `makeLLMConfiguration0` function reads resolved PromptGenerators from settings and passes them as the first argument to `constructMessages`, but then constructs messages with an empty `PromptGenerators` list to avoid double-applying generators. No model-specific overrides exist in `$modelAutoSettings`. No dependencies declared for PromptGenerators itself in `$autoSettingKeyDependencies` (but `ExperimentalFeatures` depends on it). Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook for message augmentation). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not listed in `$modelInheritedLists` (but `ParentList` merging is handled generically by `mergeChatSettings0` for all list values). Not listed in `$popOutSettings`. Not exposed in the preferences UI. |
| `"PromptGeneratorsEnabled"` | `Automatic` | **Not yet implemented.** Registered in `$defaultChatSettings` (`Settings.wl`) with a default value of `Automatic` and a `(* TODO *)` comment, but not referenced or used anywhere in the codebase. The intended purpose appears to be controlling which prompt generators are enabled, but the actual mechanism for this is currently handled by other settings: the `"PromptGenerators"` setting directly specifies which generators to use (as a list of names or `LLMPromptGenerator` objects), and the `"ExperimentalFeatures"` setting can conditionally enable additional generators (e.g., `"RelatedWolframAlphaResults"` via `"WolframAlphaCAGEnabled"`, `"WebSearch"` via `"WebSearchRAGMethod"`). Since this setting is not read by any code, changing its value has no effect. No dependencies in `$autoSettingKeyDependencies`. No model-specific overrides exist in `$modelAutoSettings`. Not in `$llmConfigPassedKeys`. Not exposed in the preferences UI. |
| `"PromptGeneratorMessagePosition"` | `2` | Position in the message list where prompt generator messages are inserted. The default is a fixed numeric value (`2`), not `Automatic`. When `Automatic`, resolves to `2` via `resolveAutoSetting0` (`Settings.wl`). Used in `augmentChatMessages` (`ChatMessages.wl`): after chat messages are constructed and optionally merged (via `MergeMessages`), prompt generators produce additional context strings (e.g., related documentation, Wolfram Alpha results). These strings are wrapped as message Associations with `"Role"` set to the `PromptGeneratorMessageRole` value, `"Content"` set to the generated text, and `"Temporary" -> True` (marking them for exclusion in subsequent chat turns). The messages are then inserted into the merged message list via `Insert[merged, generatedMessages, genPos]`, where `genPos` is this setting's resolved value. A position of `2` places the generated messages after the first message in the list (typically the system prompt), ensuring they appear early in the context before user/assistant conversation history. If the position is invalid (e.g., out of bounds for the message list), `Insert` produces a message and `throwFailure["InvalidPromptGeneratorPosition", genPos]` is called, which issues the error: `"Invalid position spec for prompt generator messages: \`1\`."` (`Common.wl`). The value can be any valid `Insert` position specification (positive or negative integer). No dependencies in `$autoSettingKeyDependencies`. No model-specific overrides exist in `$modelAutoSettings`. No chat mode overrides exist. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook for message list construction). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not listed in `$popOutSettings`. Not exposed in the preferences UI or the ChatPreferences tool. |
| `"PromptGeneratorMessageRole"` | `"System"` | Message role assigned to prompt generator messages when they are inserted into the conversation. The default is a fixed string value (`"System"`), not `Automatic`. When `Automatic`, resolves to `"System"` via `resolveAutoSetting0` (`Settings.wl`). Used in `augmentChatMessages` (`ChatMessages.wl`): after chat messages are constructed and optionally merged (via `MergeMessages`), prompt generators produce additional context strings (e.g., related documentation, Wolfram Alpha results). Each generated string is wrapped as a message Association with `"Role"` set to this setting's resolved value, `"Content"` set to the generated text, and `"Temporary" -> True` (marking them for exclusion in subsequent chat turns). The value is validated immediately after being read from settings: `If[ ! MatchQ[ genRole, "System"|"Assistant"|"User" ], throwFailure[ "InvalidPromptGeneratorRole", genRole ] ]`, which issues the error: `"Invalid role for prompt generator messages: \`1\`. Valid values are: \"System\", \"Assistant\", or \"User\"."` (`Common.wl`). The three valid values determine how the LLM interprets the generated context: `"System"` (default) frames the context as system-level instructions, `"User"` frames it as user-provided input, and `"Assistant"` frames it as prior assistant output. The generated messages are then inserted into the merged message list at the position specified by `PromptGeneratorMessagePosition`. No dependencies in `$autoSettingKeyDependencies`. No model-specific overrides exist in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook for message list construction). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not listed in `$popOutSettings`. Not exposed in the preferences UI or the ChatPreferences tool. |
| `"DiscourageExtraToolCalls"` | `Automatic` | Whether to include a base prompt component discouraging unnecessary tool calls. When enabled, adds the `"DiscourageExtraToolCalls"` base prompt component to the system prompt, which appends the text: `"Don't make more tool calls than is needed. Tool calls cost tokens, so be efficient!"`. The setting is evaluated via `discourageExtraToolCallsQ` in `ChatMessages.wl`, which returns `False` if `ToolsEnabled` is `False` or `Tools` is empty (i.e., the prompt is only included when tools are actually available). Has no dependencies on other base prompt components. Model-specific: currently only enabled (`True`) for Anthropic Claude 3.7 Sonnet via `$modelAutoSettings`. No global auto default exists, so `Automatic` effectively resolves to `False` for all other models. Not exposed in the preferences UI. |

## Tools

| Setting | Default | Description |
| ------- | ------- | ----------- |
| `"Tools"` | `Automatic` | Tool definitions available to the LLM, resolved as a list of `LLMTool` objects during chat processing. When `Automatic` (the default), tools are resolved dynamically based on the persona (`LLMEvaluator`) and `ToolsEnabled` setting via `resolveTools` (`Tools/Common.wl`). Resolution only occurs when `ToolsEnabled` is `True`; otherwise the setting remains unchanged. The resolution process works as follows: (1) `initTools` initializes the tool system, (2) `selectTools` determines which tools are active based on three inputs — tool names from `getToolNames`, per-persona tool selections from `getToolSelections` (stored in `"ToolSelections"`), and per-tool selection types from `getToolSelectionTypes` (stored in `"ToolSelectionType"`) — then (3) the resolved `$selectedTools` association (filtered by `toolEnabledQ`, which checks each tool's `"Enabled"` key) is stored as `Values @ $selectedTools` (a list of `LLMTool` objects) back into the `"Tools"` key of the settings. The `getToolNames` function (`Tools/Common.wl`) determines the initial tool name list through a two-level dispatch: if the persona declares tools (via `LLMEvaluator["Tools"]`), persona tools and setting-level tools are combined; if the persona sets `None`, no tools are used; if the persona uses `Automatic`, `Inherited`, or `ParentList`, default tools are used; and `ParentList` within a persona's tool list causes the parent scope's tools to be spliced in at that position. Accepted values for the `"Tools"` setting itself: `Automatic` (resolve from persona and defaults), `None` (no tools), `All` (all available tools), `Inherited` (inherit from parent scope), a list of tool name strings (e.g., `{"WolframLanguageEvaluator", "WebSearcher"}`), a list of `LLMTool` objects, or a mixed list containing strings, `LLMTool` objects, rules (`name -> tool`), and `ParentList`. After resolution, the value is always a flat list of `LLMTool` objects (`{ ___LLMTool }`). The resolved tools are consumed in two places: (1) `makeLLMConfiguration` (`SendChat.wl`) — when `ToolMethod` is `"Service"` or `HybridToolMethod` is `True`, tools are passed to `LLMConfiguration` as `"Tools" -> Cases[Flatten @ {as["Tools"]}, _LLMTool]`, enabling the LLM service's native tool calling API; when neither condition holds, tools are omitted from `LLMConfiguration` and tool calling is handled entirely through prompt-based instructions. (2) `constructLLMConfiguration` (`LLMUtilities.wl`) — tools are read from `settings["Tools"]`, each processed by `addToolPostProcessing`, validated with `ConfirmMatch[..., { ___LLMTool }]`, and passed to the `LLMConfiguration`. If `WolframLanguageEvaluator` is among the selected tools, `resolveTools` triggers the `"WolframLanguageEvaluatorTool"` base prompt component via `needsBasePrompt`. Persona overrides: CodeAssistant and AgentOne/AgentOneCoder declare `{"WolframLanguageEvaluator", "DocumentationSearcher", "WolframAlpha", ParentList}`, WolframAlpha declares `{"WolframAlpha", ParentList}`, PlainChat declares `{"WebSearcher", "WebImageSearcher", "WebFetcher", ParentList}`, CodeWriter declares `{ParentList}`, RawModel declares `None` (no tools), and Wolfie/Birdnardo/NotebookAssistant declare custom tool lists with `ParentList`. Dynamic tracking: changes trigger `$toolsTrigger` (`Dynamics.wl`), updating tool-dependent UI elements. Depends on `"LLMEvaluator"` and `"ToolsEnabled"` (declared in `$autoSettingKeyDependencies`). Listed in `$usableChatSettingsKeys` and exposed in the `ChatPreferences` tool (`ChatPreferences.wl`) with an `interpretTools` validator that checks tool names against `$AvailableTools`. No model-specific overrides exist in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed directly; conditionally included by `makeLLMConfiguration` as described above). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not listed in `$modelInheritedLists`. Not listed in `$popOutSettings`. Exposed indirectly in the preferences UI via the "Tools" tab (`PreferencesContent.wl`), which renders the `toolSettingsPanel` (`ToolManager.wl`) providing a grid interface to install, enable/disable, and configure tools per persona. |
| `"ToolsEnabled"` | `Automatic` | Whether tools are enabled for the current chat. Valid values: `Automatic`, `True`, or `False`. When `Automatic`, resolved per model via `toolsEnabledQ` (`Settings.wl`). The resolution logic proceeds in order: (1) if explicitly set to `True` or `False`, that value is used directly; (2) if `ToolCallFrequency` is non-positive (`0` or negative), returns `False` (tools disabled); (3) if the model has a model-specific `"ToolsEnabled"` override in `$modelAutoSettings`, that value is used; (4) the model name is checked against `$$disabledToolsModel` — models matching `"chat-bison-001"`, `"gemini-1.0-pro"` (and variants), `"gemini-pro-vision"`, or `"gemini-pro"` (case-insensitive) return `False`; (5) all other models return `True`. Model-specific overrides in `$modelAutoSettings`: `False` for GoogleGemini GeminiPro, GoogleGemini GeminiProVision, and O1Mini; `True` for GoogleGemini Gemini2 and GoogleGemini Gemini3. The resolved value affects many downstream behaviors: **Tool resolution** — `resolveTools` (`Tools/Common.wl`) only initializes and selects tools when `ToolsEnabled` is `True`; otherwise the `"Tools"` setting remains unchanged. **Tool prompt** — `getToolPrompt` (`ChatMessages.wl`) returns an empty string `""` when `ToolsEnabled` is `False`, suppressing tool instructions in the system prompt. **Discourage extra tool calls** — `discourageExtraToolCallsQ` (`ChatMessages.wl`) returns `False` when `ToolsEnabled` is `False`, even if `DiscourageExtraToolCalls` is enabled. **Response handling** — `checkResponse` (`SendChat.wl`) has a special pattern for `ToolsEnabled -> False` that either writes the result directly or defers it based on `$AutomaticAssistance`. **Stop tokens** — `autoStopTokens` (`Settings.wl`) returns `{ "[INFO]" }` when `$AutomaticAssistance` is `True` and `ToolsEnabled` is `False`, otherwise `None`. **Hybrid tool method** — `hybridToolMethodQ` (`Settings.wl`) returns `False` when `ToolsEnabled` is `False`. **Tool example prompt style** — `chooseToolExamplePromptStyle` (`Settings.wl`) returns `None` when `ToolsEnabled` is `False`. **Tool Manager warning** — `toolModelWarning` (`ToolManager.wl`) displays a warning message (`$toolsDisabledWarning`) when `ToolsEnabled` is `False` for the current scope. Depends on `"Model"` and `"ToolCallFrequency"` (declared in `$autoSettingKeyDependencies`). Is a dependency for `"HybridToolMethod"`, `"ToolCallExamplePromptStyle"`, and `"Tools"`. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook to gate tool-related features). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not listed in `$usableChatSettingsKeys` or the `ChatPreferences` tool definition. Not listed in `$popOutSettings` or `$modelInheritedLists`. Exposed in the preferences UI (`PreferencesContent.wl`) via `makeToolsEnabledMenu` as a `PopupMenu` with three choices: "Enabled by Model" (`Automatic`), "Enabled Always" (`True`), and "Enabled Never" (`False`), located in the Features content section alongside Multimodal and ToolCallFrequency settings. |
| `"ToolMethod"` | `Automatic` | Method for tool calling. Controls the mechanism by which the LLM invokes tools. Valid values: `"Service"` (uses the LLM service's native tool calling API), `"Simple"` (uses slash-command format with `/exec` markers), `"Textual"` or `"JSON"` (uses `ENDTOOLCALL`-based prompt format), or `Automatic`. When `Automatic`, resolved by `chooseToolMethod` (`Settings.wl`): if all resolved tools are "simple tools" (i.e., members of `$DefaultTools`), resolves to `"Simple"`; otherwise remains `Automatic` (which is treated as a generic prompt-based method using `ENDTOOLCALL` markers). The automatic resolution happens in `resolveAutoSettings0` after `Tools` has been resolved. When `"Service"`, `makeLLMConfiguration` (`SendChat.wl`) passes `"ToolMethod" -> "Service"` and `LLMTool` definitions to `LLMConfiguration`, enabling native tool calling. For non-`"Service"` methods, tool definitions are omitted from `LLMConfiguration` and tool calling is handled entirely through prompt-based instructions: `makeToolPrompt` (`Tools/Common.wl`) assembles a system prompt from five components (pre-prompt, listing, example, post-prompt, preference prompt), each selected by method — `"Service"` returns `Nothing` (no prompt injection), `"Simple"` uses `$simpleToolPre`/`$simpleToolListing`/`$simpleToolPost`, and other methods use `$toolPre`/`$toolListing`/`$toolPost`. Affects message preparation: when `"Service"`, `prepareMessagesForLLM0` rewrites messages via `rewriteServiceToolCalls`; otherwise, `"ToolRequests"` and `"ToolResponses"` keys are dropped from messages. Affects tool call parsing: `"Simple"` uses `simpleToolRequestParser`; other methods use `toolRequestParser` (with a TODO noting a planned `getToolRequestParser` dispatch). Affects tool response formatting: when `"Service"`, the tool response message role is forced to `"Tool"` regardless of the `ToolResponseRole` setting; otherwise uses the `ToolResponseRole` value. Affects stop tokens: `"Simple"` adds `"\n/exec"`, `"Service"` adds none (only `$endToken`), `"Textual"`/`"JSON"` adds `"ENDTOOLCALL"`, and other/fallback adds both `"ENDTOOLCALL"` and `"\n/exec"`. When `"Service"`, triggers the `"ServiceToolCallRetry"` base prompt component (`ChatMessages.wl`). The global variable `$simpleToolMethod` (`CommonSymbols.wl`, set in `Formatting.wl`) tracks whether the current method is `"Simple"`, used by `toolRequestToString` (`LLMUtilities.wl`) and `parsePartialToolCallString` (`Formatting.wl`) for formatting. Model-specific overrides in `$modelAutoSettings`: `"Service"` for Anthropic (general), AzureOpenAI, DeepSeek Chat, OpenAI (general), GPT-3.5, GPT-5, O1, O3, and O4-Mini; `Verbatim @ Automatic` (keeps prompt-based tools, often paired with `HybridToolMethod -> True`) for Claude 2, GPT-4o, GPT-4.1, and O3-Mini. Not listed in `$autoSettingKeyDependencies` (no dependencies on other settings for its own resolution), but is a dependency of `HybridToolMethod`. Not in `$llmConfigPassedKeys` (not passed directly through `LLMConfiguration`); instead, `"ToolMethod" -> "Service"` is conditionally added by `makeLLMConfiguration` when the resolved value is `"Service"` or `HybridToolMethod` is `True`. Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not exposed in the preferences UI. |
| `"HybridToolMethod"` | `Automatic` | Whether to use hybrid tool calling, combining service-level and prompt-based tool calling. When `Automatic`, resolved by `hybridToolMethodQ` (`Settings.wl`): returns `False` if `ToolsEnabled` is `False`; returns `False` if `ToolMethod` is `"Service"` (since service-level calling is already in use, hybrid mode is unnecessary); otherwise returns `True` if the model matches `$$hybridToolModel`, which requires the service to be `"OpenAI"`, `"AzureOpenAI"`, or `"LLMKit"` (or the model to be a plain string); returns `False` for all other models. When `True`, `makeLLMConfiguration` in `SendChat.wl` builds the `LLMConfiguration` with `"ToolMethod" -> "Service"` and includes `LLMTool` definitions in the `"Tools"` parameter, enabling the LLM service's native tool calling API alongside Chatbook's prompt-based tool calling. This means the model receives both prompt-based tool instructions (from the resolved `ToolMethod`, e.g., `"Simple"`) and service-level tool definitions, allowing it to use either mechanism. When `False`, the `LLMConfiguration` omits tool definitions and relies solely on the prompt-based tool method. Depends on `"Model"`, `"ToolsEnabled"`, and `"ToolMethod"` (declared in `$autoSettingKeyDependencies`). Model-specific overrides in `$modelAutoSettings`: `True` for GPT-4o, GPT-4.1, and O3-Mini (which use `ToolMethod -> Verbatim @ Automatic` to keep prompt-based tools alongside service tools); `False` for DeepSeek Reasoner, GPT-5, O1, O3, and O4-Mini (which either use pure `"Service"` tool method or have limited tool support). Not in `$llmConfigPassedKeys` (not passed directly through `LLMConfiguration`), but indirectly controls whether `LLMConfiguration` includes `"ToolMethod" -> "Service"` and `"Tools"`. Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not exposed in the preferences UI. |
| `"ToolOptions"` | `$DefaultToolOptions` | Per-tool option overrides as a nested `Association` mapping tool names to their option associations. The default value `$DefaultToolOptions` (`Tools/Common.wl`) provides options for five tools: `"WolframAlpha"` (`"DefaultPods" -> False`, `"FoldPods" -> False`, `"MaxPodByteCount" -> 1000000`), `"WolframLanguageEvaluator"` (`"AllowedExecutePaths" -> Automatic`, `"AllowedReadPaths" -> All`, `"AllowedWritePaths" -> Automatic`, `"AppendURIPrompt" -> False`, `"EvaluationTimeConstraint" -> 60`, `"Method" -> Automatic`, `"PingTimeConstraint" -> 30`), `"WebFetcher"` (`"MaxContentLength" -> 12000`), `"WebSearcher"` (`"AllowAdultContent" -> Inherited`, `"Language" -> Inherited`, `"MaxItems" -> 5`, `"Method" -> "Google"`), and `"WebImageSearcher"` (same keys as `"WebSearcher"`). Note that the default is defined with delayed evaluation (`:>`) in `$defaultChatSettings` (`Settings.wl`), so it evaluates `$DefaultToolOptions` fresh each time. During chat processing, `resolveTools` (`Tools/Common.wl`) assigns the resolved value to the `$toolOptions` global variable via `Lookup[settings, "ToolOptions", $DefaultToolOptions]`. Individual tool option values are accessed at runtime through `toolOptionValue[toolName, key]` (`Tools/ToolOptions.wl`), which checks `$toolOptions` first and falls back to `$DefaultToolOptions`. Tool implementations use this to read their configuration: e.g., `Sandbox.wl` reads `"EvaluationTimeConstraint"`, `"Method"`, `"AllowedReadPaths"`, `"AllowedWritePaths"`, `"AllowedExecutePaths"`, `"AppendURIPrompt"`, and `"PingTimeConstraint"` for `"WolframLanguageEvaluator"`; `WolframAlpha.wl` reads `"DefaultPods"`, `"FoldPods"`, and `"MaxPodByteCount"`; `WebFetcher.wl` reads `"MaxContentLength"`. Users can programmatically modify tool options via `SetToolOptions` (exported in `Main.wl`): `SetToolOptions["WolframLanguageEvaluator", "EvaluationTimeConstraint" -> 120]` sets a global override, `SetToolOptions[notebookObj, "WebSearcher", "MaxItems" -> 10]` sets a notebook-scoped override, and `SetToolOptions["WolframAlpha", Inherited]` resets a tool's options to defaults. Options are stored in `{TaggingRules, "ChatNotebookSettings", "ToolOptions", toolName, optionKey}`. Chat mode overrides exist: NotebookAssistance mode (`ShowNotebookAssistance.wl`) sets `"WolframLanguageEvaluator" -> <|"AppendURIPrompt" -> True, "Method" -> "Session"|>`. No model-specific overrides in `$modelAutoSettings`. No dependencies in `$autoSettingKeyDependencies`. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook to configure tool behavior). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not listed in the `ChatPreferences` tool definition. Not exposed in the preferences UI. |
| `"ToolSelectionType"` | `<\|\|>` | Per-tool override that controls whether a tool is enabled globally (for all personas), disabled globally, or left to per-persona selection. The value is an `Association` mapping tool canonical names (strings) to one of three values: `All` (tool is always enabled regardless of persona), `None` (tool is never enabled regardless of persona), or `Inherited` (tool enablement is determined per-persona via the `"ToolSelections"` setting). When a tool's canonical name is absent from the association, it behaves as `Inherited`. Used by `selectTools` (`Tools/Common.wl`) during tool resolution: `getToolSelectionTypes` retrieves this setting and filters it to keys present in `$AvailableTools`. Tools with selection type `All` are added to the selected tool set unconditionally (unioned with per-persona selections), while tools with selection type `None` are removed unconditionally (even if selected by the persona). The default empty association (`<\|\|>`) means all tools default to per-persona selection behavior. Exposed in the preferences UI via the Tool Manager (`ToolManager.wl`) as a `PopupMenu` per tool row with three options: "Enabled by persona" (`Inherited`), "Never" (`None`), and "Always" (`All`). The Tool Manager also provides a clear/reset button that unsets both `"ToolSelections"` and `"ToolSelectionType"` for a given tool. In the cloud UI, `cloudToolEnablePopup` (`PreferencesContent.wl`) provides a similar popup. When a per-persona checkbox is toggled manually in the Tool Manager, the `ToolSelectionType` for that tool is automatically unset (reverting to `Inherited`), since explicit per-persona selections supersede the global override. Used by `enableTool` (`ResourceInstaller.wl`) to set a newly installed tool's selection type to `All`, making it immediately available. Used by `deleteTool` (`ToolManager.wl`) to clean up the selection type when a tool is uninstalled. The `resetChatPreferences["Tools"]` function (`PreferencesContent.wl`) resets this setting to `Inherited` along with `"ToolSelections"`. No dependencies in `$autoSettingKeyDependencies`. No model-specific overrides in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook to determine the active tool set). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not listed in `$usableChatSettingsKeys` or the `ChatPreferences` tool definition. Not listed in `$popOutSettings` or `$modelInheritedLists`. |
| `"ToolCallFrequency"` | `Automatic` | Controls how often the LLM should use tools. Accepts `Automatic` or a number between `0` and `1`. When `Automatic`, no tool frequency preference prompt is injected into the system message. When set to a numeric value, `makeToolPreferencePrompt` (`Tools/Common.wl`) injects a "User Tool Call Preferences" section into the system prompt. The frequency value is scaled to a percentage and mapped to one of six explanation levels (computed as `Round[Clip[5 * freq, {0, 5}]]`): `0` (0%) = "Only use a tool if explicitly instructed"; `1` (~20%) = "Avoid using tools unless necessary"; `2` (~40%) = "Only use tools if it will significantly improve quality"; `3` (~60%) = "Use tools whenever appropriate"; `4` (~80%) = "Use tools whenever there's even a slight chance of improvement"; `5` (100%) = "ALWAYS make a tool call in EVERY response". Setting this to a non-positive value (`0` or negative) causes `ToolsEnabled` to resolve to `False` via `toolsEnabledQ` (`Settings.wl`), effectively disabling all tools. The `ToolsEnabled` setting declares a dependency on `ToolCallFrequency` in `$autoSettingKeyDependencies`. Exposed in the preferences UI (`PreferencesContent.wl`) as a popup menu (Automatic/Custom) with a slider from "Rare" to "Often" when "Custom" is selected; selecting "Custom" defaults the frequency to `0.5`. Also exposed in the advanced chat UI (`UI.wl`) as a slider with an option to reset to `Inherited`. Listed in the `ChatPreferences` tool definition (`Tools/DefaultToolDefinitions/ChatPreferences.wl`) as a `Restricted["Number", {0, 1}]` parameter, allowing the LLM itself to adjust this setting. Not in `$llmConfigPassedKeys` (not passed directly to the LLM service; used internally to generate the preference prompt). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. |
| `"ToolCallRetryMessage"` | `Automatic` | Whether to append a retry-guidance system message after each tool response. When `True`, `makeToolResponseMessage` (`SendChat.wl`) appends `$toolCallRetryMessage` — a system message with `"HoldTemporary" -> True` containing: *"IMPORTANT: If a tool call does not give the expected output, ask the user before retrying unless you are ABSOLUTELY SURE you know how to fix the issue."* The `"HoldTemporary"` flag means this message is included in the current request but not persisted in the conversation history. When `Automatic`, resolved by `toolCallRetryMessageQ` (`Settings.wl`), which delegates to `llmKitQ`: returns `True` if the session uses LLMKit authentication (i.e., `Authentication` is `"LLMKit"`, or the model's `"Service"` or `"Authentication"` is `"LLMKit"`), `False` otherwise. Model-specific overrides in `$modelAutoSettings`: explicitly `False` for GPT-4.1 and GPT-5 (and GPT-5.1/GPT-5.2 via inheritance). Depends on `"Authentication"` and `"Model"` (declared in `$autoSettingKeyDependencies`). Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook to control tool response message construction). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not exposed in the preferences UI. |
| `"ToolExamplePrompt"` | `Automatic` | Specifies the tool example prompt included in the system prompt to demonstrate tool usage patterns to the LLM. Used by `getToolExamplePrompt` (`Tools/Common.wl`) as part of `makeToolPrompt`, which assembles the full tool prompt from five components: tool pre-prompt, tool listing, tool example prompt, tool post-prompt, and tool preference prompt. Valid values: `Automatic`, `None`, or a custom template (any value matching `$$template`: `_String`, `_TemplateObject`, `_TemplateExpression`, `_TemplateSequence`). When `Automatic` (the resolved default for all services via `autoToolExamplePromptSpec` in `Settings.wl`), `getToolExamplePrompt` generates the example prompt from `$fullExamples` (`Tools/Examples.wl`) using the `ToolCallExamplePromptStyle` to format the examples — it sets `$messageTemplateType` to the style and returns the assembled examples string. When a custom template is provided, it is used directly. When `None`, no example prompt is included (`Nothing`). If `ToolCallExamplePromptStyle` is `None`, the example prompt is also omitted. Model-specific overrides in `$modelAutoSettings`: `None` for Claude 3 (Anthropic Claude3 family); explicitly `Automatic` for Claude 3.7 Sonnet (overriding the inherited `None` from Claude 3). When `Automatic`, resolved by `chooseToolExamplePromptSpec` (`Settings.wl`), which delegates to `autoToolExamplePromptSpec` based on the model's service — currently returns `Automatic` for all services (the catch-all definition `autoToolExamplePromptSpec[ _ ] := Automatic`). Personas can provide a `ToolExamplePrompt` file (`.md`, `.txt`, `.wl`, `.m`, or `.wxf`) in their LLM configuration directory, which is loaded by the persona system (`Personas.wl`) as a prompt file and used as this setting's value. Depends on `"Model"` (declared in `$autoSettingKeyDependencies`). Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook to construct the tool system prompt). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not exposed in the preferences UI. |
| `"ToolCallExamplePromptStyle"` | `Automatic` | Style of chat message templates used for tool call example prompts in the system prompt. Valid values: `"Basic"`, `"ChatML"`, `"XML"`, `"Instruct"`, `"Phi"`, `"Llama"`, `"Gemma"`, `"Nemotron"`, `"DeepSeekCoder"`, `"Zephyr"`, `"Boxed"`, `None`, or `Automatic`. When `Automatic`, resolved based on service and model family (e.g., OpenAI/AzureOpenAI → `"ChatML"`, Anthropic → `"XML"`, local models use family-specific templates). `None` when tools are disabled. Also determines style-specific stop tokens. Depends on `"Model"` and `"ToolsEnabled"`. Defined in `Tools/Examples.wl`; used by `getToolExamplePrompt` in `Tools/Common.wl`. |
| `"ToolResponseRole"` | `Automatic` | The message role assigned to tool response messages sent back to the LLM. Used by `makeToolResponseMessage` (`SendChat.wl`) to set the `"Role"` key in the tool response association. Valid values: `Automatic`, `"System"`, `"User"`, or `"Tool"`. When `Automatic`, resolved to `"System"` via the persona default in `$modelAutoSettings[ Automatic, Automatic ]` (`Settings.wl`). When `ToolMethod` is `"Service"`, the role is forced to `"Tool"` regardless of this setting's value. The role also affects message formatting: when `"User"`, the tool response content is wrapped in `<tool_response>...</tool_response>` tags; when `"SystemTags"` style is active (controlled by `ToolResponseStyle`), content is wrapped in `<system>...</system>` tags; otherwise, the response content is used as-is. Model-specific overrides in `$modelAutoSettings`: `"User"` for Anthropic Claude 2, DeepSeek DeepSeekReasoner, all MistralAI models, TogetherAI DeepSeekReasoner, and local models (Qwen, Nemotron, Mistral). No dependencies in `$autoSettingKeyDependencies`. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook to construct tool response messages). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not exposed in the preferences UI. |
| `"ToolResponseStyle"` | `Automatic` | Controls how tool response content is wrapped/formatted before being sent back to the LLM. Used by `makeToolResponseMessage0` (`SendChat.wl`) to determine the message structure for tool responses. Valid values: `Automatic`, `"SystemTags"`, or other style strings. When `"SystemTags"`, the tool response content is wrapped in `<system>...</system>` tags and the role defaults to `"System"` if not otherwise specified. When the role is `"User"` (regardless of style), the content is wrapped in `<tool_response>...</tool_response>` tags. For all other style values (including `Automatic`), the response content is used as-is without wrapping. Note that `"SystemTags"` takes priority over the `"User"` role — if both `ToolResponseStyle` is `"SystemTags"` and `ToolResponseRole` is `"User"`, the `"SystemTags"` pattern matches first. Model-specific overrides in `$modelAutoSettings`: `"SystemTags"` for MistralAI (paired with `ToolResponseRole` → `"User"`). No global default in `$modelAutoSettings[ Automatic, Automatic ]`, so `Automatic` is the effective default for most models. No dependencies in `$autoSettingKeyDependencies`. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook to format tool response messages). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not exposed in the preferences UI. |
| `"SplitToolResponseMessages"` | `Automatic` | Whether to split tool responses into separate messages. Model default: `False`. Enabled for Anthropic models as a workaround. |
| `"MaxToolResponses"` | `5` | Maximum number of tool responses allowed per chat turn before the tool-calling loop is stopped. During each chat turn, the `$toolCallCount` global variable (`CommonSymbols.wl`) is incremented each time the LLM makes a tool call (`SendChat.wl`), initialized to `0` via `ChatState.wl`. The `sendToolResponseQ` function (`SendChat.wl`) checks whether to continue the tool-calling loop: if `$toolCallCount > n` (where `n` is the `MaxToolResponses` value), it returns `False`, causing the chat to stop via `throwTop @ StopChat @ cell` — ending the current turn without sending the tool response back to the LLM. Note that the comparison uses `>` (not `>=`), so the LLM can make up to `n + 1` tool calls before being stopped (the count is incremented before the check). There is a TODO comment in the source noting that this should allow one final response when reaching the limit and disable tools for the next chat submit. Model-specific overrides in `$modelAutoSettings`: `3` for O1, O3, and O4-Mini (OpenAI reasoning models). Chat mode overrides: NotebookAssistance mode explicitly sets this to `5` in `$notebookAssistanceBaseSettings` (`ShowNotebookAssistance.wl`). Listed in `$popOutSettings` (`ConvertChatNotebook.wl`) as one of four settings shown during chat notebook conversion. No dependencies in `$autoSettingKeyDependencies`. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook to control the tool-calling loop). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not exposed in the preferences UI. |
| `"SendToolResponse"` | `Automatic` | Whether to send tool responses back to the LLM for further processing. When set to `False`, the `sendToolResponseQ` function (`SendChat.wl`) returns `False`, causing the chat turn to end via `throwTop @ StopChat @ cell` immediately after the tool response is received — the tool still executes and its output is displayed, but the result is not sent back to the LLM for a follow-up response. When `Automatic` (the default), tool responses are sent back unless the tool itself signals that it is terminal: individual tools can include `"SendToolResponse" -> False` in their output data or in the `"Output"` key of their response association, and the `terminalToolResponseQ`/`terminalQ` functions (`SendChat.wl`) detect this to stop the tool-calling loop for that specific tool call. This setting also affects `OpenToolCallBoxes` via `$autoSettingKeyDependencies` (`Settings.wl`): when `SendToolResponse` is `False`, `openToolCallBoxesQ` returns `True`, causing tool call boxes to be expanded so the user can see the tool output directly (since it won't be summarized by the LLM). No model-specific overrides in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook to control the tool-calling loop). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not exposed in the preferences UI. |
| `"EndToken"` | `Automatic` | End-of-turn token that signals the LLM has finished its response. Model default: `"/end"`. Some models use `None` (e.g., GPT-4.1). The resolved value is stored in the `$endToken` global variable (`CommonSymbols.wl`) during `resolveAutoSettings` (`Settings.wl`). Used in three ways: (1) **Base prompt instruction**: The `"EndTurnToken"` base prompt component (`Prompting.wl`) adds `"* Always end your turn by writing /end."` to the system prompt when `$endToken` is a non-empty string. The related `"EndTurnToolCall"` component (which depends on `"EndTurnToken"`) adds `"* If you are going to make a tool call, you must do so BEFORE ending your turn."`. (2) **Stop tokens**: `$endToken` is included in the stop sequences sent to the LLM API via `methodStopTokens` (`Settings.wl`), which builds tool-method-specific stop token lists (e.g., `{"ENDTOOLCALL", $endToken}` for `"Textual"`/`"JSON"` methods, `{$endToken}` for `"Service"` method, `{"\n/exec", $endToken}` for `"Simple"` method). These are composed into the `"StopTokens"` setting by `autoStopTokens` and passed to `LLMConfiguration` and the HTTP request body. (3) **Tool call example templates**: `Tools/Examples.wl` uses `$endTokenString` (which prepends `"\n"` to `$endToken` when non-empty) in assistant message templates across all example styles (Basic, Instruct, Zephyr, Phi, Boxed, ChatML, XML, DeepSeekCoder, Llama, Gemma, Nemotron) to show the LLM how to end its turn in example conversations. When `None` or empty, the end token is omitted from prompts, stop sequences, and example templates. After receiving a response, `trimStopTokens` in `SendChat.wl` removes stop tokens (including the end token) from the end of the LLM's output. No model-specific overrides exist beyond GPT-4.1 (`None`). Not exposed in the preferences UI. |

## Formatting & Output

| Setting | Default | Description |
| ------- | ------- | ----------- |
| `"AutoFormat"` | `True` | Whether to auto-format LLM output by parsing Markdown syntax and converting it to structured notebook cells. When enabled, the LLM response is processed to convert Markdown elements (code blocks with language detection, headings, bold/italic text, inline code, LaTeX math, images, bullet lists, block quotes, and tables) into properly formatted Wolfram notebook cells. Also includes the `"Formatting"` base prompt component in the system prompt, which instructs the LLM that its output will be parsed as Markdown. When disabled, output is displayed as plain text. Works in conjunction with `"DynamicAutoFormat"` to control whether formatting is applied during streaming. |
| `"DynamicAutoFormat"` | `Automatic` | Whether to apply formatting during streaming, providing live-formatted output as the LLM response streams in. When `True`, the streaming content is processed by the formatting function in real-time, converting Markdown to formatted notebook expressions as they arrive. When `False`, streaming content is displayed as raw text (via `RawBoxes @ Cell @ TextData`) without live formatting. When `Automatic`, resolves to `TrueQ` of the `"AutoFormat"` setting, so dynamic formatting is enabled whenever auto-formatting is enabled. Resolution is handled by `dynamicAutoFormatQ` in `SendChat.wl`, which first checks for an explicit `True`/`False` value, then falls back to `"AutoFormat"`. The resolved value is captured as the `reformat` variable in `activeAIAssistantCell` and passed to `dynamicTextDisplay`, which dispatches between formatted and raw text display. No model-specific overrides exist. Not exposed in the preferences UI; controlled indirectly via the `"AutoFormat"` checkbox. |
| `"StreamingOutputMethod"` | `Automatic` | Controls the method used for streaming output display, specifically whether the streaming content is progressively split into static (already-written) and dynamic (still-updating) portions during LLM response streaming. When `Automatic`, resolves to `"PartialDynamic"` via `resolveAutoSetting0` (`Settings.wl`). The setting is evaluated by the `dynamicSplitQ` function (`Settings.wl`), which determines the `$dynamicSplit` flag used in `chatHandlers` (`SendChat.wl`). Six valid string values in two groups: `"PartialDynamic"`, `"Automatic"`, and `"Inherited"` all resolve to `True` (dynamic splitting enabled), while `"FullDynamic"`, `"Dynamic"`, and `"None"` resolve to `False` (dynamic splitting disabled). Symbol values are converted to their string names. Invalid values trigger an `"InvalidStreamingOutputMethod"` warning message (`Common.wl`) and default to `True`. When dynamic splitting is enabled (`$dynamicSplit` is `True`), the `splitDynamicContent` function (`SendChat.wl`) is called on each `"BodyChunkReceived"` event: it splits the accumulated dynamic content string using `$dynamicSplitRules` (`Formatting.wl`) — a set of string patterns defining safe split points (e.g., after complete code blocks, headings, paragraphs) — then writes the completed static portions as final formatted notebook cells and keeps only the remaining dynamic portion updating live. This reduces the amount of content being dynamically re-rendered, improving performance for long responses. When disabled, all streaming content remains in a single dynamic cell until the response completes. Dynamic splitting is also disabled in headless mode (`$headlessChat`), inline chat (`$InlineChat`), cloud notebooks (`$cloudNotebooks`), and when the Wolfram Engine version is insufficient (`insufficientVersionQ["DynamicSplit"]`). No model-specific overrides exist in `$modelAutoSettings`. No dependencies in `$autoSettingKeyDependencies`. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook for streaming display optimization). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not exposed in the preferences UI. |
| `"NotebookWriteMethod"` | `Automatic` | Method for writing content to the notebook, controlling whether FrontEnd task batching is used for notebook write operations during chat. When `Automatic`, resolves to `"PreemptiveLink"` via `resolveAutoSetting0` (`Settings.wl`). Two valid values: `"PreemptiveLink"` enables FrontEnd task optimization, where notebook write operations (e.g., `NotebookWrite`, formatting toggles, error cell placement) are queued via `createFETask` (`FrontEnd.wl`) into the `$feTasks` list and executed in batches via `runFETasks`, reducing MathLink roundtrips between the kernel and FrontEnd for better responsiveness during streaming; `"ServiceLink"` disables this optimization by locally redefining `createFETask` to be the identity function (`#1 &`), causing all notebook writes to execute inline immediately. The setting is evaluated by the `feTaskQ` function (`SendChat.wl`), which returns `True` for `"PreemptiveLink"` (or unspecified values) and `False` for `"ServiceLink"`. Invalid values trigger an `"InvalidWriteMethod"` warning message (`Common.wl`) and fall back to `True` (task optimization enabled). The `feTaskQ` result is passed to `withFETasks` (`SendChat.wl`), which wraps both the `"BodyChunkReceived"` and `"TaskFinished"` handler functions in `chatHandlers`: when `True`, handlers execute normally with `createFETask` queueing enabled; when `False`, handlers execute inside `Block[{createFETask = #1 &}, ...]`, disabling task queueing. In headless mode (`$headlessChat`), `feTaskQ` always returns `False` regardless of the setting value, since there is no FrontEnd to process tasks. In cloud notebooks (`$cloudNotebooks`), `createFETask` evaluates operations immediately rather than queueing them. No model-specific overrides exist in `$modelAutoSettings`. No dependencies in `$autoSettingKeyDependencies`. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook for notebook write optimization). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not exposed in the preferences UI. |
| `"TabbedOutput"` | `True` | Whether to use paged (tabbed) output for multi-turn chat responses, so that each new LLM response replaces the previous one in the same output cell rather than creating a separate cell. When `True`, consecutive responses to the same chat input are organized as pages within a single output cell: each time `createNewChatOutput` (`SendChat.wl`) is called with an existing target `CellObject`, `prepareChatOutputPage` serializes the previous response content (via `BinarySerialize` and `BaseEncode`) and stores it in a `"PageData"` association in the cell's `TaggingRules`, with keys `"Pages"` (an association mapping page numbers to base64-encoded content), `"PageCount"`, and `"CurrentPage"`. The new response is then printed as the cell content. When `False`, `createNewChatOutput` bypasses paging and always creates a new cell via `cellPrint`, so each response appears in its own separate output cell. When reformatting the final output (`writeReformattedCell` and `reformatCell` in `SendChat.wl`), if `PageData` exists, the page metadata is carried forward via `makeReformattedCellTaggingRules`, which appends the current page's encoded content to the `"Pages"` association and increments the page count. If a response is empty (`None` string), `restoreLastPage` restores the previous page's content instead of deleting the cell. When a cell has more than one page (`PageCount > 1`), the cell dingbat uses an `"AssistantIconTabbed"` template wrapper (instead of the standard persona icon), which provides tab-style navigation UI. Users can navigate between pages via `rotateTabPage` (`Actions.wl`), which reads `"PageData"` from `TaggingRules`, computes the target page with `Mod`, deserializes the page content from the stored base64, and writes it to the cell via `writePageContent`. Overridden to `False` in workspace chat (`$workspaceDefaultSettings` in `StylesheetBuilder.wl`), notebook assistance workspace settings (`$notebookAssistanceWorkspaceSettings`), and sidebar chat settings (`$notebookAssistanceSidebarSettings`) in `ShowNotebookAssistance.wl`. Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook for output cell management). Not listed in `$popOutSettings`. Not exposed in the preferences UI. |
| `"ShowMinimized"` | `Automatic` | Whether LLM response output cells are displayed in a minimized (collapsed) state. When `True`, the output cell is created with `$closedChatCellOptions` (`CellMargins -> -2`, `CellOpen -> False`, `CellFrame -> 0`, `ShowCellBracket -> False`) and an `attachMinimizedIcon` initialization that attaches a small clickable icon to the previous cell bracket, allowing the user to expand the response on demand. When `False`, the output cell is displayed normally in an expanded state. When `Automatic`, the behavior depends on the context: in cloud notebooks (`$cloudNotebooks`), resolves to `False` (always expanded, since `$closedChatCellOptions` produces no options in cloud mode); in desktop notebooks, `Automatic` is treated equivalently to `True` for the minimization check (via `MatchQ[minimized, True|Automatic]` in `activeAIAssistantCell`, `SendChat.wl`), but only when `$AutomaticAssistance` is `True` — meaning the response was triggered by the automatic assistance system (e.g., `WidgetSend` or `autoAssistQ`-qualified evaluations) rather than a direct user chat input. For direct chat inputs (where `$AutomaticAssistance` is `False`), the minimization options are never applied regardless of this setting's value. The setting also influences `$alwaysOpen` via `alwaysOpenQ` (`Actions.wl`): when `ShowMinimized` is `True`, `$alwaysOpen` is set to `False`; when `False`, `$alwaysOpen` is set to `True`; when `Automatic`, `$alwaysOpen` depends on whether the input cell style matches `$$chatInputStyle` or whether the `BasePrompt` contains a severity tag. The `resolveAutoSetting0` for this key returns `Automatic` (`Settings.wl`). No model-specific overrides exist in `$modelAutoSettings`. No dependencies in `$autoSettingKeyDependencies`. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook for output cell rendering). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not listed in `$popOutSettings`. Not exposed in the preferences UI. |
| `"ShowProgressText"` | `Automatic` | Whether to show progress text (e.g., status labels with an ellipsis indicator) in the progress panel while the LLM is generating a response. When `True` (or when the resolved value is truthy), the `$showProgressText` flag (`CommonSymbols.wl`) is set to `True` during `resolveAutoSettings` (`Settings.wl`), which causes `basicProgressTextRow` (`Utils.wl`) to render a styled `"ProgressTitle"` row in the progress panel showing the current operation label (e.g., "Sending request", "Waiting for response") with a `ProgressIndicator[Appearance -> "Ellipsis"]` appended. When `False`, `basicProgressTextRow` returns `Nothing`, hiding the text row entirely so only the progress bar (if enabled via `$showProgressBar`) is shown. The `$showProgressText` flag is also forced to `True` when `"ForceSynchronous"` is truthy, regardless of this setting's value (line 660 in `Settings.wl`). The global default for `$showProgressText` is `False` (`Utils.wl`), so progress text is hidden unless explicitly enabled by this setting or by `ForceSynchronous`. The flag is preserved across handler evaluations via `ChatState.wl` (`$showProgressText = $showProgressText`). The `ContentSuggestions` chat mode (`ChatModes/ContentSuggestions.wl`) locally sets `$showProgressText = True` regardless of this setting. When `Automatic`, resolves to `True` via the model default in `$modelAutoSettings[Automatic, Automatic]`. No model-specific overrides exist beyond the default. No dependencies in `$autoSettingKeyDependencies`. No `resolveAutoSetting0` definition for this key. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook for progress UI rendering). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not listed in `$popOutSettings`. Not exposed in the preferences UI. Model default: `True`. |
| `"OpenToolCallBoxes"` | `Automatic` | Whether tool call display boxes are initially expanded (open) when rendered in the notebook. When `Automatic`, resolved by `openToolCallBoxesQ` (`Settings.wl`): returns `True` if `SendToolResponse` is `False` (meaning the user will not see further LLM processing of the tool result, so the tool output should be visible directly); otherwise returns `Automatic` (which evaluates to `False` via `TrueQ`, keeping boxes collapsed). Depends on `"SendToolResponse"` (declared in `$autoSettingKeyDependencies`). The resolved value is stored in the `$openToolCallBoxes` global variable (`CommonSymbols.wl`) during `resolveAutoSettings` (`Settings.wl`) and preserved across handler evaluation via `ChatState.wl` (initialized to `Automatic`). Used in two places: (1) **Tool call box rendering**: in `parseFullToolCallString` (`Formatting.wl`), the parsed tool call data includes `"Open" -> TrueQ @ $openToolCallBoxes`, which is then passed to `makeToolCallBoxLabel` where it controls whether the `openerView` displaying the tool call details (raw and interpreted views) starts in the open or closed state. (2) **Markdown output handling**: in `checkMarkdownOutput` (`SendChat.wl`), when `$openToolCallBoxes` is truthy, tool output containing markdown images is passed through unchanged; when falsy, `$useMarkdownMessage` is appended to the tool response, which tells the LLM: `"The user does not see the output of this tool call. You must use this output in your response for them to see it."` — prompting the LLM to include image links and other formatted content in its visible response text. No model-specific overrides exist in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook for UI rendering and tool response handling). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not listed in `$popOutSettings`. Not exposed in the preferences UI. |
| `"TrackScrollingWhenPlaced"` | `Automatic` | Whether to auto-scroll the notebook to follow new output as it is placed during and after LLM response streaming. This setting controls the FrontEnd's `"TrackScrollingWhenPlaced"` private cell option on output cells, which causes the notebook to automatically scroll to keep the output visible as new content is written. When `True`, `scrollOutputQ` (`SendChat.wl`) returns `True`, and the output cell created by `activeAIAssistantCell` includes `PrivateCellOptions -> {"TrackScrollingWhenPlaced" -> True}`, enabling auto-scroll during streaming. Additionally, after the final reformatted cell is written via `WriteChatOutputCell`, the `scrollOutput` function calls `SelectionMove` with `AutoScroll -> True` to scroll the notebook to the completed output cell. When `False`, auto-scrolling is disabled entirely: the private cell option is omitted and no post-write scroll occurs. When `Automatic`, resolves via `scrollOutputQ`, which checks whether the Wolfram Engine version meets the minimum requirement of 14.0 (defined in `$versionRequirements` in `Common.wl` via `sufficientVersionQ["TrackScrollingWhenPlaced"]`); if the version is sufficient, auto-scrolling is enabled, otherwise it is disabled. Note that `scrollOutputQ` has a two-argument form `scrollOutputQ[settings, cell]` that always returns `False` — this is used when computing the `"ScrollOutput"` key for the `reformatCell` path, meaning the post-write `SelectionMove`-based scrolling does not occur for reformatted cells (the `PrivateCellOptions`-based scrolling during streaming is sufficient). The `resolveAutoSetting0` for this key delegates to `scrollOutputQ` (`Settings.wl`). No model-specific overrides exist in `$modelAutoSettings`. No dependencies in `$autoSettingKeyDependencies`. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook for notebook scroll behavior). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not listed in `$popOutSettings`. Not exposed in the preferences UI. |
| `"AppendCitations"` | `Automatic` | Whether to automatically append formatted source citations to the LLM response. When enabled, citations are generated from sources gathered by prompt generators (e.g., documentation, web searches, WolframAlpha results) and appended as a markdown section. When disabled, the WolframAlpha prompt generator instead includes a hint asking the LLM to cite sources inline. Model default: `False`. The WolframAlpha persona overrides this to `True`. |

## Personas & UI

| Setting | Default | Description |
| ------- | ------- | ----------- |
| `"LLMEvaluator"` | `"CodeAssistant"` | The persona (LLM evaluator) to use. Determines the system prompt, available tools, and other settings. Can be a persona name string (e.g., `"CodeAssistant"`, `"PlainChat"`, `"RawModel"`) or a full persona `Association` with keys like `"Prompts"`, `"Tools"`, `"Icon"`, `"BasePrompt"`, etc. Built-in personas are defined in `LLMConfiguration/Personas/`. During `resolveAutoSettings` (`Settings.wl`), the string value is resolved via `getLLMEvaluator`, which calls `getNamedLLMEvaluator` to look up persona data from `GetCachedPersonaData`. If the name is not found as a built-in persona, `tryPromptRepositoryPersona` attempts to load it from the Wolfram Prompt Repository via `ResourceObject["Prompt" -> name]`. The resolved persona `Association` is merged with the current settings via `mergeChatSettings`, with `$nonInheritedPersonaValues` keys (including `"LLMEvaluator"` itself) dropped from the persona data before merging, preventing circular inheritance. After resolution, the `"LLMEvaluator"` key in the settings is replaced with the full persona `Association` (or the original string/`None` if unresolvable). When writing settings back to notebook `TaggingRules`, `toSmallSettings` in `SendChat.wl` converts the resolved `Association` back to just the persona name string (via the `"LLMEvaluatorName"` key) to save space. The persona determines: the system prompt (via `"Prompts"`, `"Pre"`, `"PromptTemplate"`, and `"BasePrompt"` keys), available tools (the `"Tools"` setting depends on `"LLMEvaluator"` and `"ToolsEnabled"` in `$autoSettingKeyDependencies`; `selectTools` in `Tools/Common.wl` uses the persona name to look up per-persona tool selections), and the output cell dingbat icon (`makeOutputDingbat` and `makeActiveOutputDingbat` in `SendChat.wl` extract the persona's `"PersonaIcon"` or `"Icon"` key). When `"RawOutput"` is enabled during chat evaluation, the persona is overridden with `GetCachedPersonaData["RawModel"]` in `sendChat` (`SendChat.wl`). `CreateChatDrivenNotebook` defaults this to `"PlainChat"`. Exposed in the notebook preferences UI as a persona PopupMenu selector under the "Notebooks" section (`PreferencesContent.wl`). Also central to the chat action menu in `UI.wl`, where each persona appears as a selectable menu item that writes to `{TaggingRules, "ChatNotebookSettings", "LLMEvaluator"}`, with a "Reset" option that sets the value to `Inherited`. Listed in `$nonInheritedPersonaValues`, so it retains its value from the notebook/cell scope rather than inheriting from persona configurations. No dependencies in `$autoSettingKeyDependencies` (but `"Tools"` depends on it). No model-specific overrides exist in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed to the LLM service). |
| `"PersonaFavorites"` | N/A | List of persona name strings that are marked as favorites, controlling which personas appear at the top of the persona selector menu and their display order. Not included in `$defaultChatSettings` — the setting is lazily initialized at `$FrontEnd` scope. When first accessed, if the value does not match `{___String}`, `filterPersonas` (`UI.wl`) initializes it to `{"CodeAssistant", "CodeWriter", "PlainChat"}`; the `CreatePersonaManagerPanel` function (`PersonaManager.wl`) similarly falls back to `$corePersonaNames` (`{"CodeAssistant", "CodeWriter", "PlainChat", "RawModel"}`) via `Replace[..., Except[{___String}] :> $corePersonaNames]`. Always read from and written to `$FrontEnd` scope (global persistent setting). Used in two primary locations: (1) **Persona selector menu ordering** (`filterPersonas` in `UI.wl`): favorites are placed first in their stored order via `KeyTake[personas, favorites]`, followed by the remaining visible personas in alphabetical order via `KeySort @ KeyTake[personas, Complement[Keys[personas], favorites]]`. (2) **Persona Manager dialog** (`CreatePersonaManagerPanel` in `PersonaManager.wl`): the `favorites` variable is initialized from this setting on panel creation, favorites are listed before non-favorites in the management grid with a visual divider, and the updated favorites list is saved back to `CurrentChatSettings[$FrontEnd, "PersonaFavorites"]` on panel deinitialization. When the "Personas" preferences are reset (`resetChatPreferences["Personas"]` in `PreferencesContent.wl`), the value is set to `$corePersonaNames`. Listed in `$nonInheritedPersonaValues` (`Settings.wl`), so it retains its value from notebook/cell scope rather than inheriting from persona configurations. Excluded from debug/diagnostic data (listed in `$droppedSettingsKeys` in `Common.wl`). No dependencies in `$autoSettingKeyDependencies`. No model-specific overrides exist in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed to the LLM service). Exposed indirectly in the preferences UI via the Persona Manager panel. |
| `"VisiblePersonas"` | `$corePersonaNames` | List of persona name strings controlling which personas appear in the persona selector UI. The default value is `$corePersonaNames`, defined in `Personas.wl` as `{"CodeAssistant", "CodeWriter", "PlainChat", "RawModel"}`. Always read from and written to `$FrontEnd` scope (global persistent setting). Used primarily in `filterPersonas` (`UI.wl`): when building the persona selector menu, `KeyTake[personas, CurrentChatSettings[$FrontEnd, "VisiblePersonas"]]` filters the full persona data to only include personas in this list; personas not in this list are hidden from the menu. The persona selector then orders visible personas by `PersonaFavorites` (favorites first, then remaining visible personas alphabetically). Lazy initialization occurs in `filterPersonas` (`UI.wl`): if the value does not match `{___String}`, it is set to `DeleteCases[Keys[personas], Alternatives["Birdnardo", "RawModel", "Wolfie"]]`, excluding certain personas from the default visible set. The `CreatePersonaManagerPanel` (`PersonaManager.wl`) sanitizes this setting on initialization by intersecting it with the keys of `$CachedPersonaData` to remove any stale persona names that no longer exist. In the Persona Manager dialog, each persona row includes a checkbox (`addRemovePersonaListingCheckbox`) that toggles membership in this list: checking adds the persona name via `Union`, unchecking removes it via `Complement`. When a new persona is installed from the Prompt Repository (`ResourceInstaller.wl`), `addToVisiblePersonas` automatically appends the new persona name to this list via `Union @ Append[...]`. When the "Personas" preferences are reset (`resetChatPreferences["Personas"]` in `PreferencesContent.wl`), the value is reset to `$corePersonaNames`. Not listed in `$nonInheritedPersonaValues`. Excluded from debug/diagnostic data (listed in `$droppedSettingsKeys` in `Common.wl`). No dependencies in `$autoSettingKeyDependencies`. No model-specific overrides exist in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed to the LLM service). Not listed in `$popOutSettings`. Exposed in the preferences UI via the Persona Manager panel checkboxes. |
| `"ChatDrivenNotebook"` | `False` | **Deprecated.** Whether the entire notebook operates in "chat-driven" mode rather than the default "chat-enabled" mode. When `True`, new cells default to `"ChatInput"` style, the persona selector prioritizes PlainChat/RawModel/CodeWriter/CodeAssistant at the top of the list (`UI.wl` `filterPersonas`), and the cloud toolbar displays "Chat-Driven Notebook" instead of "Chat Notebook" (`CloudToolbar.wl`). Used by `CreateChatDrivenNotebook[]`, which wraps `CreateChatNotebook` with `"ChatDrivenNotebook" -> True`, `"LLMEvaluator" -> "PlainChat"`, and `DefaultNewCellStyle -> "ChatInput"`. This is a non-inherited persona value (listed in `$nonInheritedPersonaValues`). |
| `"InitialChatCell"` | `True` | Whether to create an initial empty chat input cell when opening a new chat notebook. When `True`, `CreateChatNotebook` inserts an empty `"ChatInput"` cell (via `initialChatCells` in `CreateChatNotebook.wl`); for cloud notebooks (`$cloudNotebooks`), an additional selection-mover cell is appended to position the cursor. When `False`, the notebook is created with no initial cells. The value is evaluated via `TrueQ`, so only an explicit `True` creates the cell; `Automatic` or other non-boolean values behave as `False`. This is an **unsaved setting** (listed in `$unsavedSettings` in `CreateChatNotebook.wl`): it is used only at notebook creation time and is explicitly dropped from the notebook's `TaggingRules` by `makeChatNotebookSettings`, so it does not persist in the saved notebook. Defined as an option of `CreateChatNotebook` (inherited from `$defaultChatSettings`). When creating a notebook from a `ChatObject`, `initialChatCells` is locally overridden to return the converted message cells instead. No dependencies on other settings. No model-specific overrides exist in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed to the LLM service). Listed in `$nonInheritedPersonaValues`, so it retains its value from notebook/cell scope rather than inheriting from persona configurations. Not exposed in the preferences UI. |
| `"ChatInputIndicator"` | `Automatic` | Text prefix prepended to `"ChatInput"` cells when serializing notebook content for the LLM. When `Automatic`, resolves to `"\|01f4ac"` (speech balloon emoji). Can be any string (e.g., `"[USER]"`), or `None`/`""` to disable the indicator entirely. Only applied when the content is mixed (i.e., when `mixedContentQ` returns `True` in `ChatMessages.wl`, indicating the conversation includes both chat input cells and other cell types). When the indicator is used, the `"ChatInputIndicator"` base prompt component (`Prompting.wl`) is automatically included in the system prompt to explain the indicator's meaning to the LLM: it tells the model that cells prefixed with this symbol are actual user messages, while other cells are context. The indicator text is distinct from cell dingbats controlled by `"SetCellDingbat"`, which are visual notebook icons. The indicator symbol is set per chat evaluation via `chatIndicatorSymbol` in `SendChat.wl` and stored in the global `$chatIndicatorSymbol` variable (`Common.wl`). No model-specific overrides exist. |
| `"SetCellDingbat"` | `True` | Whether to set cell dingbats (icons) on chat cells. When `True`, chat input cells receive dingbat icons (e.g., `"ChatInputCellDingbat"` / `"ChatInputActiveCellDingbat"` template boxes) and assistant output cells receive persona-based dingbat icons (via `makeActiveOutputDingbat` during streaming and `makeOutputDingbat` after completion in `SendChat.wl`). When `False`, no `CellDingbat` option is set on generated cells. Checked via `TrueQ @ settings["SetCellDingbat"]` for output cells and `settings["SetCellDingbat"]` (truthy) for input cells; additionally guarded by `! TrueQ @ $cloudNotebooks`, so dingbats are never set in cloud notebooks regardless of this setting. The input cell dingbat logic replaces `"ChatInputActiveCellDingbat"` with `"ChatInputCellDingbat"` once evaluation begins. For output cells, the active dingbat includes a `"ChatOutputStopButtonWrapper"` that provides a stop button during streaming; after completion, the dingbat is replaced with a static persona icon. For tabbed output (`"TabbedOutput"`), an `"AssistantIconTabbed"` wrapper is used instead. Also used in `ConvertChatNotebook.wl`: `updateCellDingbats` applies `$evaluatedChatInputDingbat` to input cells and a persona-derived dingbat to output cells when converting messages to notebook format. Overridden to `False` in workspace chat (`$workspaceDefaultSettings` in `StylesheetBuilder.wl` and `WorkspaceChat.nb` stylesheet), notebook assistance workspace settings (`$notebookAssistanceWorkspaceSettings`), and sidebar chat settings (`$notebookAssistanceSidebarSettings`) in `ShowNotebookAssistance.wl`. No dependencies in `$autoSettingKeyDependencies`. No model-specific overrides exist in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed to the LLM service). Not exposed in the preferences UI. |
| `"EnableChatGroupSettings"` | `False` | Whether chat group-level settings are enabled. When `True`, during chat evaluation (`SendChat.wl`), `getChatGroupSettings` is called on the evaluation cell to retrieve prompt text from parent group header cells. The feature walks backward through notebook cells to find parent group headers using cell grouping rules (`"TitleGrouping"` and `"SectionGrouping"`) and collects `"Prompt"` values stored in their `TaggingRules` at the path `"ChatNotebookSettings"` → `"ChatGroupSettings"` → `"Prompt"`. Multiple prompts from different grouping levels are joined with `"\n\n"`. The collected group prompt is stored in the settings as `"ChatGroupSettings"` and incorporated into the system prompt via `buildSystemPrompt` in `ChatMessages.wl`, where it appears as the `"Group"` section in the prompt template (between `"Pre"` and `"Base"` sections). When `False`, no group settings are resolved and the `"Group"` section of the system prompt is omitted. Implementation is in `ChatGroups.wl`. Not exposed in the preferences UI. No model-specific overrides exist. |
| `"AllowSelectionContext"` | `Automatic` | Whether to allow the current selection to be used as context. Resolves to `True` when using workspace chat, inline chat, or sidebar chat. |
| `"CurrentPreferencesTab"` | `"Services"` | Persists the user's last-selected tab in the Chatbook preferences dialog. When the preferences dialog opens, the tab is initialized from this setting (defaulting to `"Services"` if unset); when the dialog closes, the current tab selection is saved back. The `openPreferencesPage` function in `PreferencesContent.wl` also writes to this setting at `$FrontEnd` scope to navigate directly to a specific preferences page. This is a non-inherited persona value (listed in `$nonInheritedPersonaValues`) and is excluded from debug/diagnostic data (listed in `$droppedSettingsKeys` in `Common.wl`). Not included in `$defaultChatSettings`. No model-specific overrides exist. |

## Storage & Conversations

| Setting | Default | Description |
| ------- | ------- | ----------- |
| `"ConversationUUID"` | `None` | UUID identifying the current conversation. `None` means no conversation tracking. When a conversation is saved (via `SaveChat`), `ensureConversationUUID` in `Storage.wl` checks whether the current setting is a valid string; if not, it generates a new UUID via `CreateUUID[]` and writes it back to the notebook or cell's `CurrentChatSettings`. Used as the primary key for persistent conversation storage (`Storage.wl`), chat search indexing (`Search.wl`), and chat history listings (`ChatModes/UI.wl`). The `AutoSaveConversations` setting depends on `ConversationUUID` being a valid string (along with `AppName`). Chat modes may override this: workspace chat and sidebar chat automatically generate a new UUID when starting a new conversation (`ChatModes/UI.wl`) or when initialized via NotebookAssistance settings (`ChatModes/ShowNotebookAssistance.wl`); inline chat does not set a UUID (falls back to `None`). When loading a saved conversation, the stored UUID is restored to `CurrentChatSettings` for the target notebook or cell. |
| `"AutoSaveConversations"` | `Automatic` | Whether to automatically save conversations to persistent storage after chat evaluations. When `Automatic`, resolves to `True` if both `AppName` is a valid string and `ConversationUUID` is a valid string; otherwise resolves to `False`. Depends on `"AppName"` and `"ConversationUUID"`. When `True`, conversations are saved after each chat evaluation, subject to the `"MinimumResponsesToSave"` threshold. Chat modes may override this: workspace chat and sidebar chat set it to `True` (with a new `ConversationUUID`), while inline chat sets it to `False`. |
| `"AppName"` | `Automatic` | Application name used to namespace saved conversations, search indexes, and chat history listings. When `Automatic`, resolves to `$defaultAppName` (`"Default"`). When set to a non-default string value, also establishes a service caller context via `setServiceCaller`. Chat modes may override this (e.g., NotebookAssistance uses `"NotebookAssistance"`). The `AutoSaveConversations` setting depends on `AppName` being a valid string. |
| `"MinimumResponsesToSave"` | `1` | Minimum number of assistant responses required before a conversation is automatically saved. Used by `autoSaveQ` in `Storage.wl` to gate auto-saving: after each chat evaluation, the function counts messages with `"Role" -> "Assistant"` in the conversation and only proceeds with saving if the count is greater than or equal to this value. The value must be a positive integer (`_Integer? Positive`); invalid values trigger an `"MinResponses"` confirmation failure. The default of `1` means that auto-saving occurs as soon as the first assistant response is present. Chat mode overrides: the `"GettingStarted"` alias in `ShowNotebookAssistance.wl` sets this to `2`, requiring at least two assistant responses before saving (preventing the initial getting-started prompt from being saved as a conversation). This setting only takes effect when `"AutoSaveConversations"` resolves to `True` (which itself requires valid `"AppName"` and `"ConversationUUID"` values). No dependencies in `$autoSettingKeyDependencies`. No model-specific overrides exist in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook for conversation storage). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not exposed in the preferences UI. |
| `"TargetCloudObject"` | `Automatic` | Target `CloudObject` location for deploying cloud-based chat notebooks via `CreateChatNotebook`. When `Automatic`, `CloudDeploy` is called without a target location, allowing the Wolfram Cloud to assign a default URL. When set to a `CloudObject`, the notebook is deployed to that specific cloud location. Used exclusively in the cloud notebook creation path (`createCloudChatNotebook` in `CreateChatNotebook.wl`): the value is read via `OptionValue[CreateChatNotebook, validOpts, "TargetCloudObject"]` and passed to `deployCloudNotebook`, which calls `CloudDeploy[nb, obj, CloudObjectURLType -> "Environment"]` if the value matches `$$cloudObject` (`HoldPattern[_CloudObject]`), or `CloudDeploy[nb, CloudObjectURLType -> "Environment"]` otherwise. This is an **unsaved setting** (listed in `$unsavedSettings` in `CreateChatNotebook.wl`): it is used only at notebook creation time and is explicitly dropped from the notebook's `TaggingRules` by `makeChatNotebookSettings`, so it does not persist in the saved notebook. Cloud notebook creation is triggered when `$cloudNotebooks` is `True`. No dependencies in `$autoSettingKeyDependencies`. No model-specific overrides exist in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed to the LLM service). Not listed in `$nonInheritedPersonaValues`. Not exposed in the preferences UI. |

## Advanced / Internal

| Setting | Default | Description |
| ------- | ------- | ----------- |
| `"Tokenizer"` | `Automatic` | Tokenizer function used for token counting throughout the chat pipeline. When `Automatic`, resolved via `getTokenizer` (`ChatMessages.wl`) using a three-step fallback: (1) if an explicit non-`Automatic` tokenizer function is already set, it is used directly; (2) if `"TokenizerName"` is a string, the corresponding cached tokenizer is looked up via `cachedTokenizer`; (3) otherwise, the tokenizer is derived from the `"Model"` setting by extracting the model name and matching it to a known tokenizer. Pre-cached tokenizer functions exist for `"chat-bison"` (UTF-8 byte encoding via `ToCharacterCode`), `"gpt-4-vision"` and `"gpt-4o"` (with special image token counting for `Graphics` content), `"claude-3"` (with Claude-specific image token counting), and `"generic"` (GPT-2 fallback). Additional tokenizers are loaded on demand from `.wxf` files in the `Assets/Tokenizers/` directory, or discovered via `Wolfram`LLMFunctions`Utilities`Tokenization`FindTokenizer`; if no model-specific tokenizer is found, the generic GPT-2 tokenizer is used as a fallback. The resolved tokenizer is applied via `applyTokenizer` in `tokenCount` (`ChatMessages.wl`), which tokenizes message content and returns the token list length. The `"Tokenizer"` value can also be set to a custom function (any expression other than `Automatic`/`$$unspecified`), in which case that function is used directly; if the custom value is a string, it is treated as a tokenizer name and the `"TokenizerName"` key is set to the resolved name while `"Tokenizer"` is reset to `Automatic` during `resolveAutoSettings` (`Settings.wl`). Explicitly dropped from saved notebook settings via `toSmallSettings` (`SendChat.wl`, `KeyDrop[as, {"OpenAIKey", "Tokenizer"}]`) because tokenizer functions cannot be serialized. Serialized to a name-based reference (`<| "_Object" -> "Tokenizer", "Data" -> name |>`) in `Feedback.wl` for diagnostic reporting. Depends on `"TokenizerName"` in `$autoSettingKeyDependencies`, which in turn depends on `"Model"`. No model-specific overrides exist in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed to the LLM service via `LLMConfiguration`; used internally by Chatbook for token counting). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not in `$popOutSettings`. Not exposed in the preferences UI. |
| `"HandlerFunctions"` | `$DefaultChatHandlerFunctions` | Callback functions invoked at various stages of chat processing. The value is an `Association` mapping event name strings to handler functions (or `None` to skip). The default value `$DefaultChatHandlerFunctions` (`Settings.wl`) defines 9 event keys, all defaulting to `None`: `"AppendCitationsStart"`, `"AppendCitationsEnd"`, `"ChatAbort"`, `"ChatPost"`, `"ChatPre"`, `"PromptGeneratorEnd"`, `"PromptGeneratorStart"`, `"ToolRequestReceived"`, and `"ToolResponseGenerated"`. The `"ChatAbort"`, `"ChatPost"`, and `"ChatPre"` entries use `RuleDelayed` (`:>`) pointing to global variables `$ChatAbort`, `$ChatPost`, and `$ChatPre` (all initially `None`), allowing runtime reassignment without modifying the association. A 10th event, `"ToolResponseReceived"`, is also dispatched via `applyHandlerFunction` (`SendChat.wl`) but is not included in the default association (falls back to `None` via `getHandlerFunction`). Custom handler values are merged with defaults during resolution: `resolveHandlers` in `Handlers.wl` creates a new association with `$DefaultChatHandlerFunctions` as the base, overlaid with user-provided handlers (after `replaceCellContext` processing), plus a `"Resolved" -> True` marker to prevent re-resolution. Resolution occurs during `resolveAutoSettings` (`Settings.wl`), where `getHandlerFunctions` is called on the settings and the result replaces the `"HandlerFunctions"` key. Each handler function is invoked via `applyHandlerFunction` (`Handlers.wl`), which constructs an argument association containing: `"EventName"` (the event type string), `"ChatNotebookSettings"` (current settings with `"Data"` and `"OpenAIKey"` keys dropped), and event-specific data. This argument association is accumulated in the `$ChatHandlerData` global variable (publicly exported) via `addHandlerArguments`, which merges new data with existing handler state (supporting nested association merging). The handler receives `$ChatHandlerData` with `"DefaultProcessingFunction"` dropped. Event dispatch locations: `"ChatPre"` is called in `sendChat` (`SendChat.wl`) before chat submission, with `"EvaluationCell"` and `"Messages"` in the arguments; `"ChatPost"` and `"ChatAbort"` are called in `applyChatPost` (`Actions.wl`) after chat completion or abort, with `"ChatObject"` and `"NotebookObject"` in the arguments; `"ToolRequestReceived"` is called after parsing a tool call (`SendChat.wl`), with `"ToolRequest"` in the arguments; `"ToolResponseGenerated"` is called after generating a tool response (`SendChat.wl`), with `"ToolResponse"` and `"ToolResponseString"` in the arguments; `"ToolResponseReceived"` is called after the tool response is formatted and ready to send back (`SendChat.wl`), with `"ToolResponse"` in the arguments; `"PromptGeneratorStart"` and `"PromptGeneratorEnd"` are called in `DefaultPromptGenerators.wl` around each prompt generator execution, with `"PromptGenerator"` in the arguments (and `"PromptGeneratorResult"` added for the end event); `"AppendCitationsStart"` and `"AppendCitationsEnd"` are called in `Citations.wl` around citation generation, with `"Sources"` and `"CitationString"` respectively. In the streaming chat submission path (`chatHandlers` in `SendChat.wl`), the resolved handlers are passed to `LLMServices`ChatSubmit` via the `HandlerFunctions` parameter, but `"ChatPost"`, `"ChatPre"`, and `"Resolved"` keys are dropped (listed in `$chatSubmitDroppedHandlers`). The `chatHandlers` function also wraps custom `"BodyChunkReceived"` and `"TaskFinished"` handlers (if provided) inside Chatbook's own streaming logic, calling the user's handler before Chatbook's processing for each body chunk and after task completion. Not passed through `LLMConfiguration` (not in `$llmConfigPassedKeys`). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. No dependencies in `$autoSettingKeyDependencies`. No model-specific overrides exist in `$modelAutoSettings`. Not exposed in the preferences UI. |
| `"HandlerFunctionsKeys"` | `Automatic` | Keys to include in the handler functions callback data passed to `LLMServices`ChatSubmit` and `URLSubmit`. Controls which fields from the streaming response are available to handler functions. When `Automatic` or unspecified, resolves to `$defaultHandlerKeys` (`SendChat.wl`): `{"Body", "BodyChunk", "BodyChunkProcessed", "StatusCode", "TaskStatus", "EventName"}`. When a list of strings, the user-provided keys are merged with `$defaultHandlerKeys` via `Union` (so the default keys are always included). When a single string, it is treated as a one-element list. Invalid values trigger an `"InvalidHandlerKeys"` warning (`Common.wl`) and fall back to `$defaultHandlerKeys`. Resolution is performed by `chatHandlerFunctionsKeys` (`SendChat.wl`), which is called from `resolveAutoSetting0` (`Settings.wl`). The resolved value is passed directly as the `HandlerFunctionsKeys` parameter to `LLMServices`ChatSubmit` (for LLMServices-based chat) and `URLSubmit` (for legacy HTTP-based chat) in `chatSubmit0` (`SendChat.wl`). Also used in other `URLSubmit` calls outside of chat: `VectorDatabases.wl` uses `{"ByteCountDownloaded", "StatusCode"}` for vector database downloads, and `RelatedWolframAlphaResults.wl` uses `{"StatusCode", "BodyByteArray"}` for Wolfram Alpha result fetching. Not passed through `LLMConfiguration` (not in `$llmConfigPassedKeys`). Depends on `"EnableLLMServices"` for resolution ordering in `$autoSettingKeyDependencies`. Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. No model-specific overrides exist in `$modelAutoSettings`. Not exposed in the preferences UI. |
| `"InheritanceTest"` | N/A | Internal diagnostic flag used by the settings inheritance verification system. Not a user-configurable setting and not included in `$defaultChatSettings`. During `verifyInheritance0` (`Settings.wl`), this flag is set to `True` at the `$FrontEnd` scope via `setCurrentValue[fe, {TaggingRules, "ChatNotebookSettings", "InheritanceTest"}, True]` to mark that the inheritance chain for tagging rules has been properly initialized. Subsequent calls to `verifyInheritance` check this flag via `inheritingQ`, which reads `AbsoluteCurrentValue[obj, {TaggingRules, "ChatNotebookSettings", "InheritanceTest"}]` — if `True` (or if the read fails), the object is considered to have valid inheritance and initialization is skipped. During `repairTaggingRules`, the flag is explicitly removed from child objects (notebooks, cells) so that it only persists at the top-level `FrontEndObject`, preventing it from appearing as an explicit override in child scopes. The `verifyInheritance` function is called by `currentChatSettings0` before reading or writing settings, ensuring the inheritance chain is intact. Listed in `$nonInheritedPersonaValues`, so it retains its value from notebook/cell scope rather than inheriting from persona configurations. No model-specific overrides exist. Not exposed in the preferences UI. |
| `"ProcessingFunctions"` | `$DefaultChatProcessingFunctions` | An `Association` of callback functions that control the chat processing pipeline, allowing complete customization of how cells are converted to messages, how messages are post-processed, how chat requests are submitted, how output is formatted, and how output cells are written. The default value `$DefaultChatProcessingFunctions` (`Settings.wl`) is defined using `RuleDelayed` (`:>`) in `$defaultChatSettings` so it is evaluated lazily. It contains six keys: `"CellToChatMessage" -> CellToChatMessage` (converts individual notebook `Cell` expressions to message Associations with `"Role"` and `"Content"` keys; retrieved via `getCellMessageFunction` in `ChatMessages.wl` and wrapped by `checkedMessageFunction`, which validates that custom functions return valid message results — plain strings are auto-wrapped with the cell's role, and invalid results fall back to the default `CellToChatMessage`); `"ChatMessages" -> (#1 &)` (identity function that post-processes the combined message list after construction and prompt generator augmentation; called via `applyProcessingFunction[settings, "ChatMessages", HoldComplete[combined, $ChatHandlerData]]` in `augmentChatMessages` (`ChatMessages.wl`); the result is validated against `$$validMessageResults` and if invalid, an `"InvalidMessages"` warning is printed and the original messages are used instead); `"ChatSubmit" -> Automatic` (submits the prepared messages to the LLM service; when `Automatic`, resolves to `LLMServices``ChatSubmit` in the modern LLMServices path or `URLSubmit` in the legacy HTTP path, selected via the `"DefaultSubmitFunction"` parameter passed to `applyProcessingFunction`; called in `chatSubmit0` (`SendChat.wl`) with the standardized messages, `LLMConfiguration`, authentication, handler functions, and handler function keys as arguments); `"FormatChatOutput" -> FormatChatOutput` (formats the LLM response text for notebook display; retrieved via `getFormattingFunction` in `SendChat.wl`, which wraps it to set `$ChatHandlerData["EventName"]` to `"FormatChatOutput"` before calling; `FormatChatOutput` (`Formatting.wl`) dispatches on a status Association with a `"Status"` key — `"Streaming"` for live-formatted output during streaming, `"Finished"` for final formatting, `"Waiting"` for a progress indicator — and converts Markdown to formatted notebook expressions via `reformatTextData`); `"FormatToolCall" -> FormatToolCall` (formats tool call data for display in the notebook; retrieved both via `getToolFormatter` in `SendChat.wl` (which wraps it similarly to set `$ChatHandlerData["EventName"]`) and directly from `$ChatHandlerData["ChatNotebookSettings", "ProcessingFunctions", "FormatToolCall"]` in `inlineToolCall` (`Formatting.wl`) for inline tool call rendering; `FormatToolCall` (`Formatting.wl`) takes a raw tool call string and parsed data Association, with an optional info Association containing `"Status"`); `"WriteChatOutputCell" -> WriteChatOutputCell` (writes the formatted output cell to the notebook; called via `applyProcessingFunction[settings, "WriteChatOutputCell", HoldComplete[cell, new, info]]` inside `createTask` in `writeReformattedCell` (`SendChat.wl`); `WriteChatOutputCell` (`SendChat.wl`) has two main definitions: for inline chat (`$InlineChat`), it delegates to `writeInlineChatOutputCell`; for regular chat, it uses `NotebookWrite` to insert the cell, sets cell tags from the `"ExpressionUUID"` in the info Association, attaches the chat output menu via `attachChatOutputMenu`, and handles scrolling via `scrollOutput` based on the `"ScrollOutput"` info key). During `resolveAutoSettings` (`Settings.wl`), the value is resolved via `getProcessingFunctions` (`Handlers.wl`), which calls `resolveFunctions`: this merges user-provided overrides on top of `$DefaultChatProcessingFunctions`, applies `replaceCellContext` to convert `$CellContext` symbols to the global context, and marks the result with `"Resolved" -> True` to prevent re-resolution. Invalid values trigger an `"InvalidFunctions"` warning (`Common.wl`) and fall back to defaults. Each processing function is invoked via `applyProcessingFunction` (`Handlers.wl`), which retrieves the function via `getProcessingFunction` (falling back through the resolved association, then `$DefaultChatProcessingFunctions`), merges parameters with `$ChatHandlerData` (adding `"ChatNotebookSettings"` and `"DefaultProcessingFunction"` to the handler data), applies the function to its held arguments, and logs timing via `LogChatTiming`. The `$DefaultChatProcessingFunctions` variable is publicly exported (`Main.wl`). No dependencies in `$autoSettingKeyDependencies`. No model-specific overrides exist in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook for pipeline customization). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not listed in `$popOutSettings`. Not listed in `$droppedSettingsKeys`. Not exposed in the preferences UI. |
| `"ConversionRules"` | `None` | Custom transformation rules applied to notebook cells before they are serialized to chat message strings. When `None` (default), cells are passed through unmodified. When set to a list of replacement rules, the rules are compiled into a `Dispatch` table (cached for reuse) and applied to each cell via `ReplaceRepeated` inside `CellToString` (`Serialization.wl`) before string conversion occurs. This enables custom box-level or expression-level transformations of cell content prior to sending it to the LLM. The setting value is read from `CurrentChatSettings` in `makeChatMessages` (`ChatMessages.wl`) and stored in the dynamic variable `$conversionRules`, which `CellToString` picks up as its default `"ConversionRules"` option. Invalid values (neither a rule list nor a valid `Dispatch` table) trigger an `"InvalidConversionRules"` warning and fall back to `None`. |
| `"ExperimentalFeatures"` | `Automatic` | List of enabled experimental feature names, resolved dynamically from other settings. When `Automatic`, the `autoExperimentalFeatures` function (`Settings.wl`) builds a list based on two conditions: `"RelatedWolframAlphaResults"` is included if `"WolframAlphaCAGEnabled"` is `True` or `"RelatedWolframAlphaResults"` is in the `"PromptGenerators"` list; `"RelatedWebSearchResults"` is included if `"WebSearchRAGMethod"` is `"Tavily"` or `"WebSearch"` is in the `"PromptGenerators"` list. The resolved list is stored in the `$experimentalFeatures` global variable (`Settings.wl`) and preserved across handler evaluation via `ChatState.wl`. Individual features are checked at runtime via `featureEnabledQ` (`Settings.wl`), which tests membership in the resolved list. The primary consumer is `resolvePromptGenerators` in `PromptGenerators/Common.wl`, which appends `"RelatedWolframAlphaResults"` and/or `"WebSearch"` to the active prompt generators list based on feature flags. Depends on `"WolframAlphaCAGEnabled"`, `"WebSearchRAGMethod"`, and `"PromptGenerators"` (declared in `$autoSettingKeyDependencies`). No model-specific overrides exist. Not exposed in the preferences UI. |
| `"OpenAIKey"` | `Automatic` | **Deprecated.** OpenAI API key used for direct OpenAI API authentication in the legacy (non-LLMServices) HTTP request path. Only used when `EnableLLMServices` resolves to `False` (i.e., the `Wolfram/LLMFunctions` paclet is not installed or `EnableLLMServices` is explicitly `False`). When `Automatic`, the key is resolved at chat time by `toAPIKey` (`Actions.wl`), which checks in order: (1) `SystemCredential["OPENAI_API_KEY"]`, (2) `Environment["OPENAI_API_KEY"]`, (3) an interactive API key dialog (`apiKeyDialog`); if none produces a valid string, throws a `"NoAPIKey"` failure. If already a string, the value is used directly. The resolved key is assigned back into the settings association (`settings["OpenAIKey"] = key`) in the legacy `sendChat` overload (`SendChat.wl`), then used by `makeHTTPRequest` (`SendChat.wl`) to construct the `"Authorization"` header (`"Bearer " <> key`) for the `HTTPRequest` sent to `OpenAIAPICompletionURL`. The entire legacy `sendChat` overload and `toAPIKey` function are marked with TODO comments indicating they are obsolete once LLMServices is widely available. In the modern code path (`$useLLMServices` is `True`), this setting is completely unused — the `LLMServices` framework handles authentication internally via the `Authentication` setting and service-specific credential management. **Security handling**: the `maskOpenAIKey` function (`Common.wl`) replaces actual key values with `"**********"` in all diagnostic, debug, and error output. The key is explicitly dropped from: saved notebook settings via `toSmallSettings` (`SendChat.wl`, `KeyDrop[as, {"OpenAIKey", "Tokenizer"}]`), handler function callback data via `$settingsDroppedKeys` (`Handlers.wl`), and notebook settings diagnostic output (`Common.wl`). Not resolved during `resolveAutoSettings` — there is no `resolveAutoSetting0` case for `"OpenAIKey"`, so `Automatic` persists until `toAPIKey` is called at chat time. No dependencies in `$autoSettingKeyDependencies`. No model-specific overrides exist in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed through `LLMConfiguration`). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not in `$popOutSettings`. Not exposed in the preferences UI. |
| `"OpenAIAPICompletionURL"` | `"https://api.openai.com/v1/chat/completions"` | **Deprecated.** OpenAI API completion endpoint URL, used only in the legacy (non-LLMServices) HTTP request path. The default value is a fixed string (`"https://api.openai.com/v1/chat/completions"`), not `Automatic`. Only used when `$useLLMServices` is `False` (i.e., when the `EnableLLMServices` setting resolves to `False` or the required `Wolfram/LLMFunctions` paclet is not installed). In this legacy path, `makeHTTPRequest` (`SendChat.wl`) reads the value from settings via `Lookup[settings, "OpenAIAPICompletionURL"]`, confirms it is a string, and uses it as the URL for the `HTTPRequest` sent to the OpenAI chat completions API, with the `"OpenAIKey"` setting providing the `"Authorization"` header. The entire `sendChat` overload that uses this setting is marked with a TODO comment: `"this definition is obsolete once LLMServices is widely available"`. When `$useLLMServices` is `True` (the primary/modern code path), this setting is completely unused — the LLMServices framework handles endpoint routing internally. Not included in `$llmConfigPassedKeys` (`SendChat.wl`), so it is NOT passed through `LLMConfiguration` when using the LLMServices framework. No model-specific overrides exist in `$modelAutoSettings`. No dependencies in `$autoSettingKeyDependencies`. Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not listed in `$popOutSettings`. Conditionally exposed in the preferences UI (`PreferencesContent.wl`): the `makeOpenAICompletionURLInput` function returns `Nothing` when `$useLLMServices` is `True` (hiding the control entirely) and renders a string input field in the "Notebooks" tab via `highlightControl` when `$useLLMServices` is `False`, reading and writing `CurrentChatSettings[$preferencesScope, "OpenAIAPICompletionURL"]`. |

---

## Model-Specific Auto Settings

When a setting has a value of `Automatic`, the resolution pipeline checks `$modelAutoSettings` for a model-specific default. Settings are looked up in order of specificity:

1. Service + model name (e.g., `$modelAutoSettings["Anthropic", "Claude4"]`)
2. Service + model ID
3. Service + model family
4. Any service + model name (e.g., `$modelAutoSettings[Automatic, "GPT4Omni"]`)
5. Any service + model ID
6. Any service + model family
7. Service-level default (e.g., `$modelAutoSettings["Anthropic", Automatic]`)
8. Global default (`$modelAutoSettings[Automatic, Automatic]`)

The first match wins. For details on how to add support for new models, see [TODO: How to Add Support for New Models].

### Global Auto Setting Defaults

These are the fallback values from `$modelAutoSettings[Automatic, Automatic]` when no model-specific override exists:

| Setting | Default |
| ------- | ------- |
| `"AppendCitations"` | `False` |
| `"ConvertSystemRoleToUser"` | `False` |
| `"EndToken"` | `"/end"` |
| `"ExcludedBasePrompts"` | `{ParentList}` |
| `"PresencePenalty"` | `0.1` |
| `"ReplaceUnicodeCharacters"` | `False` |
| `"ShowProgressText"` | `True` |
| `"SplitToolResponseMessages"` | `False` |
| `"Temperature"` | `0.7` |
| `"ToolResponseRole"` | `"System"` |

### Non-Inherited Persona Values

The following settings are not inherited from the persona configuration when resolving settings. They retain their value from the notebook/cell scope:

- `"ChatDrivenNotebook"`
- `"CurrentPreferencesTab"`
- `"EnableLLMServices"`
- `"Icon"`
- `"InheritanceTest"`
- `"InitialChatCell"`
- `"LLMEvaluator"`
- `"PersonaFavorites"`
- `"ServiceDefaultModel"`
