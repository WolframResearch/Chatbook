# Settings Listing

## Overview

Chatbook settings control LLM behavior, prompt construction, tool usage, formatting, and UI behavior. Settings are stored in notebook tagging rules under `"ChatNotebookSettings"` and follow a hierarchical inheritance model.

### Accessing Settings

Use `CurrentChatSettings` to read and write settings:

```wl
(* Read global settings *)
CurrentChatSettings[]
CurrentChatSettings["Temperature"]

(* Read settings scoped to a notebook or cell *)
CurrentChatSettings[notebookObj]
CurrentChatSettings[cellObj, "Model"]

(* Write settings *)
CurrentChatSettings[$FrontEnd, "Temperature"] = 0.5
CurrentChatSettings[notebookObj, "AutoFormat"] = False

(* Reset to inherited value *)
CurrentChatSettings[notebookObj, "Temperature"] =.
```

### Inheritance Model

Settings resolve through a hierarchy, with more specific scopes overriding broader ones:

| Scope              | Description                      |
| ------------------ | -------------------------------- |
| `CellObject`       | Per-cell override                |
| `NotebookObject`   | Per-notebook settings            |
| `$FrontEndSession` | Session-wide (non-persistent)    |
| `$FrontEnd`        | Global persistent settings       |

If a setting is not defined at a given scope, it inherits from the next broader scope. A value of `Inherited` explicitly defers to the parent scope.

### Automatic Values

Many settings default to `Automatic`, meaning they are resolved at runtime based on the current model, service, and other settings. The resolution pipeline is defined in `Settings.wl` via `resolveAutoSettings`, which evaluates `Automatic` values in topologically sorted dependency order. Model-specific defaults are looked up from `$modelAutoSettings`.

---

## Model & Service

| Setting | Default | Description |
| ------- | ------- | ----------- |
| `"Model"` | `$DefaultModel` | The LLM model specification. For Wolfram Engine 14.1+, defaults to `<\|"Service" -> "LLMKit", "Name" -> Automatic\|>`. For older versions, defaults to `<\|"Service" -> "OpenAI", "Name" -> "gpt-4o"\|>`. Can be an `Association` with `"Service"` and `"Name"` keys. |
| `"Authentication"` | `Automatic` | Authentication method for the LLM service. When `Automatic`, resolves based on the model specification: if the model has an explicit `"Authentication"` field, that value is used; if the model's `"Service"` is `"LLMKit"`, resolves to `"LLMKit"`; otherwise remains `Automatic` (uses the service's default authentication). Depends on `"Model"`. Passed directly to `LLMServices`Chat` and `LLMServices`ChatSubmit` (not via `LLMConfiguration`). |
| `"EnableLLMServices"` | `Automatic` | Whether Chatbook uses the `LLMServices` framework for LLM communication. When `Automatic`, resolves to the internal `$useLLMServices` flag, which evaluates to `True` only if `$enableLLMServices` is `Automatic` or `True` AND the `Wolfram/LLMFunctions` paclet (version 1.2.2+) is installed (`Services.wl`). When `True`, Chatbook routes all chat requests through `LLMServices`Chat`/`LLMServices`ChatSubmit`, the OpenAI completion URL input is hidden from the preferences UI (`PreferencesContent.wl`), and available services are discovered dynamically. When `False`, Chatbook falls back to direct API calls using legacy service configuration with `$fallBackServices`, and the OpenAI completion URL input is shown in the preferences UI. The setting value is read from `CurrentChatSettings` and assigned to the `$enableLLMServices` variable in `Actions.wl` before each `sendChat` call. The `sendChat` function in `SendChat.wl` has a condition `/; $useLLMServices` that gates the primary chat execution path. Other settings depend on this: `HandlerFunctionsKeys` depends on `EnableLLMServices` for resolution order, and `Multimodal` depends on both `EnableLLMServices` and `Model` (when LLM Services are disabled but the model supports multimodal, multimodal is enabled directly; when enabled, it additionally checks for multimodal paclet availability). This is a non-inherited persona value (listed in `$nonInheritedPersonaValues` in `Settings.wl`), meaning it retains its value from the notebook/cell scope rather than inheriting from the persona. No model-specific overrides exist. Not exposed directly in the preferences UI. |
| `"Multimodal"` | `Automatic` | Whether multimodal (image) input is supported. Resolved per model (e.g., `True` for Claude 4, GPT-4.1, Gemini 2+; `False` for O1-Mini, O3-Mini). |
| `"Reasoning"` | `Automatic` | Whether model reasoning/chain-of-thought is enabled. Model-specific; only supported by certain models (e.g., O-series, GPT-5). Models that don't support it return `Missing["NotSupported"]`. |

## LLM Parameters

| Setting | Default | Description |
| ------- | ------- | ----------- |
| `"Temperature"` | `Automatic` | Sampling temperature for the LLM. Higher values increase randomness. Model default: `0.7`. Some models (e.g., O4-Mini, GPT-5) return `Missing["NotSupported"]`. |
| `"FrequencyPenalty"` | `0.1` | Penalty applied to tokens based on their frequency in the text so far. Reduces repetition. Default is a fixed numeric value (`0.1`), not `Automatic`. Only used in the legacy (non-LLMServices) HTTP request path: in `makeHTTPRequest` (`SendChat.wl`), the value is looked up from settings via `Lookup[settings, "FrequencyPenalty", 0.1]` and passed to the OpenAI API as the `"frequency_penalty"` field in the request body. Values of `Automatic` or `Missing` are stripped from the request via `DeleteCases`. Not included in `$llmConfigPassedKeys` (`SendChat.wl`), so it is NOT passed through `LLMConfiguration` when using the LLMServices framework (`LLMServices`Chat`/`LLMServices`ChatSubmit`). No model-specific overrides exist in `$modelAutoSettings`. No dependencies on other settings. Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not exposed in the preferences UI. |
| `"PresencePenalty"` | `Automatic` | Penalty applied to tokens based on whether they have appeared in the text so far. Model default: `0.1`. Some models (e.g., Google Gemini, O4-Mini, GPT-5) return `Missing["NotSupported"]`. |
| `"TopP"` | `1` | Top-p (nucleus) sampling parameter. A value of `1` considers all tokens. |
| `"MaxTokens"` | `Automatic` | Maximum number of tokens in the LLM response. Resolved per model. |
| `"MaxContextTokens"` | `Automatic` | Maximum token capacity of the context window. Model-specific (e.g., 200,000 for Claude 3/4, 128,000 for GPT-4o, 1,047,576 for Gemini 2+/GPT-4.1). |
| `"StopTokens"` | `Automatic` | Stop sequences that signal the LLM to stop generating. Some models return `Missing["NotSupported"]`. The resolved stop tokens include the `"EndToken"` value when applicable. |
| `"TokenBudgetMultiplier"` | `Automatic` | Multiplier for the token budget calculation. Default: `1`. |

## Chat Behavior

| Setting | Default | Description |
| ------- | ------- | ----------- |
| `"IncludeHistory"` | `Automatic` | Whether to include chat history (preceding cells) in the context sent to the LLM. When `Automatic`, resolves to `Automatic` (no further resolution via `resolveAutoSetting0`). In practice, `Automatic` behaves the same as `True`: the `If[ ! settings["IncludeHistory"], cells = { evalCell } ]` check in `sendChat` (`SendChat.wl`) does not trigger because `! Automatic` evaluates to `Not[Automatic]` (not `True`), so the full cell list is passed to `constructMessages`. When explicitly `True`, the same behavior applies. When `False`, only the evaluation cell itself is included — no preceding chat history. The `selectChatCells` function (`SendChat.wl`) first selects candidate cells (up to `ChatHistoryLength` cells, bounded by chat delimiter cells), and then the `IncludeHistory` check decides whether to keep those cells or replace them with just the current cell. In `ChatHistory.wl`, `selectChatHistoryCells` dispatches on the setting value: `False` returns only the current cell; any other value (including `Automatic`) applies the `ChatHistoryLength` limit to the full cell list. Cell style overrides: cells with the `"SideChat"` style automatically set `IncludeHistory` to `False` via `addCellStyleSettings` in `Actions.wl`. Exposed in the notebook preferences UI as an "Include chat history" checkbox under the "Chat Notebook Cells" section (`PreferencesContent.wl`), where both `True` and `Automatic` display as checked. No dependencies in `$autoSettingKeyDependencies`. No model-specific overrides exist in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed to the LLM service). Not in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. |
| `"ChatHistoryLength"` | `1000` | Maximum number of chat cells to include in the context. When sending a chat message, the system selects cells starting from the current cell and looking backwards, limited to this count. Used in two paths: `selectChatCells` in `SendChat.wl` (which sets `$maxChatCells` from this value and applies `Take[..., UpTo @ $maxChatCells]` on the filtered cell list) and `selectChatHistoryCells` in `ChatHistory.wl` (which applies the same kind of limit on cell information entries). When the value is not a positive integer, falls back to the default `$maxChatCells`. Exposed in the notebook preferences UI as a numeric input field under the "Notebooks" section. Note: this is a cell count limit, not a token limit; token budgeting is handled separately by `"MaxContextTokens"` and `"TokenBudgetMultiplier"`. |
| `"MergeMessages"` | `True` | Whether to merge consecutive messages with the same role into a single message. |
| `"MaxCellStringLength"` | `Automatic` | Maximum string length for input cell content included in the context. Resolved based on model and `MaxContextTokens`. |
| `"MaxOutputCellStringLength"` | `Automatic` | Maximum string length for output cell content included in the context. Resolved based on `MaxCellStringLength`. |
| `"ForceSynchronous"` | `Automatic` | Whether to force synchronous (non-streaming) chat requests. When `Automatic`, resolves via `forceSynchronousQ` (`Settings.wl`), which returns `True` if the model's service is `"GoogleGemini"` (since Google Gemini uses a non-streaming API by default) and `False` otherwise. Model-specific overrides in `$modelAutoSettings` set this to `True` for O1, O3, and O4-Mini (OpenAI reasoning models that do not support streaming), and explicitly to `False` for O1-Mini, Gemini 2, and Gemini 3 (which do support streaming, overriding the service-level default for Gemini). When `True`, `chatSubmit0` in `SendChat.wl` uses the synchronous `LLMServices`Chat` function instead of the streaming `LLMServices`ChatSubmit`, waits for the complete response before writing output, sets progress display to `"WaitingForResponse"`, and returns `None` instead of a `TaskObject`. Additionally, when `ForceSynchronous` is `True`, `$showProgressText` is forced to `True` in `resolveAutoSettings` (`Settings.wl`) regardless of the `ShowProgressText` setting. Depends on `"Model"`. The `"BypassResponseChecking"` setting depends on this: when `ForceSynchronous` is `True`, `BypassResponseChecking` also resolves to `True`, skipping HTTP status code validation, empty response detection, and JSON error parsing. Not exposed in the preferences UI. |
| `"TimeConstraint"` | `Automatic` | Time limit (in seconds) for chat evaluation. |
| `"ConvertSystemRoleToUser"` | `Automatic` | Whether to convert system-role messages to user-role messages. Model default: `False`. Required for some models (e.g., O1-Mini). |
| `"ReplaceUnicodeCharacters"` | `Automatic` | Whether to replace Unicode characters with ASCII equivalents before sending to the LLM. Model default: `False`. Enabled for Anthropic models and some OpenAI models (e.g., GPT-5.2). |
| `"BypassResponseChecking"` | `Automatic` | Whether to bypass response validation after receiving an LLM response. When `True`, the response is immediately written as a formatted output cell without validating the HTTP status code, checking for empty responses, or extracting error data from the response body. When `False`, the response goes through full validation: the debug log is processed to extract body chunks, status codes are checked (non-200 responses trigger error cells), empty responses are detected, and JSON error data is parsed before writing output. Resolves to `True` when `ForceSynchronous` is `True`, `False` otherwise. Depends on `"ForceSynchronous"`. |
| `"Assistance"` | `Automatic` | Whether automatic assistance mode is enabled. When `Automatic`, resolves to `False`. When `True`, LLM responses are processed immediately rather than being queued for user approval, output cells use `"AssistantOutput"` styles instead of `"ChatOutput"`, and certain tools are disabled (WolframLanguageEvaluator, CreateNotebook, WolframAlpha). Controlled in the notebook preferences UI as "Enable automatic assistance". |

## Prompting

| Setting | Default | Description |
| ------- | ------- | ----------- |
| `"BasePrompt"` | `Automatic` | Specifies which base prompt components to include in the system prompt. Can be `Automatic` (inherited from persona/model settings), `None` (disables all base prompting, as used by the RawModel persona), a single component name string, or a list of component names. Lists can include `ParentList` to inherit from the parent scope while adding additional components (e.g., `{ParentList, "Notebooks", "WolframLanguageStyle"}`). Available components are defined in `Prompting.wl` via `$basePromptComponents` and `$basePromptOrder`, and include individual components (e.g., `"Markdown"`, `"CodeBlocks"`, `"MathExpressions"`, `"EscapedCharacters"`, `"WolframLanguageStyle"`, `"EndTurnToken"`) as well as class names that expand to groups of components (e.g., `"Notebooks"`, `"WolframLanguage"`, `"Math"`, `"Formatting"`, `"All"`). Dependencies between components are automatically resolved via `$basePromptDependencies`. Part of `$modelInheritedLists`, which enables special list-merging behavior with `ParentList`. Interacts with `ExcludedBasePrompts`, which removes specified components from the resolved list. Personas typically set this to include `ParentList` plus persona-specific components (e.g., CodeAssistant uses `{ParentList, "Notebooks", "WolframLanguageStyle"}`). |
| `"ExcludedBasePrompts"` | `Automatic` | List of base prompt component names to exclude from the system prompt. When `Automatic`, resolves to `{ParentList}` via the global model auto default, meaning it inherits exclusions from the parent model settings. Can be a list containing strings (component names or class names) and/or `ParentList` for inheritance. Valid values must match `{ (_String\|ParentList)... }`; invalid values trigger an `"InvalidExcludedBasePrompts"` failure. Applied after the `"BasePrompt"` list is resolved: in `augmentChatMessages` (`ChatMessages.wl`), the resolved `BasePrompt` list is filtered via `DeleteCases` to remove any components matching the exclusion list. Additionally, the excluded components are removed from the collected prompt components via `removeBasePrompt` in `Prompting.wl`, which drops matching keys from `$collectedPromptComponents` and strips the corresponding text from the system message content. The resolved value (with `ParentList` entries removed) is stored in the `$excludedBasePrompts` global variable (`Settings.wl`), which is also checked by `needsBasePrompt` (`Prompting.wl`) to prevent excluded components from being collected during prompt construction. Part of `$modelInheritedLists` (along with `"BasePrompt"`), which enables special list-merging behavior in `inheritModelSettings`: when the value contains `ParentList`, it is merged with the model-specific default via `mergeChatSettings`, allowing syntax like `{ParentList, "EscapedCharacters"}` to mean "inherit parent exclusions and also exclude EscapedCharacters." Model-specific overrides: GPT-5.2 uses `{ParentList, "EscapedCharacters"}` (because it has improved Unicode handling). The `$defaultConfigSettings` in `LLMUtilities.wl` sets this to `{"Notebooks", "NotebooksPreamble"}` for LLM configuration generation (not the regular chat notebook flow). No explicit dependencies in `$autoSettingKeyDependencies`. Not exposed in the preferences UI. |
| `"ChatContextPreprompt"` | `Automatic` | **Deprecated.** Legacy preprompt text used as the "Pre" section of the system prompt sent to the LLM. Resolved via `getPrePrompt` in `ChatMessages.wl`, which checks the following in priority order: persona-level `"ChatContextPreprompt"`, persona-level `"Pre"` / `"PromptTemplate"` / `"Prompts"`, then global `"ChatContextPreprompt"`, then global `"Pre"` / `"PromptTemplate"` / `"Prompts"`. The value must be a `String`, `TemplateObject`, or list thereof. Exposed in the chat context settings dialog (`Actions.wl`) as a text input field with a default of `"You are a helpful Wolfram Language programming assistant. Your job is to offer Wolfram Language code suggestions based on previous inputs and offer code suggestions to fix errors."`. Automatic value resolution is not implemented (noted as TODO in `Settings.wl`). Superseded by persona-based prompting via `"LLMEvaluator"` and the `"BasePrompt"` component system. |
| `"UserInstructions"` | `Automatic` | User-provided instructions to include in the system prompt. |
| `"Prompts"` | `{}` | Additional prompt messages to include in the conversation. |
| `"PromptGenerators"` | `Automatic` | [TODO] List of prompt generators to use for augmenting prompts with additional context (e.g., related documentation, Wolfram Alpha results). Behavior defined in the `PromptGenerators/` directory. Default when `Automatic`: `{}`. |
| `"PromptGeneratorsEnabled"` | `Automatic` | [TODO] Which prompt generators are enabled. Behavior defined in the `PromptGenerators/` directory. |
| `"PromptGeneratorMessagePosition"` | `2` | Position in the message list where prompt generator messages are inserted. |
| `"PromptGeneratorMessageRole"` | `"System"` | Message role used for prompt generator messages. |
| `"DiscourageExtraToolCalls"` | `Automatic` | Whether to include a base prompt component discouraging unnecessary tool calls. When enabled, adds the `"DiscourageExtraToolCalls"` base prompt component to the system prompt, which appends the text: `"Don't make more tool calls than is needed. Tool calls cost tokens, so be efficient!"`. The setting is evaluated via `discourageExtraToolCallsQ` in `ChatMessages.wl`, which returns `False` if `ToolsEnabled` is `False` or `Tools` is empty (i.e., the prompt is only included when tools are actually available). Has no dependencies on other base prompt components. Model-specific: currently only enabled (`True`) for Anthropic Claude 3.7 Sonnet via `$modelAutoSettings`. No global auto default exists, so `Automatic` effectively resolves to `False` for all other models. Not exposed in the preferences UI. |

## Tools

| Setting | Default | Description |
| ------- | ------- | ----------- |
| `"Tools"` | `Automatic` | [TODO] Tool definitions available to the LLM. Behavior defined in the `Tools/` directory. When `Automatic`, tools are resolved based on the persona and `ToolsEnabled` setting. |
| `"ToolsEnabled"` | `Automatic` | Whether tools are enabled for the current chat. Resolved per model (e.g., disabled for Gemini Pro/Pro Vision). |
| `"ToolMethod"` | `Automatic` | Method for tool calling. `"Service"` uses the LLM service's native tool calling API. Other values use prompt-based tool calling. Model-specific defaults. |
| `"HybridToolMethod"` | `Automatic` | Whether to use hybrid tool calling, combining service-level and prompt-based tool calling. When `Automatic`, resolved by `hybridToolMethodQ` (`Settings.wl`): returns `False` if `ToolsEnabled` is `False`; returns `False` if `ToolMethod` is `"Service"` (since service-level calling is already in use, hybrid mode is unnecessary); otherwise returns `True` if the model matches `$$hybridToolModel`, which requires the service to be `"OpenAI"`, `"AzureOpenAI"`, or `"LLMKit"` (or the model to be a plain string); returns `False` for all other models. When `True`, `makeLLMConfiguration` in `SendChat.wl` builds the `LLMConfiguration` with `"ToolMethod" -> "Service"` and includes `LLMTool` definitions in the `"Tools"` parameter, enabling the LLM service's native tool calling API alongside Chatbook's prompt-based tool calling. This means the model receives both prompt-based tool instructions (from the resolved `ToolMethod`, e.g., `"Simple"`) and service-level tool definitions, allowing it to use either mechanism. When `False`, the `LLMConfiguration` omits tool definitions and relies solely on the prompt-based tool method. Depends on `"Model"`, `"ToolsEnabled"`, and `"ToolMethod"` (declared in `$autoSettingKeyDependencies`). Model-specific overrides in `$modelAutoSettings`: `True` for GPT-4o, GPT-4.1, and O3-Mini (which use `ToolMethod -> Verbatim @ Automatic` to keep prompt-based tools alongside service tools); `False` for DeepSeek Reasoner, GPT-5, O1, O3, and O4-Mini (which either use pure `"Service"` tool method or have limited tool support). Not in `$llmConfigPassedKeys` (not passed directly through `LLMConfiguration`), but indirectly controls whether `LLMConfiguration` includes `"ToolMethod" -> "Service"` and `"Tools"`. Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not exposed in the preferences UI. |
| `"ToolOptions"` | `$DefaultToolOptions` | Per-tool option overrides. Default provides options for `"WolframAlpha"`, `"WolframLanguageEvaluator"`, `"WebFetcher"`, `"WebSearcher"`, and `"WebImageSearcher"`. See `Tools/Common.wl` for the full default. |
| `"ToolSelectionType"` | `<\|\|>` | Tool selection configuration. An empty association by default. |
| `"ToolCallFrequency"` | `Automatic` | How often the LLM is allowed to make tool calls. Default: `Automatic`. |
| `"ToolCallRetryMessage"` | `Automatic` | Whether to send retry messages when a tool call fails. Resolves to `True` for LLMKit-authenticated sessions, `False` for some models (e.g., GPT-4.1, GPT-5). |
| `"ToolExamplePrompt"` | `Automatic` | Tool example prompt specification included in the system prompt to guide tool usage. Resolved per model (e.g., `None` for Claude 3). |
| `"ToolCallExamplePromptStyle"` | `Automatic` | Style of tool call example prompts (`"Basic"` or `Automatic`). Model-specific. |
| `"ToolResponseRole"` | `Automatic` | Message role used for tool response messages. Model default: `"System"`. Some models use `"User"` (e.g., Claude 2, MistralAI, DeepSeek Reasoner, local models like Qwen/Nemotron/Mistral). |
| `"ToolResponseStyle"` | `Automatic` | Style used for formatting tool responses. MistralAI uses `"SystemTags"`. |
| `"SplitToolResponseMessages"` | `Automatic` | Whether to split tool responses into separate messages. Model default: `False`. Enabled for Anthropic models as a workaround. |
| `"MaxToolResponses"` | `5` | Maximum number of tool responses per chat turn. Some models use lower values (e.g., `3` for O1, O3, O4-Mini). |
| `"SendToolResponse"` | `Automatic` | Whether to send tool responses back to the LLM for further processing. |
| `"EndToken"` | `Automatic` | End-of-turn token that signals the LLM has finished its response. Model default: `"/end"`. Some models use `None` (e.g., GPT-4.1). The resolved value is stored in the `$endToken` global variable (`CommonSymbols.wl`) during `resolveAutoSettings` (`Settings.wl`). Used in three ways: (1) **Base prompt instruction**: The `"EndTurnToken"` base prompt component (`Prompting.wl`) adds `"* Always end your turn by writing /end."` to the system prompt when `$endToken` is a non-empty string. The related `"EndTurnToolCall"` component (which depends on `"EndTurnToken"`) adds `"* If you are going to make a tool call, you must do so BEFORE ending your turn."`. (2) **Stop tokens**: `$endToken` is included in the stop sequences sent to the LLM API via `methodStopTokens` (`Settings.wl`), which builds tool-method-specific stop token lists (e.g., `{"ENDTOOLCALL", $endToken}` for `"Textual"`/`"JSON"` methods, `{$endToken}` for `"Service"` method, `{"\n/exec", $endToken}` for `"Simple"` method). These are composed into the `"StopTokens"` setting by `autoStopTokens` and passed to `LLMConfiguration` and the HTTP request body. (3) **Tool call example templates**: `Tools/Examples.wl` uses `$endTokenString` (which prepends `"\n"` to `$endToken` when non-empty) in assistant message templates across all example styles (Basic, Instruct, Zephyr, Phi, Boxed, ChatML, XML, DeepSeekCoder, Llama, Gemma, Nemotron) to show the LLM how to end its turn in example conversations. When `None` or empty, the end token is omitted from prompts, stop sequences, and example templates. After receiving a response, `trimStopTokens` in `SendChat.wl` removes stop tokens (including the end token) from the end of the LLM's output. No model-specific overrides exist beyond GPT-4.1 (`None`). Not exposed in the preferences UI. |

## Formatting & Output

| Setting | Default | Description |
| ------- | ------- | ----------- |
| `"AutoFormat"` | `True` | Whether to auto-format LLM output by parsing Markdown syntax and converting it to structured notebook cells. When enabled, the LLM response is processed to convert Markdown elements (code blocks with language detection, headings, bold/italic text, inline code, LaTeX math, images, bullet lists, block quotes, and tables) into properly formatted Wolfram notebook cells. Also includes the `"Formatting"` base prompt component in the system prompt, which instructs the LLM that its output will be parsed as Markdown. When disabled, output is displayed as plain text. Works in conjunction with `"DynamicAutoFormat"` to control whether formatting is applied during streaming. |
| `"DynamicAutoFormat"` | `Automatic` | Whether to apply formatting during streaming, providing live-formatted output as the LLM response streams in. When `True`, the streaming content is processed by the formatting function in real-time, converting Markdown to formatted notebook expressions as they arrive. When `False`, streaming content is displayed as raw text (via `RawBoxes @ Cell @ TextData`) without live formatting. When `Automatic`, resolves to `TrueQ` of the `"AutoFormat"` setting, so dynamic formatting is enabled whenever auto-formatting is enabled. Resolution is handled by `dynamicAutoFormatQ` in `SendChat.wl`, which first checks for an explicit `True`/`False` value, then falls back to `"AutoFormat"`. The resolved value is captured as the `reformat` variable in `activeAIAssistantCell` and passed to `dynamicTextDisplay`, which dispatches between formatted and raw text display. No model-specific overrides exist. Not exposed in the preferences UI; controlled indirectly via the `"AutoFormat"` checkbox. |
| `"StreamingOutputMethod"` | `Automatic` | Method for streaming output display. Default: `"PartialDynamic"`. |
| `"NotebookWriteMethod"` | `Automatic` | Method for writing content to the notebook. Default: `"PreemptiveLink"`. |
| `"TabbedOutput"` | `True` | Whether to use tabbed output for organizing long or multi-part responses. |
| `"ShowMinimized"` | `Automatic` | Whether to show output in a minimized/collapsed state. |
| `"ShowProgressText"` | `Automatic` | Whether to show progress text while the LLM is generating a response. Model default: `True`. |
| `"OpenToolCallBoxes"` | `Automatic` | Whether tool call display boxes are open by default. Resolves to `True` when `SendToolResponse` is `False`, otherwise `Automatic`. |
| `"TrackScrollingWhenPlaced"` | `Automatic` | Whether to auto-scroll the notebook to follow new output as it is placed. |
| `"AppendCitations"` | `Automatic` | Whether to automatically append formatted source citations to the LLM response. When enabled, citations are generated from sources gathered by prompt generators (e.g., documentation, web searches, WolframAlpha results) and appended as a markdown section. When disabled, the WolframAlpha prompt generator instead includes a hint asking the LLM to cite sources inline. Model default: `False`. The WolframAlpha persona overrides this to `True`. |

## Personas & UI

| Setting | Default | Description |
| ------- | ------- | ----------- |
| `"LLMEvaluator"` | `"CodeAssistant"` | The persona (LLM evaluator) to use. Determines the system prompt, available tools, and other settings. Can be a persona name string (e.g., `"CodeAssistant"`, `"PlainChat"`, `"RawModel"`) or a full persona `Association` with keys like `"Prompts"`, `"Tools"`, `"Icon"`, `"BasePrompt"`, etc. Built-in personas are defined in `LLMConfiguration/Personas/`. During `resolveAutoSettings` (`Settings.wl`), the string value is resolved via `getLLMEvaluator`, which calls `getNamedLLMEvaluator` to look up persona data from `GetCachedPersonaData`. If the name is not found as a built-in persona, `tryPromptRepositoryPersona` attempts to load it from the Wolfram Prompt Repository via `ResourceObject["Prompt" -> name]`. The resolved persona `Association` is merged with the current settings via `mergeChatSettings`, with `$nonInheritedPersonaValues` keys (including `"LLMEvaluator"` itself) dropped from the persona data before merging, preventing circular inheritance. After resolution, the `"LLMEvaluator"` key in the settings is replaced with the full persona `Association` (or the original string/`None` if unresolvable). When writing settings back to notebook `TaggingRules`, `toSmallSettings` in `SendChat.wl` converts the resolved `Association` back to just the persona name string (via the `"LLMEvaluatorName"` key) to save space. The persona determines: the system prompt (via `"Prompts"`, `"Pre"`, `"PromptTemplate"`, and `"BasePrompt"` keys), available tools (the `"Tools"` setting depends on `"LLMEvaluator"` and `"ToolsEnabled"` in `$autoSettingKeyDependencies`; `selectTools` in `Tools/Common.wl` uses the persona name to look up per-persona tool selections), and the output cell dingbat icon (`makeOutputDingbat` and `makeActiveOutputDingbat` in `SendChat.wl` extract the persona's `"PersonaIcon"` or `"Icon"` key). When `"RawOutput"` is enabled during chat evaluation, the persona is overridden with `GetCachedPersonaData["RawModel"]` in `sendChat` (`SendChat.wl`). `CreateChatDrivenNotebook` defaults this to `"PlainChat"`. Exposed in the notebook preferences UI as a persona PopupMenu selector under the "Notebooks" section (`PreferencesContent.wl`). Also central to the chat action menu in `UI.wl`, where each persona appears as a selectable menu item that writes to `{TaggingRules, "ChatNotebookSettings", "LLMEvaluator"}`, with a "Reset" option that sets the value to `Inherited`. Listed in `$nonInheritedPersonaValues`, so it retains its value from the notebook/cell scope rather than inheriting from persona configurations. No dependencies in `$autoSettingKeyDependencies` (but `"Tools"` depends on it). No model-specific overrides exist in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed to the LLM service). |
| `"VisiblePersonas"` | `$corePersonaNames` | List of persona names visible in the persona selector UI. |
| `"ChatDrivenNotebook"` | `False` | **Deprecated.** Whether the entire notebook operates in "chat-driven" mode rather than the default "chat-enabled" mode. When `True`, new cells default to `"ChatInput"` style, the persona selector prioritizes PlainChat/RawModel/CodeWriter/CodeAssistant at the top of the list (`UI.wl` `filterPersonas`), and the cloud toolbar displays "Chat-Driven Notebook" instead of "Chat Notebook" (`CloudToolbar.wl`). Used by `CreateChatDrivenNotebook[]`, which wraps `CreateChatNotebook` with `"ChatDrivenNotebook" -> True`, `"LLMEvaluator" -> "PlainChat"`, and `DefaultNewCellStyle -> "ChatInput"`. This is a non-inherited persona value (listed in `$nonInheritedPersonaValues`). |
| `"InitialChatCell"` | `True` | Whether to create an initial empty chat input cell when opening a new chat notebook. When `True`, `CreateChatNotebook` inserts an empty `"ChatInput"` cell (via `initialChatCells` in `CreateChatNotebook.wl`); for cloud notebooks (`$cloudNotebooks`), an additional selection-mover cell is appended to position the cursor. When `False`, the notebook is created with no initial cells. The value is evaluated via `TrueQ`, so only an explicit `True` creates the cell; `Automatic` or other non-boolean values behave as `False`. This is an **unsaved setting** (listed in `$unsavedSettings` in `CreateChatNotebook.wl`): it is used only at notebook creation time and is explicitly dropped from the notebook's `TaggingRules` by `makeChatNotebookSettings`, so it does not persist in the saved notebook. Defined as an option of `CreateChatNotebook` (inherited from `$defaultChatSettings`). When creating a notebook from a `ChatObject`, `initialChatCells` is locally overridden to return the converted message cells instead. No dependencies on other settings. No model-specific overrides exist in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed to the LLM service). Listed in `$nonInheritedPersonaValues`, so it retains its value from notebook/cell scope rather than inheriting from persona configurations. Not exposed in the preferences UI. |
| `"ChatInputIndicator"` | `Automatic` | Text prefix prepended to `"ChatInput"` cells when serializing notebook content for the LLM. When `Automatic`, resolves to `"\|01f4ac"` (speech balloon emoji). Can be any string (e.g., `"[USER]"`), or `None`/`""` to disable the indicator entirely. Only applied when the content is mixed (i.e., when `mixedContentQ` returns `True` in `ChatMessages.wl`, indicating the conversation includes both chat input cells and other cell types). When the indicator is used, the `"ChatInputIndicator"` base prompt component (`Prompting.wl`) is automatically included in the system prompt to explain the indicator's meaning to the LLM: it tells the model that cells prefixed with this symbol are actual user messages, while other cells are context. The indicator text is distinct from cell dingbats controlled by `"SetCellDingbat"`, which are visual notebook icons. The indicator symbol is set per chat evaluation via `chatIndicatorSymbol` in `SendChat.wl` and stored in the global `$chatIndicatorSymbol` variable (`Common.wl`). No model-specific overrides exist. |
| `"SetCellDingbat"` | `True` | Whether to set cell dingbats (icons) on chat cells. |
| `"EnableChatGroupSettings"` | `False` | Whether chat group-level settings are enabled. When `True`, during chat evaluation (`SendChat.wl`), `getChatGroupSettings` is called on the evaluation cell to retrieve prompt text from parent group header cells. The feature walks backward through notebook cells to find parent group headers using cell grouping rules (`"TitleGrouping"` and `"SectionGrouping"`) and collects `"Prompt"` values stored in their `TaggingRules` at the path `"ChatNotebookSettings"` → `"ChatGroupSettings"` → `"Prompt"`. Multiple prompts from different grouping levels are joined with `"\n\n"`. The collected group prompt is stored in the settings as `"ChatGroupSettings"` and incorporated into the system prompt via `buildSystemPrompt` in `ChatMessages.wl`, where it appears as the `"Group"` section in the prompt template (between `"Pre"` and `"Base"` sections). When `False`, no group settings are resolved and the `"Group"` section of the system prompt is omitted. Implementation is in `ChatGroups.wl`. Not exposed in the preferences UI. No model-specific overrides exist. |
| `"AllowSelectionContext"` | `Automatic` | Whether to allow the current selection to be used as context. Resolves to `True` when using workspace chat, inline chat, or sidebar chat. |
| `"CurrentPreferencesTab"` | `"Services"` | Persists the user's last-selected tab in the Chatbook preferences dialog. When the preferences dialog opens, the tab is initialized from this setting (defaulting to `"Services"` if unset); when the dialog closes, the current tab selection is saved back. The `openPreferencesPage` function in `PreferencesContent.wl` also writes to this setting at `$FrontEnd` scope to navigate directly to a specific preferences page. This is a non-inherited persona value (listed in `$nonInheritedPersonaValues`) and is excluded from debug/diagnostic data (listed in `$droppedSettingsKeys` in `Common.wl`). Not included in `$defaultChatSettings`. No model-specific overrides exist. |

## Storage & Conversations

| Setting | Default | Description |
| ------- | ------- | ----------- |
| `"ConversationUUID"` | `None` | UUID identifying the current conversation. `None` means no conversation tracking. When a conversation is saved (via `SaveChat`), `ensureConversationUUID` in `Storage.wl` checks whether the current setting is a valid string; if not, it generates a new UUID via `CreateUUID[]` and writes it back to the notebook or cell's `CurrentChatSettings`. Used as the primary key for persistent conversation storage (`Storage.wl`), chat search indexing (`Search.wl`), and chat history listings (`ChatModes/UI.wl`). The `AutoSaveConversations` setting depends on `ConversationUUID` being a valid string (along with `AppName`). Chat modes may override this: workspace chat and sidebar chat automatically generate a new UUID when starting a new conversation (`ChatModes/UI.wl`) or when initialized via NotebookAssistance settings (`ChatModes/ShowNotebookAssistance.wl`); inline chat does not set a UUID (falls back to `None`). When loading a saved conversation, the stored UUID is restored to `CurrentChatSettings` for the target notebook or cell. |
| `"AutoSaveConversations"` | `Automatic` | Whether to automatically save conversations to persistent storage after chat evaluations. When `Automatic`, resolves to `True` if both `AppName` is a valid string and `ConversationUUID` is a valid string; otherwise resolves to `False`. Depends on `"AppName"` and `"ConversationUUID"`. When `True`, conversations are saved after each chat evaluation, subject to the `"MinimumResponsesToSave"` threshold. Chat modes may override this: workspace chat and sidebar chat set it to `True` (with a new `ConversationUUID`), while inline chat sets it to `False`. |
| `"AppName"` | `Automatic` | Application name used to namespace saved conversations, search indexes, and chat history listings. When `Automatic`, resolves to `$defaultAppName` (`"Default"`). When set to a non-default string value, also establishes a service caller context via `setServiceCaller`. Chat modes may override this (e.g., NotebookAssistance uses `"NotebookAssistance"`). The `AutoSaveConversations` setting depends on `AppName` being a valid string. |
| `"MinimumResponsesToSave"` | `1` | Minimum number of assistant responses required before a conversation is saved. |
| `"TargetCloudObject"` | `Automatic` | Target cloud object for cloud-based conversation storage. |

## Advanced / Internal

| Setting | Default | Description |
| ------- | ------- | ----------- |
| `"Tokenizer"` | `Automatic` | Tokenizer used for token counting. Resolved based on the model's tokenizer name. |
| `"HandlerFunctions"` | `$DefaultChatHandlerFunctions` | Callback functions invoked at various stages of chat processing. The value is an `Association` mapping event name strings to handler functions (or `None` to skip). The default value `$DefaultChatHandlerFunctions` (`Settings.wl`) defines 9 event keys, all defaulting to `None`: `"AppendCitationsStart"`, `"AppendCitationsEnd"`, `"ChatAbort"`, `"ChatPost"`, `"ChatPre"`, `"PromptGeneratorEnd"`, `"PromptGeneratorStart"`, `"ToolRequestReceived"`, and `"ToolResponseGenerated"`. The `"ChatAbort"`, `"ChatPost"`, and `"ChatPre"` entries use `RuleDelayed` (`:>`) pointing to global variables `$ChatAbort`, `$ChatPost`, and `$ChatPre` (all initially `None`), allowing runtime reassignment without modifying the association. A 10th event, `"ToolResponseReceived"`, is also dispatched via `applyHandlerFunction` (`SendChat.wl`) but is not included in the default association (falls back to `None` via `getHandlerFunction`). Custom handler values are merged with defaults during resolution: `resolveHandlers` in `Handlers.wl` creates a new association with `$DefaultChatHandlerFunctions` as the base, overlaid with user-provided handlers (after `replaceCellContext` processing), plus a `"Resolved" -> True` marker to prevent re-resolution. Resolution occurs during `resolveAutoSettings` (`Settings.wl`), where `getHandlerFunctions` is called on the settings and the result replaces the `"HandlerFunctions"` key. Each handler function is invoked via `applyHandlerFunction` (`Handlers.wl`), which constructs an argument association containing: `"EventName"` (the event type string), `"ChatNotebookSettings"` (current settings with `"Data"` and `"OpenAIKey"` keys dropped), and event-specific data. This argument association is accumulated in the `$ChatHandlerData` global variable (publicly exported) via `addHandlerArguments`, which merges new data with existing handler state (supporting nested association merging). The handler receives `$ChatHandlerData` with `"DefaultProcessingFunction"` dropped. Event dispatch locations: `"ChatPre"` is called in `sendChat` (`SendChat.wl`) before chat submission, with `"EvaluationCell"` and `"Messages"` in the arguments; `"ChatPost"` and `"ChatAbort"` are called in `applyChatPost` (`Actions.wl`) after chat completion or abort, with `"ChatObject"` and `"NotebookObject"` in the arguments; `"ToolRequestReceived"` is called after parsing a tool call (`SendChat.wl`), with `"ToolRequest"` in the arguments; `"ToolResponseGenerated"` is called after generating a tool response (`SendChat.wl`), with `"ToolResponse"` and `"ToolResponseString"` in the arguments; `"ToolResponseReceived"` is called after the tool response is formatted and ready to send back (`SendChat.wl`), with `"ToolResponse"` in the arguments; `"PromptGeneratorStart"` and `"PromptGeneratorEnd"` are called in `DefaultPromptGenerators.wl` around each prompt generator execution, with `"PromptGenerator"` in the arguments (and `"PromptGeneratorResult"` added for the end event); `"AppendCitationsStart"` and `"AppendCitationsEnd"` are called in `Citations.wl` around citation generation, with `"Sources"` and `"CitationString"` respectively. In the streaming chat submission path (`chatHandlers` in `SendChat.wl`), the resolved handlers are passed to `LLMServices`ChatSubmit` via the `HandlerFunctions` parameter, but `"ChatPost"`, `"ChatPre"`, and `"Resolved"` keys are dropped (listed in `$chatSubmitDroppedHandlers`). The `chatHandlers` function also wraps custom `"BodyChunkReceived"` and `"TaskFinished"` handlers (if provided) inside Chatbook's own streaming logic, calling the user's handler before Chatbook's processing for each body chunk and after task completion. Not passed through `LLMConfiguration` (not in `$llmConfigPassedKeys`). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. No dependencies in `$autoSettingKeyDependencies`. No model-specific overrides exist in `$modelAutoSettings`. Not exposed in the preferences UI. |
| `"HandlerFunctionsKeys"` | `Automatic` | Keys to include in the handler functions callback data passed to `LLMServices`ChatSubmit` and `URLSubmit`. Controls which fields from the streaming response are available to handler functions. When `Automatic` or unspecified, resolves to `$defaultHandlerKeys` (`SendChat.wl`): `{"Body", "BodyChunk", "BodyChunkProcessed", "StatusCode", "TaskStatus", "EventName"}`. When a list of strings, the user-provided keys are merged with `$defaultHandlerKeys` via `Union` (so the default keys are always included). When a single string, it is treated as a one-element list. Invalid values trigger an `"InvalidHandlerKeys"` warning (`Common.wl`) and fall back to `$defaultHandlerKeys`. Resolution is performed by `chatHandlerFunctionsKeys` (`SendChat.wl`), which is called from `resolveAutoSetting0` (`Settings.wl`). The resolved value is passed directly as the `HandlerFunctionsKeys` parameter to `LLMServices`ChatSubmit` (for LLMServices-based chat) and `URLSubmit` (for legacy HTTP-based chat) in `chatSubmit0` (`SendChat.wl`). Also used in other `URLSubmit` calls outside of chat: `VectorDatabases.wl` uses `{"ByteCountDownloaded", "StatusCode"}` for vector database downloads, and `RelatedWolframAlphaResults.wl` uses `{"StatusCode", "BodyByteArray"}` for Wolfram Alpha result fetching. Not passed through `LLMConfiguration` (not in `$llmConfigPassedKeys`). Depends on `"EnableLLMServices"` for resolution ordering in `$autoSettingKeyDependencies`. Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. No model-specific overrides exist in `$modelAutoSettings`. Not exposed in the preferences UI. |
| `"InheritanceTest"` | N/A | Internal diagnostic flag used by the settings inheritance verification system. Not a user-configurable setting and not included in `$defaultChatSettings`. During `verifyInheritance0` (`Settings.wl`), this flag is set to `True` at the `$FrontEnd` scope via `setCurrentValue[fe, {TaggingRules, "ChatNotebookSettings", "InheritanceTest"}, True]` to mark that the inheritance chain for tagging rules has been properly initialized. Subsequent calls to `verifyInheritance` check this flag via `inheritingQ`, which reads `AbsoluteCurrentValue[obj, {TaggingRules, "ChatNotebookSettings", "InheritanceTest"}]` — if `True` (or if the read fails), the object is considered to have valid inheritance and initialization is skipped. During `repairTaggingRules`, the flag is explicitly removed from child objects (notebooks, cells) so that it only persists at the top-level `FrontEndObject`, preventing it from appearing as an explicit override in child scopes. The `verifyInheritance` function is called by `currentChatSettings0` before reading or writing settings, ensuring the inheritance chain is intact. Listed in `$nonInheritedPersonaValues`, so it retains its value from notebook/cell scope rather than inheriting from persona configurations. No model-specific overrides exist. Not exposed in the preferences UI. |
| `"ProcessingFunctions"` | `$DefaultChatProcessingFunctions` | [TODO] Functions that control the chat processing pipeline (e.g., `"CellToChatMessage"`, `"ChatMessages"`, `"ChatSubmit"`, `"FormatChatOutput"`, `"FormatToolCall"`, `"WriteChatOutputCell"`). Spans multiple files. |
| `"ConversionRules"` | `None` | Custom transformation rules applied to notebook cells before they are serialized to chat message strings. When `None` (default), cells are passed through unmodified. When set to a list of replacement rules, the rules are compiled into a `Dispatch` table (cached for reuse) and applied to each cell via `ReplaceRepeated` inside `CellToString` (`Serialization.wl`) before string conversion occurs. This enables custom box-level or expression-level transformations of cell content prior to sending it to the LLM. The setting value is read from `CurrentChatSettings` in `makeChatMessages` (`ChatMessages.wl`) and stored in the dynamic variable `$conversionRules`, which `CellToString` picks up as its default `"ConversionRules"` option. Invalid values (neither a rule list nor a valid `Dispatch` table) trigger an `"InvalidConversionRules"` warning and fall back to `None`. |
| `"ExperimentalFeatures"` | `Automatic` | List of enabled experimental feature names, resolved dynamically from other settings. When `Automatic`, the `autoExperimentalFeatures` function (`Settings.wl`) builds a list based on two conditions: `"RelatedWolframAlphaResults"` is included if `"WolframAlphaCAGEnabled"` is `True` or `"RelatedWolframAlphaResults"` is in the `"PromptGenerators"` list; `"RelatedWebSearchResults"` is included if `"WebSearchRAGMethod"` is `"Tavily"` or `"WebSearch"` is in the `"PromptGenerators"` list. The resolved list is stored in the `$experimentalFeatures` global variable (`Settings.wl`) and preserved across handler evaluation via `ChatState.wl`. Individual features are checked at runtime via `featureEnabledQ` (`Settings.wl`), which tests membership in the resolved list. The primary consumer is `resolvePromptGenerators` in `PromptGenerators/Common.wl`, which appends `"RelatedWolframAlphaResults"` and/or `"WebSearch"` to the active prompt generators list based on feature flags. Depends on `"WolframAlphaCAGEnabled"`, `"WebSearchRAGMethod"`, and `"PromptGenerators"` (declared in `$autoSettingKeyDependencies`). No model-specific overrides exist. Not exposed in the preferences UI. |
| `"OpenAIKey"` | `Automatic` | OpenAI API key. Legacy setting for direct OpenAI API authentication. |
| `"OpenAIAPICompletionURL"` | `"https://api.openai.com/v1/chat/completions"` | OpenAI API completion endpoint URL. Legacy setting for direct OpenAI API access. |

---

## Model-Specific Auto Settings

When a setting has a value of `Automatic`, the resolution pipeline checks `$modelAutoSettings` for a model-specific default. Settings are looked up in order of specificity:

1. Service + model name (e.g., `$modelAutoSettings["Anthropic", "Claude4"]`)
2. Service + model ID
3. Service + model family
4. Any service + model name (e.g., `$modelAutoSettings[Automatic, "GPT4Omni"]`)
5. Any service + model ID
6. Any service + model family
7. Service-level default (e.g., `$modelAutoSettings["Anthropic", Automatic]`)
8. Global default (`$modelAutoSettings[Automatic, Automatic]`)

The first match wins. For details on how to add support for new models, see [TODO: How to Add Support for New Models].

### Global Auto Setting Defaults

These are the fallback values from `$modelAutoSettings[Automatic, Automatic]` when no model-specific override exists:

| Setting | Default |
| ------- | ------- |
| `"AppendCitations"` | `False` |
| `"ConvertSystemRoleToUser"` | `False` |
| `"EndToken"` | `"/end"` |
| `"ExcludedBasePrompts"` | `{ParentList}` |
| `"PresencePenalty"` | `0.1` |
| `"ReplaceUnicodeCharacters"` | `False` |
| `"ShowProgressText"` | `True` |
| `"SplitToolResponseMessages"` | `False` |
| `"Temperature"` | `0.7` |
| `"ToolResponseRole"` | `"System"` |

### Non-Inherited Persona Values

The following settings are not inherited from the persona configuration when resolving settings. They retain their value from the notebook/cell scope:

- `"ChatDrivenNotebook"`
- `"CurrentPreferencesTab"`
- `"EnableLLMServices"`
- `"Icon"`
- `"InheritanceTest"`
- `"InitialChatCell"`
- `"LLMEvaluator"`
- `"PersonaFavorites"`
- `"ServiceDefaultModel"`
