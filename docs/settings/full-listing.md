# Settings Listing

## Overview

Chatbook settings control LLM behavior, prompt construction, tool usage, formatting, and UI behavior. Settings are stored in notebook tagging rules under `"ChatNotebookSettings"` and follow a hierarchical inheritance model.

### Accessing Settings

Use `CurrentChatSettings` to read and write settings:

```wl
(* Read global settings *)
CurrentChatSettings[]
CurrentChatSettings["Temperature"]

(* Read settings scoped to a notebook or cell *)
CurrentChatSettings[notebookObj]
CurrentChatSettings[cellObj, "Model"]

(* Write settings *)
CurrentChatSettings[$FrontEnd, "Temperature"] = 0.5
CurrentChatSettings[notebookObj, "AutoFormat"] = False

(* Reset to inherited value *)
CurrentChatSettings[notebookObj, "Temperature"] =.
```

### Inheritance Model

Settings resolve through a hierarchy, with more specific scopes overriding broader ones:

| Scope              | Description                      |
| ------------------ | -------------------------------- |
| `CellObject`       | Per-cell override                |
| `NotebookObject`   | Per-notebook settings            |
| `$FrontEndSession` | Session-wide (non-persistent)    |
| `$FrontEnd`        | Global persistent settings       |

If a setting is not defined at a given scope, it inherits from the next broader scope. A value of `Inherited` explicitly defers to the parent scope.

### Automatic Values

Many settings default to `Automatic`, meaning they are resolved at runtime based on the current model, service, and other settings. The resolution pipeline is defined in `Settings.wl` via `resolveAutoSettings`, which evaluates `Automatic` values in topologically sorted dependency order. Model-specific defaults are looked up from `$modelAutoSettings`.

---

## Model & Service

| Setting | Default | Description |
| ------- | ------- | ----------- |
| `"Model"` | `$DefaultModel` | The LLM model specification. |
| `"Authentication"` | `Automatic` | Authentication method for the LLM service. |
| `"EnableLLMServices"` | `Automatic` | Whether Chatbook uses the `LLMServices` framework for LLM communication. |
| `"Multimodal"` | `Automatic` | Whether multimodal (image) input is supported, controlling whether graphics and images in notebook cells are encoded and included in messages sent to the LLM. |
| `"Reasoning"` | `Automatic` | Whether model reasoning/chain-of-thought is enabled. |

See additional details in [Model and Service Settings](setting-groups/model-and-service.md).

## LLM Parameters

| Setting | Default | Description |
| ------- | ------- | ----------- |
| `"Temperature"` | `Automatic` | Sampling temperature for the LLM, controlling randomness in token selection. |
| `"FrequencyPenalty"` | `0.1` | Penalty applied to tokens based on their frequency in the text so far. |
| `"PresencePenalty"` | `Automatic` | Penalty applied to tokens based on whether they have appeared in the text so far, encouraging the model to introduce new topics. |
| `"TopP"` | `1` | Top-p (nucleus) sampling parameter, controlling the cumulative probability threshold for token selection. |
| `"MaxTokens"` | `Automatic` | Maximum number of tokens the LLM may generate in its response (output token limit). |
| `"MaxContextTokens"` | `Automatic` | Maximum token capacity of the context window, used internally for token budgeting and context management. |
| `"Reasoning"` | `Automatic` | Controls the reasoning/thinking effort level for models that support extended thinking. |
| `"StopTokens"` | `Automatic` | Stop sequences that signal the LLM to stop generating. |
| `"TokenBudgetMultiplier"` | `Automatic` | Multiplier applied to `MaxContextTokens` to produce the working token budget for chat message construction. |

See additional details in [LLM Parameter Settings](setting-groups/llm-parameters.md).

## Chat Behavior

| Setting | Default | Description |
| ------- | ------- | ----------- |
| `"IncludeHistory"` | `Automatic` | Whether to include chat history (preceding cells) in the context sent to the LLM. |
| `"ChatHistoryLength"` | `1000` | Maximum number of chat cells to include in the context. |
| `"MergeMessages"` | `True` | Whether to merge consecutive messages with the same role into a single message. |
| `"MaxCellStringLength"` | `Automatic` | Maximum string length for cell content included in the LLM context. |
| `"MaxOutputCellStringLength"` | `Automatic` | Maximum string length for output cell content included in the LLM context. |
| `"ForceSynchronous"` | `Automatic` | Whether to force synchronous (non-streaming) chat requests. |
| `"TimeConstraint"` | `Automatic` | Time limit (in seconds) for the overall chat task evaluation. |
| `"ConvertSystemRoleToUser"` | `Automatic` | Whether to convert system-role messages to user-role messages. |
| `"ReplaceUnicodeCharacters"` | `Automatic` | Whether to replace Wolfram Language special characters (Unicode private use area codepoints such as `\[FreeformPrompt]`) with ASCII equivalents before sending messages to the LLM. |
| `"BypassResponseChecking"` | `Automatic` | Whether to bypass response validation after receiving an LLM response. |
| `"Assistance"` | `Automatic` | Whether automatic assistance mode is enabled. |

See additional details in [Chat Behavior Settings](setting-groups/chat-behavior.md).

## Prompting

| Setting | Default | Description |
| ------- | ------- | ----------- |
| `"BasePrompt"` | `Automatic` | Specifies which base prompt components to include in the system prompt. |
| `"ExcludedBasePrompts"` | `Automatic` | List of base prompt component names to exclude from the system prompt. |
| `"ChatContextPreprompt"` | `Automatic` | **Deprecated.** Legacy preprompt text used as the "Pre" section of the system prompt. |
| `"UserInstructions"` | `Automatic` | User-provided instructions to include in the system prompt. |
| `"Prompts"` | `{}` | A list of custom prompt strings to append to the system prompt. |
| `"PromptGenerators"` | `Automatic` | List of prompt generators used to augment the conversation with additional context before sending messages to the LLM. |
| `"PromptGeneratorsEnabled"` | `Automatic` | **Not yet implemented.** Intended to control which prompt generators are enabled. |
| `"PromptGeneratorMessagePosition"` | `2` | Position in the message list where prompt generator messages are inserted. |
| `"PromptGeneratorMessageRole"` | `"System"` | Message role assigned to prompt generator messages when they are inserted into the conversation. |
| `"DiscourageExtraToolCalls"` | `Automatic` | Whether to include a base prompt component discouraging unnecessary tool calls. |

See additional details in [Prompting Settings](setting-groups/prompting.md).

## Tools

| Setting | Default | Description |
| ------- | ------- | ----------- |
| `"Tools"` | `Automatic` | Tool definitions available to the LLM, resolved as a list of `LLMTool` objects during chat processing. When `Automatic` (the default), tools are resolved dynamically based on the persona (`LLMEvaluator`) and `ToolsEnabled` setting via `resolveTools` (`Tools/Common.wl`). Resolution only occurs when `ToolsEnabled` is `True`; otherwise the setting remains unchanged. The resolution process works as follows: (1) `initTools` initializes the tool system, (2) `selectTools` determines which tools are active based on three inputs — tool names from `getToolNames`, per-persona tool selections from `getToolSelections` (stored in `"ToolSelections"`), and per-tool selection types from `getToolSelectionTypes` (stored in `"ToolSelectionType"`) — then (3) the resolved `$selectedTools` association (filtered by `toolEnabledQ`, which checks each tool's `"Enabled"` key) is stored as `Values @ $selectedTools` (a list of `LLMTool` objects) back into the `"Tools"` key of the settings. The `getToolNames` function (`Tools/Common.wl`) determines the initial tool name list through a two-level dispatch: if the persona declares tools (via `LLMEvaluator["Tools"]`), persona tools and setting-level tools are combined; if the persona sets `None`, no tools are used; if the persona uses `Automatic`, `Inherited`, or `ParentList`, default tools are used; and `ParentList` within a persona's tool list causes the parent scope's tools to be spliced in at that position. Accepted values for the `"Tools"` setting itself: `Automatic` (resolve from persona and defaults), `None` (no tools), `All` (all available tools), `Inherited` (inherit from parent scope), a list of tool name strings (e.g., `{"WolframLanguageEvaluator", "WebSearcher"}`), a list of `LLMTool` objects, or a mixed list containing strings, `LLMTool` objects, rules (`name -> tool`), and `ParentList`. After resolution, the value is always a flat list of `LLMTool` objects (`{ ___LLMTool }`). The resolved tools are consumed in two places: (1) `makeLLMConfiguration` (`SendChat.wl`) — when `ToolMethod` is `"Service"` or `HybridToolMethod` is `True`, tools are passed to `LLMConfiguration` as `"Tools" -> Cases[Flatten @ {as["Tools"]}, _LLMTool]`, enabling the LLM service's native tool calling API; when neither condition holds, tools are omitted from `LLMConfiguration` and tool calling is handled entirely through prompt-based instructions. (2) `constructLLMConfiguration` (`LLMUtilities.wl`) — tools are read from `settings["Tools"]`, each processed by `addToolPostProcessing`, validated with `ConfirmMatch[..., { ___LLMTool }]`, and passed to the `LLMConfiguration`. If `WolframLanguageEvaluator` is among the selected tools, `resolveTools` triggers the `"WolframLanguageEvaluatorTool"` base prompt component via `needsBasePrompt`. Persona overrides: CodeAssistant and AgentOne/AgentOneCoder declare `{"WolframLanguageEvaluator", "DocumentationSearcher", "WolframAlpha", ParentList}`, WolframAlpha declares `{"WolframAlpha", ParentList}`, PlainChat declares `{"WebSearcher", "WebImageSearcher", "WebFetcher", ParentList}`, CodeWriter declares `{ParentList}`, RawModel declares `None` (no tools), and Wolfie/Birdnardo/NotebookAssistant declare custom tool lists with `ParentList`. Dynamic tracking: changes trigger `$toolsTrigger` (`Dynamics.wl`), updating tool-dependent UI elements. Depends on `"LLMEvaluator"` and `"ToolsEnabled"` (declared in `$autoSettingKeyDependencies`). Listed in `$usableChatSettingsKeys` and exposed in the `ChatPreferences` tool (`ChatPreferences.wl`) with an `interpretTools` validator that checks tool names against `$AvailableTools`. No model-specific overrides exist in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed directly; conditionally included by `makeLLMConfiguration` as described above). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not listed in `$modelInheritedLists`. Not listed in `$popOutSettings`. Exposed indirectly in the preferences UI via the "Tools" tab (`PreferencesContent.wl`), which renders the `toolSettingsPanel` (`ToolManager.wl`) providing a grid interface to install, enable/disable, and configure tools per persona. |
| `"ToolsEnabled"` | `Automatic` | Whether tools are enabled for the current chat. Valid values: `Automatic`, `True`, or `False`. When `Automatic`, resolved per model via `toolsEnabledQ` (`Settings.wl`). The resolution logic proceeds in order: (1) if explicitly set to `True` or `False`, that value is used directly; (2) if `ToolCallFrequency` is non-positive (`0` or negative), returns `False` (tools disabled); (3) if the model has a model-specific `"ToolsEnabled"` override in `$modelAutoSettings`, that value is used; (4) the model name is checked against `$$disabledToolsModel` — models matching `"chat-bison-001"`, `"gemini-1.0-pro"` (and variants), `"gemini-pro-vision"`, or `"gemini-pro"` (case-insensitive) return `False`; (5) all other models return `True`. Model-specific overrides in `$modelAutoSettings`: `False` for GoogleGemini GeminiPro, GoogleGemini GeminiProVision, and O1Mini; `True` for GoogleGemini Gemini2 and GoogleGemini Gemini3. The resolved value affects many downstream behaviors: **Tool resolution** — `resolveTools` (`Tools/Common.wl`) only initializes and selects tools when `ToolsEnabled` is `True`; otherwise the `"Tools"` setting remains unchanged. **Tool prompt** — `getToolPrompt` (`ChatMessages.wl`) returns an empty string `""` when `ToolsEnabled` is `False`, suppressing tool instructions in the system prompt. **Discourage extra tool calls** — `discourageExtraToolCallsQ` (`ChatMessages.wl`) returns `False` when `ToolsEnabled` is `False`, even if `DiscourageExtraToolCalls` is enabled. **Response handling** — `checkResponse` (`SendChat.wl`) has a special pattern for `ToolsEnabled -> False` that either writes the result directly or defers it based on `$AutomaticAssistance`. **Stop tokens** — `autoStopTokens` (`Settings.wl`) returns `{ "[INFO]" }` when `$AutomaticAssistance` is `True` and `ToolsEnabled` is `False`, otherwise `None`. **Hybrid tool method** — `hybridToolMethodQ` (`Settings.wl`) returns `False` when `ToolsEnabled` is `False`. **Tool example prompt style** — `chooseToolExamplePromptStyle` (`Settings.wl`) returns `None` when `ToolsEnabled` is `False`. **Tool Manager warning** — `toolModelWarning` (`ToolManager.wl`) displays a warning message (`$toolsDisabledWarning`) when `ToolsEnabled` is `False` for the current scope. Depends on `"Model"` and `"ToolCallFrequency"` (declared in `$autoSettingKeyDependencies`). Is a dependency for `"HybridToolMethod"`, `"ToolCallExamplePromptStyle"`, and `"Tools"`. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook to gate tool-related features). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not listed in `$usableChatSettingsKeys` or the `ChatPreferences` tool definition. Not listed in `$popOutSettings` or `$modelInheritedLists`. Exposed in the preferences UI (`PreferencesContent.wl`) via `makeToolsEnabledMenu` as a `PopupMenu` with three choices: "Enabled by Model" (`Automatic`), "Enabled Always" (`True`), and "Enabled Never" (`False`), located in the Features content section alongside Multimodal and ToolCallFrequency settings. |
| `"ToolMethod"` | `Automatic` | Method for tool calling. Controls the mechanism by which the LLM invokes tools. Valid values: `"Service"` (uses the LLM service's native tool calling API), `"Simple"` (uses slash-command format with `/exec` markers), `"Textual"` or `"JSON"` (uses `ENDTOOLCALL`-based prompt format), or `Automatic`. When `Automatic`, resolved by `chooseToolMethod` (`Settings.wl`): if all resolved tools are "simple tools" (i.e., members of `$DefaultTools`), resolves to `"Simple"`; otherwise remains `Automatic` (which is treated as a generic prompt-based method using `ENDTOOLCALL` markers). The automatic resolution happens in `resolveAutoSettings0` after `Tools` has been resolved. When `"Service"`, `makeLLMConfiguration` (`SendChat.wl`) passes `"ToolMethod" -> "Service"` and `LLMTool` definitions to `LLMConfiguration`, enabling native tool calling. For non-`"Service"` methods, tool definitions are omitted from `LLMConfiguration` and tool calling is handled entirely through prompt-based instructions: `makeToolPrompt` (`Tools/Common.wl`) assembles a system prompt from five components (pre-prompt, listing, example, post-prompt, preference prompt), each selected by method — `"Service"` returns `Nothing` (no prompt injection), `"Simple"` uses `$simpleToolPre`/`$simpleToolListing`/`$simpleToolPost`, and other methods use `$toolPre`/`$toolListing`/`$toolPost`. Affects message preparation: when `"Service"`, `prepareMessagesForLLM0` rewrites messages via `rewriteServiceToolCalls`; otherwise, `"ToolRequests"` and `"ToolResponses"` keys are dropped from messages. Affects tool call parsing: `"Simple"` uses `simpleToolRequestParser`; other methods use `toolRequestParser` (with a TODO noting a planned `getToolRequestParser` dispatch). Affects tool response formatting: when `"Service"`, the tool response message role is forced to `"Tool"` regardless of the `ToolResponseRole` setting; otherwise uses the `ToolResponseRole` value. Affects stop tokens: `"Simple"` adds `"\n/exec"`, `"Service"` adds none (only `$endToken`), `"Textual"`/`"JSON"` adds `"ENDTOOLCALL"`, and other/fallback adds both `"ENDTOOLCALL"` and `"\n/exec"`. When `"Service"`, triggers the `"ServiceToolCallRetry"` base prompt component (`ChatMessages.wl`). The global variable `$simpleToolMethod` (`CommonSymbols.wl`, set in `Formatting.wl`) tracks whether the current method is `"Simple"`, used by `toolRequestToString` (`LLMUtilities.wl`) and `parsePartialToolCallString` (`Formatting.wl`) for formatting. Model-specific overrides in `$modelAutoSettings`: `"Service"` for Anthropic (general), AzureOpenAI, DeepSeek Chat, OpenAI (general), GPT-3.5, GPT-5, O1, O3, and O4-Mini; `Verbatim @ Automatic` (keeps prompt-based tools, often paired with `HybridToolMethod -> True`) for Claude 2, GPT-4o, GPT-4.1, and O3-Mini. Not listed in `$autoSettingKeyDependencies` (no dependencies on other settings for its own resolution), but is a dependency of `HybridToolMethod`. Not in `$llmConfigPassedKeys` (not passed directly through `LLMConfiguration`); instead, `"ToolMethod" -> "Service"` is conditionally added by `makeLLMConfiguration` when the resolved value is `"Service"` or `HybridToolMethod` is `True`. Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not exposed in the preferences UI. |
| `"HybridToolMethod"` | `Automatic` | Whether to use hybrid tool calling, combining service-level and prompt-based tool calling. When `Automatic`, resolved by `hybridToolMethodQ` (`Settings.wl`): returns `False` if `ToolsEnabled` is `False`; returns `False` if `ToolMethod` is `"Service"` (since service-level calling is already in use, hybrid mode is unnecessary); otherwise returns `True` if the model matches `$$hybridToolModel`, which requires the service to be `"OpenAI"`, `"AzureOpenAI"`, or `"LLMKit"` (or the model to be a plain string); returns `False` for all other models. When `True`, `makeLLMConfiguration` in `SendChat.wl` builds the `LLMConfiguration` with `"ToolMethod" -> "Service"` and includes `LLMTool` definitions in the `"Tools"` parameter, enabling the LLM service's native tool calling API alongside Chatbook's prompt-based tool calling. This means the model receives both prompt-based tool instructions (from the resolved `ToolMethod`, e.g., `"Simple"`) and service-level tool definitions, allowing it to use either mechanism. When `False`, the `LLMConfiguration` omits tool definitions and relies solely on the prompt-based tool method. Depends on `"Model"`, `"ToolsEnabled"`, and `"ToolMethod"` (declared in `$autoSettingKeyDependencies`). Model-specific overrides in `$modelAutoSettings`: `True` for GPT-4o, GPT-4.1, and O3-Mini (which use `ToolMethod -> Verbatim @ Automatic` to keep prompt-based tools alongside service tools); `False` for DeepSeek Reasoner, GPT-5, O1, O3, and O4-Mini (which either use pure `"Service"` tool method or have limited tool support). Not in `$llmConfigPassedKeys` (not passed directly through `LLMConfiguration`), but indirectly controls whether `LLMConfiguration` includes `"ToolMethod" -> "Service"` and `"Tools"`. Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not exposed in the preferences UI. |
| `"ToolOptions"` | `$DefaultToolOptions` | Per-tool option overrides as a nested `Association` mapping tool names to their option associations. The default value `$DefaultToolOptions` (`Tools/Common.wl`) provides options for five tools: `"WolframAlpha"` (`"DefaultPods" -> False`, `"FoldPods" -> False`, `"MaxPodByteCount" -> 1000000`), `"WolframLanguageEvaluator"` (`"AllowedExecutePaths" -> Automatic`, `"AllowedReadPaths" -> All`, `"AllowedWritePaths" -> Automatic`, `"AppendURIPrompt" -> False`, `"EvaluationTimeConstraint" -> 60`, `"Method" -> Automatic`, `"PingTimeConstraint" -> 30`), `"WebFetcher"` (`"MaxContentLength" -> 12000`), `"WebSearcher"` (`"AllowAdultContent" -> Inherited`, `"Language" -> Inherited`, `"MaxItems" -> 5`, `"Method" -> "Google"`), and `"WebImageSearcher"` (same keys as `"WebSearcher"`). Note that the default is defined with delayed evaluation (`:>`) in `$defaultChatSettings` (`Settings.wl`), so it evaluates `$DefaultToolOptions` fresh each time. During chat processing, `resolveTools` (`Tools/Common.wl`) assigns the resolved value to the `$toolOptions` global variable via `Lookup[settings, "ToolOptions", $DefaultToolOptions]`. Individual tool option values are accessed at runtime through `toolOptionValue[toolName, key]` (`Tools/ToolOptions.wl`), which checks `$toolOptions` first and falls back to `$DefaultToolOptions`. Tool implementations use this to read their configuration: e.g., `Sandbox.wl` reads `"EvaluationTimeConstraint"`, `"Method"`, `"AllowedReadPaths"`, `"AllowedWritePaths"`, `"AllowedExecutePaths"`, `"AppendURIPrompt"`, and `"PingTimeConstraint"` for `"WolframLanguageEvaluator"`; `WolframAlpha.wl` reads `"DefaultPods"`, `"FoldPods"`, and `"MaxPodByteCount"`; `WebFetcher.wl` reads `"MaxContentLength"`. Users can programmatically modify tool options via `SetToolOptions` (exported in `Main.wl`): `SetToolOptions["WolframLanguageEvaluator", "EvaluationTimeConstraint" -> 120]` sets a global override, `SetToolOptions[notebookObj, "WebSearcher", "MaxItems" -> 10]` sets a notebook-scoped override, and `SetToolOptions["WolframAlpha", Inherited]` resets a tool's options to defaults. Options are stored in `{TaggingRules, "ChatNotebookSettings", "ToolOptions", toolName, optionKey}`. Chat mode overrides exist: NotebookAssistance mode (`ShowNotebookAssistance.wl`) sets `"WolframLanguageEvaluator" -> <|"AppendURIPrompt" -> True, "Method" -> "Session"|>`. No model-specific overrides in `$modelAutoSettings`. No dependencies in `$autoSettingKeyDependencies`. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook to configure tool behavior). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not listed in the `ChatPreferences` tool definition. Not exposed in the preferences UI. |
| `"ToolSelectionType"` | `<\|\|>` | Per-tool override that controls whether a tool is enabled globally (for all personas), disabled globally, or left to per-persona selection. The value is an `Association` mapping tool canonical names (strings) to one of three values: `All` (tool is always enabled regardless of persona), `None` (tool is never enabled regardless of persona), or `Inherited` (tool enablement is determined per-persona via the `"ToolSelections"` setting). When a tool's canonical name is absent from the association, it behaves as `Inherited`. Used by `selectTools` (`Tools/Common.wl`) during tool resolution: `getToolSelectionTypes` retrieves this setting and filters it to keys present in `$AvailableTools`. Tools with selection type `All` are added to the selected tool set unconditionally (unioned with per-persona selections), while tools with selection type `None` are removed unconditionally (even if selected by the persona). The default empty association (`<\|\|>`) means all tools default to per-persona selection behavior. Exposed in the preferences UI via the Tool Manager (`ToolManager.wl`) as a `PopupMenu` per tool row with three options: "Enabled by persona" (`Inherited`), "Never" (`None`), and "Always" (`All`). The Tool Manager also provides a clear/reset button that unsets both `"ToolSelections"` and `"ToolSelectionType"` for a given tool. In the cloud UI, `cloudToolEnablePopup` (`PreferencesContent.wl`) provides a similar popup. When a per-persona checkbox is toggled manually in the Tool Manager, the `ToolSelectionType` for that tool is automatically unset (reverting to `Inherited`), since explicit per-persona selections supersede the global override. Used by `enableTool` (`ResourceInstaller.wl`) to set a newly installed tool's selection type to `All`, making it immediately available. Used by `deleteTool` (`ToolManager.wl`) to clean up the selection type when a tool is uninstalled. The `resetChatPreferences["Tools"]` function (`PreferencesContent.wl`) resets this setting to `Inherited` along with `"ToolSelections"`. No dependencies in `$autoSettingKeyDependencies`. No model-specific overrides in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook to determine the active tool set). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not listed in `$usableChatSettingsKeys` or the `ChatPreferences` tool definition. Not listed in `$popOutSettings` or `$modelInheritedLists`. |
| `"ToolCallFrequency"` | `Automatic` | Controls how often the LLM should use tools. Accepts `Automatic` or a number between `0` and `1`. When `Automatic`, no tool frequency preference prompt is injected into the system message. When set to a numeric value, `makeToolPreferencePrompt` (`Tools/Common.wl`) injects a "User Tool Call Preferences" section into the system prompt. The frequency value is scaled to a percentage and mapped to one of six explanation levels (computed as `Round[Clip[5 * freq, {0, 5}]]`): `0` (0%) = "Only use a tool if explicitly instructed"; `1` (~20%) = "Avoid using tools unless necessary"; `2` (~40%) = "Only use tools if it will significantly improve quality"; `3` (~60%) = "Use tools whenever appropriate"; `4` (~80%) = "Use tools whenever there's even a slight chance of improvement"; `5` (100%) = "ALWAYS make a tool call in EVERY response". Setting this to a non-positive value (`0` or negative) causes `ToolsEnabled` to resolve to `False` via `toolsEnabledQ` (`Settings.wl`), effectively disabling all tools. The `ToolsEnabled` setting declares a dependency on `ToolCallFrequency` in `$autoSettingKeyDependencies`. Exposed in the preferences UI (`PreferencesContent.wl`) as a popup menu (Automatic/Custom) with a slider from "Rare" to "Often" when "Custom" is selected; selecting "Custom" defaults the frequency to `0.5`. Also exposed in the advanced chat UI (`UI.wl`) as a slider with an option to reset to `Inherited`. Listed in the `ChatPreferences` tool definition (`Tools/DefaultToolDefinitions/ChatPreferences.wl`) as a `Restricted["Number", {0, 1}]` parameter, allowing the LLM itself to adjust this setting. Not in `$llmConfigPassedKeys` (not passed directly to the LLM service; used internally to generate the preference prompt). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. |
| `"ToolCallRetryMessage"` | `Automatic` | Whether to append a retry-guidance system message after each tool response. When `True`, `makeToolResponseMessage` (`SendChat.wl`) appends `$toolCallRetryMessage` — a system message with `"HoldTemporary" -> True` containing: *"IMPORTANT: If a tool call does not give the expected output, ask the user before retrying unless you are ABSOLUTELY SURE you know how to fix the issue."* The `"HoldTemporary"` flag means this message is included in the current request but not persisted in the conversation history. When `Automatic`, resolved by `toolCallRetryMessageQ` (`Settings.wl`), which delegates to `llmKitQ`: returns `True` if the session uses LLMKit authentication (i.e., `Authentication` is `"LLMKit"`, or the model's `"Service"` or `"Authentication"` is `"LLMKit"`), `False` otherwise. Model-specific overrides in `$modelAutoSettings`: explicitly `False` for GPT-4.1 and GPT-5 (and GPT-5.1/GPT-5.2 via inheritance). Depends on `"Authentication"` and `"Model"` (declared in `$autoSettingKeyDependencies`). Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook to control tool response message construction). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not exposed in the preferences UI. |
| `"ToolExamplePrompt"` | `Automatic` | Specifies the tool example prompt included in the system prompt to demonstrate tool usage patterns to the LLM. Used by `getToolExamplePrompt` (`Tools/Common.wl`) as part of `makeToolPrompt`, which assembles the full tool prompt from five components: tool pre-prompt, tool listing, tool example prompt, tool post-prompt, and tool preference prompt. Valid values: `Automatic`, `None`, or a custom template (any value matching `$$template`: `_String`, `_TemplateObject`, `_TemplateExpression`, `_TemplateSequence`). When `Automatic` (the resolved default for all services via `autoToolExamplePromptSpec` in `Settings.wl`), `getToolExamplePrompt` generates the example prompt from `$fullExamples` (`Tools/Examples.wl`) using the `ToolCallExamplePromptStyle` to format the examples — it sets `$messageTemplateType` to the style and returns the assembled examples string. When a custom template is provided, it is used directly. When `None`, no example prompt is included (`Nothing`). If `ToolCallExamplePromptStyle` is `None`, the example prompt is also omitted. Model-specific overrides in `$modelAutoSettings`: `None` for Claude 3 (Anthropic Claude3 family); explicitly `Automatic` for Claude 3.7 Sonnet (overriding the inherited `None` from Claude 3). When `Automatic`, resolved by `chooseToolExamplePromptSpec` (`Settings.wl`), which delegates to `autoToolExamplePromptSpec` based on the model's service — currently returns `Automatic` for all services (the catch-all definition `autoToolExamplePromptSpec[ _ ] := Automatic`). Personas can provide a `ToolExamplePrompt` file (`.md`, `.txt`, `.wl`, `.m`, or `.wxf`) in their LLM configuration directory, which is loaded by the persona system (`Personas.wl`) as a prompt file and used as this setting's value. Depends on `"Model"` (declared in `$autoSettingKeyDependencies`). Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook to construct the tool system prompt). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not exposed in the preferences UI. |
| `"ToolCallExamplePromptStyle"` | `Automatic` | Style of chat message templates used for tool call example prompts in the system prompt. Valid values: `"Basic"`, `"ChatML"`, `"XML"`, `"Instruct"`, `"Phi"`, `"Llama"`, `"Gemma"`, `"Nemotron"`, `"DeepSeekCoder"`, `"Zephyr"`, `"Boxed"`, `None`, or `Automatic`. When `Automatic`, resolved based on service and model family (e.g., OpenAI/AzureOpenAI → `"ChatML"`, Anthropic → `"XML"`, local models use family-specific templates). `None` when tools are disabled. Also determines style-specific stop tokens. Depends on `"Model"` and `"ToolsEnabled"`. Defined in `Tools/Examples.wl`; used by `getToolExamplePrompt` in `Tools/Common.wl`. |
| `"ToolResponseRole"` | `Automatic` | The message role assigned to tool response messages sent back to the LLM. Used by `makeToolResponseMessage` (`SendChat.wl`) to set the `"Role"` key in the tool response association. Valid values: `Automatic`, `"System"`, `"User"`, or `"Tool"`. When `Automatic`, resolved to `"System"` via the persona default in `$modelAutoSettings[ Automatic, Automatic ]` (`Settings.wl`). When `ToolMethod` is `"Service"`, the role is forced to `"Tool"` regardless of this setting's value. The role also affects message formatting: when `"User"`, the tool response content is wrapped in `<tool_response>...</tool_response>` tags; when `"SystemTags"` style is active (controlled by `ToolResponseStyle`), content is wrapped in `<system>...</system>` tags; otherwise, the response content is used as-is. Model-specific overrides in `$modelAutoSettings`: `"User"` for Anthropic Claude 2, DeepSeek DeepSeekReasoner, all MistralAI models, TogetherAI DeepSeekReasoner, and local models (Qwen, Nemotron, Mistral). No dependencies in `$autoSettingKeyDependencies`. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook to construct tool response messages). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not exposed in the preferences UI. |
| `"ToolResponseStyle"` | `Automatic` | Controls how tool response content is wrapped/formatted before being sent back to the LLM. Used by `makeToolResponseMessage0` (`SendChat.wl`) to determine the message structure for tool responses. Valid values: `Automatic`, `"SystemTags"`, or other style strings. When `"SystemTags"`, the tool response content is wrapped in `<system>...</system>` tags and the role defaults to `"System"` if not otherwise specified. When the role is `"User"` (regardless of style), the content is wrapped in `<tool_response>...</tool_response>` tags. For all other style values (including `Automatic`), the response content is used as-is without wrapping. Note that `"SystemTags"` takes priority over the `"User"` role — if both `ToolResponseStyle` is `"SystemTags"` and `ToolResponseRole` is `"User"`, the `"SystemTags"` pattern matches first. Model-specific overrides in `$modelAutoSettings`: `"SystemTags"` for MistralAI (paired with `ToolResponseRole` → `"User"`). No global default in `$modelAutoSettings[ Automatic, Automatic ]`, so `Automatic` is the effective default for most models. No dependencies in `$autoSettingKeyDependencies`. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook to format tool response messages). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not exposed in the preferences UI. |
| `"SplitToolResponseMessages"` | `Automatic` | Whether to split tool responses into separate messages. Model default: `False`. Enabled for Anthropic models as a workaround. |
| `"MaxToolResponses"` | `5` | Maximum number of tool responses allowed per chat turn before the tool-calling loop is stopped. During each chat turn, the `$toolCallCount` global variable (`CommonSymbols.wl`) is incremented each time the LLM makes a tool call (`SendChat.wl`), initialized to `0` via `ChatState.wl`. The `sendToolResponseQ` function (`SendChat.wl`) checks whether to continue the tool-calling loop: if `$toolCallCount > n` (where `n` is the `MaxToolResponses` value), it returns `False`, causing the chat to stop via `throwTop @ StopChat @ cell` — ending the current turn without sending the tool response back to the LLM. Note that the comparison uses `>` (not `>=`), so the LLM can make up to `n + 1` tool calls before being stopped (the count is incremented before the check). There is a TODO comment in the source noting that this should allow one final response when reaching the limit and disable tools for the next chat submit. Model-specific overrides in `$modelAutoSettings`: `3` for O1, O3, and O4-Mini (OpenAI reasoning models). Chat mode overrides: NotebookAssistance mode explicitly sets this to `5` in `$notebookAssistanceBaseSettings` (`ShowNotebookAssistance.wl`). Listed in `$popOutSettings` (`ConvertChatNotebook.wl`) as one of four settings shown during chat notebook conversion. No dependencies in `$autoSettingKeyDependencies`. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook to control the tool-calling loop). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not exposed in the preferences UI. |
| `"SendToolResponse"` | `Automatic` | Whether to send tool responses back to the LLM for further processing. When set to `False`, the `sendToolResponseQ` function (`SendChat.wl`) returns `False`, causing the chat turn to end via `throwTop @ StopChat @ cell` immediately after the tool response is received — the tool still executes and its output is displayed, but the result is not sent back to the LLM for a follow-up response. When `Automatic` (the default), tool responses are sent back unless the tool itself signals that it is terminal: individual tools can include `"SendToolResponse" -> False` in their output data or in the `"Output"` key of their response association, and the `terminalToolResponseQ`/`terminalQ` functions (`SendChat.wl`) detect this to stop the tool-calling loop for that specific tool call. This setting also affects `OpenToolCallBoxes` via `$autoSettingKeyDependencies` (`Settings.wl`): when `SendToolResponse` is `False`, `openToolCallBoxesQ` returns `True`, causing tool call boxes to be expanded so the user can see the tool output directly (since it won't be summarized by the LLM). No model-specific overrides in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook to control the tool-calling loop). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not exposed in the preferences UI. |
| `"EndToken"` | `Automatic` | End-of-turn token that signals the LLM has finished its response. Model default: `"/end"`. Some models use `None` (e.g., GPT-4.1). The resolved value is stored in the `$endToken` global variable (`CommonSymbols.wl`) during `resolveAutoSettings` (`Settings.wl`). Used in three ways: (1) **Base prompt instruction**: The `"EndTurnToken"` base prompt component (`Prompting.wl`) adds `"* Always end your turn by writing /end."` to the system prompt when `$endToken` is a non-empty string. The related `"EndTurnToolCall"` component (which depends on `"EndTurnToken"`) adds `"* If you are going to make a tool call, you must do so BEFORE ending your turn."`. (2) **Stop tokens**: `$endToken` is included in the stop sequences sent to the LLM API via `methodStopTokens` (`Settings.wl`), which builds tool-method-specific stop token lists (e.g., `{"ENDTOOLCALL", $endToken}` for `"Textual"`/`"JSON"` methods, `{$endToken}` for `"Service"` method, `{"\n/exec", $endToken}` for `"Simple"` method). These are composed into the `"StopTokens"` setting by `autoStopTokens` and passed to `LLMConfiguration` and the HTTP request body. (3) **Tool call example templates**: `Tools/Examples.wl` uses `$endTokenString` (which prepends `"\n"` to `$endToken` when non-empty) in assistant message templates across all example styles (Basic, Instruct, Zephyr, Phi, Boxed, ChatML, XML, DeepSeekCoder, Llama, Gemma, Nemotron) to show the LLM how to end its turn in example conversations. When `None` or empty, the end token is omitted from prompts, stop sequences, and example templates. After receiving a response, `trimStopTokens` in `SendChat.wl` removes stop tokens (including the end token) from the end of the LLM's output. No model-specific overrides exist beyond GPT-4.1 (`None`). Not exposed in the preferences UI. |

## Formatting & Output

| Setting | Default | Description |
| ------- | ------- | ----------- |
| `"AutoFormat"` | `True` | Whether to auto-format LLM output by parsing Markdown syntax and converting it to structured notebook cells. When enabled, the LLM response is processed to convert Markdown elements (code blocks with language detection, headings, bold/italic text, inline code, LaTeX math, images, bullet lists, block quotes, and tables) into properly formatted Wolfram notebook cells. Also includes the `"Formatting"` base prompt component in the system prompt, which instructs the LLM that its output will be parsed as Markdown. When disabled, output is displayed as plain text. Works in conjunction with `"DynamicAutoFormat"` to control whether formatting is applied during streaming. |
| `"DynamicAutoFormat"` | `Automatic` | Whether to apply formatting during streaming, providing live-formatted output as the LLM response streams in. When `True`, the streaming content is processed by the formatting function in real-time, converting Markdown to formatted notebook expressions as they arrive. When `False`, streaming content is displayed as raw text (via `RawBoxes @ Cell @ TextData`) without live formatting. When `Automatic`, resolves to `TrueQ` of the `"AutoFormat"` setting, so dynamic formatting is enabled whenever auto-formatting is enabled. Resolution is handled by `dynamicAutoFormatQ` in `SendChat.wl`, which first checks for an explicit `True`/`False` value, then falls back to `"AutoFormat"`. The resolved value is captured as the `reformat` variable in `activeAIAssistantCell` and passed to `dynamicTextDisplay`, which dispatches between formatted and raw text display. No model-specific overrides exist. Not exposed in the preferences UI; controlled indirectly via the `"AutoFormat"` checkbox. |
| `"StreamingOutputMethod"` | `Automatic` | Controls the method used for streaming output display, specifically whether the streaming content is progressively split into static (already-written) and dynamic (still-updating) portions during LLM response streaming. When `Automatic`, resolves to `"PartialDynamic"` via `resolveAutoSetting0` (`Settings.wl`). The setting is evaluated by the `dynamicSplitQ` function (`Settings.wl`), which determines the `$dynamicSplit` flag used in `chatHandlers` (`SendChat.wl`). Six valid string values in two groups: `"PartialDynamic"`, `"Automatic"`, and `"Inherited"` all resolve to `True` (dynamic splitting enabled), while `"FullDynamic"`, `"Dynamic"`, and `"None"` resolve to `False` (dynamic splitting disabled). Symbol values are converted to their string names. Invalid values trigger an `"InvalidStreamingOutputMethod"` warning message (`Common.wl`) and default to `True`. When dynamic splitting is enabled (`$dynamicSplit` is `True`), the `splitDynamicContent` function (`SendChat.wl`) is called on each `"BodyChunkReceived"` event: it splits the accumulated dynamic content string using `$dynamicSplitRules` (`Formatting.wl`) — a set of string patterns defining safe split points (e.g., after complete code blocks, headings, paragraphs) — then writes the completed static portions as final formatted notebook cells and keeps only the remaining dynamic portion updating live. This reduces the amount of content being dynamically re-rendered, improving performance for long responses. When disabled, all streaming content remains in a single dynamic cell until the response completes. Dynamic splitting is also disabled in headless mode (`$headlessChat`), inline chat (`$InlineChat`), cloud notebooks (`$cloudNotebooks`), and when the Wolfram Engine version is insufficient (`insufficientVersionQ["DynamicSplit"]`). No model-specific overrides exist in `$modelAutoSettings`. No dependencies in `$autoSettingKeyDependencies`. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook for streaming display optimization). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not exposed in the preferences UI. |
| `"NotebookWriteMethod"` | `Automatic` | Method for writing content to the notebook, controlling whether FrontEnd task batching is used for notebook write operations during chat. When `Automatic`, resolves to `"PreemptiveLink"` via `resolveAutoSetting0` (`Settings.wl`). Two valid values: `"PreemptiveLink"` enables FrontEnd task optimization, where notebook write operations (e.g., `NotebookWrite`, formatting toggles, error cell placement) are queued via `createFETask` (`FrontEnd.wl`) into the `$feTasks` list and executed in batches via `runFETasks`, reducing MathLink roundtrips between the kernel and FrontEnd for better responsiveness during streaming; `"ServiceLink"` disables this optimization by locally redefining `createFETask` to be the identity function (`#1 &`), causing all notebook writes to execute inline immediately. The setting is evaluated by the `feTaskQ` function (`SendChat.wl`), which returns `True` for `"PreemptiveLink"` (or unspecified values) and `False` for `"ServiceLink"`. Invalid values trigger an `"InvalidWriteMethod"` warning message (`Common.wl`) and fall back to `True` (task optimization enabled). The `feTaskQ` result is passed to `withFETasks` (`SendChat.wl`), which wraps both the `"BodyChunkReceived"` and `"TaskFinished"` handler functions in `chatHandlers`: when `True`, handlers execute normally with `createFETask` queueing enabled; when `False`, handlers execute inside `Block[{createFETask = #1 &}, ...]`, disabling task queueing. In headless mode (`$headlessChat`), `feTaskQ` always returns `False` regardless of the setting value, since there is no FrontEnd to process tasks. In cloud notebooks (`$cloudNotebooks`), `createFETask` evaluates operations immediately rather than queueing them. No model-specific overrides exist in `$modelAutoSettings`. No dependencies in `$autoSettingKeyDependencies`. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook for notebook write optimization). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not exposed in the preferences UI. |
| `"TabbedOutput"` | `True` | Whether to use paged (tabbed) output for multi-turn chat responses, so that each new LLM response replaces the previous one in the same output cell rather than creating a separate cell. When `True`, consecutive responses to the same chat input are organized as pages within a single output cell: each time `createNewChatOutput` (`SendChat.wl`) is called with an existing target `CellObject`, `prepareChatOutputPage` serializes the previous response content (via `BinarySerialize` and `BaseEncode`) and stores it in a `"PageData"` association in the cell's `TaggingRules`, with keys `"Pages"` (an association mapping page numbers to base64-encoded content), `"PageCount"`, and `"CurrentPage"`. The new response is then printed as the cell content. When `False`, `createNewChatOutput` bypasses paging and always creates a new cell via `cellPrint`, so each response appears in its own separate output cell. When reformatting the final output (`writeReformattedCell` and `reformatCell` in `SendChat.wl`), if `PageData` exists, the page metadata is carried forward via `makeReformattedCellTaggingRules`, which appends the current page's encoded content to the `"Pages"` association and increments the page count. If a response is empty (`None` string), `restoreLastPage` restores the previous page's content instead of deleting the cell. When a cell has more than one page (`PageCount > 1`), the cell dingbat uses an `"AssistantIconTabbed"` template wrapper (instead of the standard persona icon), which provides tab-style navigation UI. Users can navigate between pages via `rotateTabPage` (`Actions.wl`), which reads `"PageData"` from `TaggingRules`, computes the target page with `Mod`, deserializes the page content from the stored base64, and writes it to the cell via `writePageContent`. Overridden to `False` in workspace chat (`$workspaceDefaultSettings` in `StylesheetBuilder.wl`), notebook assistance workspace settings (`$notebookAssistanceWorkspaceSettings`), and sidebar chat settings (`$notebookAssistanceSidebarSettings`) in `ShowNotebookAssistance.wl`. Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook for output cell management). Not listed in `$popOutSettings`. Not exposed in the preferences UI. |
| `"ShowMinimized"` | `Automatic` | Whether LLM response output cells are displayed in a minimized (collapsed) state. When `True`, the output cell is created with `$closedChatCellOptions` (`CellMargins -> -2`, `CellOpen -> False`, `CellFrame -> 0`, `ShowCellBracket -> False`) and an `attachMinimizedIcon` initialization that attaches a small clickable icon to the previous cell bracket, allowing the user to expand the response on demand. When `False`, the output cell is displayed normally in an expanded state. When `Automatic`, the behavior depends on the context: in cloud notebooks (`$cloudNotebooks`), resolves to `False` (always expanded, since `$closedChatCellOptions` produces no options in cloud mode); in desktop notebooks, `Automatic` is treated equivalently to `True` for the minimization check (via `MatchQ[minimized, True|Automatic]` in `activeAIAssistantCell`, `SendChat.wl`), but only when `$AutomaticAssistance` is `True` — meaning the response was triggered by the automatic assistance system (e.g., `WidgetSend` or `autoAssistQ`-qualified evaluations) rather than a direct user chat input. For direct chat inputs (where `$AutomaticAssistance` is `False`), the minimization options are never applied regardless of this setting's value. The setting also influences `$alwaysOpen` via `alwaysOpenQ` (`Actions.wl`): when `ShowMinimized` is `True`, `$alwaysOpen` is set to `False`; when `False`, `$alwaysOpen` is set to `True`; when `Automatic`, `$alwaysOpen` depends on whether the input cell style matches `$$chatInputStyle` or whether the `BasePrompt` contains a severity tag. The `resolveAutoSetting0` for this key returns `Automatic` (`Settings.wl`). No model-specific overrides exist in `$modelAutoSettings`. No dependencies in `$autoSettingKeyDependencies`. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook for output cell rendering). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not listed in `$popOutSettings`. Not exposed in the preferences UI. |
| `"ShowProgressText"` | `Automatic` | Whether to show progress text (e.g., status labels with an ellipsis indicator) in the progress panel while the LLM is generating a response. When `True` (or when the resolved value is truthy), the `$showProgressText` flag (`CommonSymbols.wl`) is set to `True` during `resolveAutoSettings` (`Settings.wl`), which causes `basicProgressTextRow` (`Utils.wl`) to render a styled `"ProgressTitle"` row in the progress panel showing the current operation label (e.g., "Sending request", "Waiting for response") with a `ProgressIndicator[Appearance -> "Ellipsis"]` appended. When `False`, `basicProgressTextRow` returns `Nothing`, hiding the text row entirely so only the progress bar (if enabled via `$showProgressBar`) is shown. The `$showProgressText` flag is also forced to `True` when `"ForceSynchronous"` is truthy, regardless of this setting's value (line 660 in `Settings.wl`). The global default for `$showProgressText` is `False` (`Utils.wl`), so progress text is hidden unless explicitly enabled by this setting or by `ForceSynchronous`. The flag is preserved across handler evaluations via `ChatState.wl` (`$showProgressText = $showProgressText`). The `ContentSuggestions` chat mode (`ChatModes/ContentSuggestions.wl`) locally sets `$showProgressText = True` regardless of this setting. When `Automatic`, resolves to `True` via the model default in `$modelAutoSettings[Automatic, Automatic]`. No model-specific overrides exist beyond the default. No dependencies in `$autoSettingKeyDependencies`. No `resolveAutoSetting0` definition for this key. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook for progress UI rendering). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not listed in `$popOutSettings`. Not exposed in the preferences UI. Model default: `True`. |
| `"OpenToolCallBoxes"` | `Automatic` | Whether tool call display boxes are initially expanded (open) when rendered in the notebook. When `Automatic`, resolved by `openToolCallBoxesQ` (`Settings.wl`): returns `True` if `SendToolResponse` is `False` (meaning the user will not see further LLM processing of the tool result, so the tool output should be visible directly); otherwise returns `Automatic` (which evaluates to `False` via `TrueQ`, keeping boxes collapsed). Depends on `"SendToolResponse"` (declared in `$autoSettingKeyDependencies`). The resolved value is stored in the `$openToolCallBoxes` global variable (`CommonSymbols.wl`) during `resolveAutoSettings` (`Settings.wl`) and preserved across handler evaluation via `ChatState.wl` (initialized to `Automatic`). Used in two places: (1) **Tool call box rendering**: in `parseFullToolCallString` (`Formatting.wl`), the parsed tool call data includes `"Open" -> TrueQ @ $openToolCallBoxes`, which is then passed to `makeToolCallBoxLabel` where it controls whether the `openerView` displaying the tool call details (raw and interpreted views) starts in the open or closed state. (2) **Markdown output handling**: in `checkMarkdownOutput` (`SendChat.wl`), when `$openToolCallBoxes` is truthy, tool output containing markdown images is passed through unchanged; when falsy, `$useMarkdownMessage` is appended to the tool response, which tells the LLM: `"The user does not see the output of this tool call. You must use this output in your response for them to see it."` — prompting the LLM to include image links and other formatted content in its visible response text. No model-specific overrides exist in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook for UI rendering and tool response handling). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not listed in `$popOutSettings`. Not exposed in the preferences UI. |
| `"TrackScrollingWhenPlaced"` | `Automatic` | Whether to auto-scroll the notebook to follow new output as it is placed during and after LLM response streaming. This setting controls the FrontEnd's `"TrackScrollingWhenPlaced"` private cell option on output cells, which causes the notebook to automatically scroll to keep the output visible as new content is written. When `True`, `scrollOutputQ` (`SendChat.wl`) returns `True`, and the output cell created by `activeAIAssistantCell` includes `PrivateCellOptions -> {"TrackScrollingWhenPlaced" -> True}`, enabling auto-scroll during streaming. Additionally, after the final reformatted cell is written via `WriteChatOutputCell`, the `scrollOutput` function calls `SelectionMove` with `AutoScroll -> True` to scroll the notebook to the completed output cell. When `False`, auto-scrolling is disabled entirely: the private cell option is omitted and no post-write scroll occurs. When `Automatic`, resolves via `scrollOutputQ`, which checks whether the Wolfram Engine version meets the minimum requirement of 14.0 (defined in `$versionRequirements` in `Common.wl` via `sufficientVersionQ["TrackScrollingWhenPlaced"]`); if the version is sufficient, auto-scrolling is enabled, otherwise it is disabled. Note that `scrollOutputQ` has a two-argument form `scrollOutputQ[settings, cell]` that always returns `False` — this is used when computing the `"ScrollOutput"` key for the `reformatCell` path, meaning the post-write `SelectionMove`-based scrolling does not occur for reformatted cells (the `PrivateCellOptions`-based scrolling during streaming is sufficient). The `resolveAutoSetting0` for this key delegates to `scrollOutputQ` (`Settings.wl`). No model-specific overrides exist in `$modelAutoSettings`. No dependencies in `$autoSettingKeyDependencies`. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook for notebook scroll behavior). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not listed in `$popOutSettings`. Not exposed in the preferences UI. |
| `"AppendCitations"` | `Automatic` | Whether to automatically append formatted source citations to the LLM response. When enabled, citations are generated from sources gathered by prompt generators (e.g., documentation, web searches, WolframAlpha results) and appended as a markdown section. When disabled, the WolframAlpha prompt generator instead includes a hint asking the LLM to cite sources inline. Model default: `False`. The WolframAlpha persona overrides this to `True`. |

## Personas & UI

| Setting | Default | Description |
| ------- | ------- | ----------- |
| `"LLMEvaluator"` | `"CodeAssistant"` | The persona (LLM evaluator) to use. Determines the system prompt, available tools, and other settings. Can be a persona name string (e.g., `"CodeAssistant"`, `"PlainChat"`, `"RawModel"`) or a full persona `Association` with keys like `"Prompts"`, `"Tools"`, `"Icon"`, `"BasePrompt"`, etc. Built-in personas are defined in `LLMConfiguration/Personas/`. During `resolveAutoSettings` (`Settings.wl`), the string value is resolved via `getLLMEvaluator`, which calls `getNamedLLMEvaluator` to look up persona data from `GetCachedPersonaData`. If the name is not found as a built-in persona, `tryPromptRepositoryPersona` attempts to load it from the Wolfram Prompt Repository via `ResourceObject["Prompt" -> name]`. The resolved persona `Association` is merged with the current settings via `mergeChatSettings`, with `$nonInheritedPersonaValues` keys (including `"LLMEvaluator"` itself) dropped from the persona data before merging, preventing circular inheritance. After resolution, the `"LLMEvaluator"` key in the settings is replaced with the full persona `Association` (or the original string/`None` if unresolvable). When writing settings back to notebook `TaggingRules`, `toSmallSettings` in `SendChat.wl` converts the resolved `Association` back to just the persona name string (via the `"LLMEvaluatorName"` key) to save space. The persona determines: the system prompt (via `"Prompts"`, `"Pre"`, `"PromptTemplate"`, and `"BasePrompt"` keys), available tools (the `"Tools"` setting depends on `"LLMEvaluator"` and `"ToolsEnabled"` in `$autoSettingKeyDependencies`; `selectTools` in `Tools/Common.wl` uses the persona name to look up per-persona tool selections), and the output cell dingbat icon (`makeOutputDingbat` and `makeActiveOutputDingbat` in `SendChat.wl` extract the persona's `"PersonaIcon"` or `"Icon"` key). When `"RawOutput"` is enabled during chat evaluation, the persona is overridden with `GetCachedPersonaData["RawModel"]` in `sendChat` (`SendChat.wl`). `CreateChatDrivenNotebook` defaults this to `"PlainChat"`. Exposed in the notebook preferences UI as a persona PopupMenu selector under the "Notebooks" section (`PreferencesContent.wl`). Also central to the chat action menu in `UI.wl`, where each persona appears as a selectable menu item that writes to `{TaggingRules, "ChatNotebookSettings", "LLMEvaluator"}`, with a "Reset" option that sets the value to `Inherited`. Listed in `$nonInheritedPersonaValues`, so it retains its value from the notebook/cell scope rather than inheriting from persona configurations. No dependencies in `$autoSettingKeyDependencies` (but `"Tools"` depends on it). No model-specific overrides exist in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed to the LLM service). |
| `"PersonaFavorites"` | N/A | List of persona name strings that are marked as favorites, controlling which personas appear at the top of the persona selector menu and their display order. Not included in `$defaultChatSettings` — the setting is lazily initialized at `$FrontEnd` scope. When first accessed, if the value does not match `{___String}`, `filterPersonas` (`UI.wl`) initializes it to `{"CodeAssistant", "CodeWriter", "PlainChat"}`; the `CreatePersonaManagerPanel` function (`PersonaManager.wl`) similarly falls back to `$corePersonaNames` (`{"CodeAssistant", "CodeWriter", "PlainChat", "RawModel"}`) via `Replace[..., Except[{___String}] :> $corePersonaNames]`. Always read from and written to `$FrontEnd` scope (global persistent setting). Used in two primary locations: (1) **Persona selector menu ordering** (`filterPersonas` in `UI.wl`): favorites are placed first in their stored order via `KeyTake[personas, favorites]`, followed by the remaining visible personas in alphabetical order via `KeySort @ KeyTake[personas, Complement[Keys[personas], favorites]]`. (2) **Persona Manager dialog** (`CreatePersonaManagerPanel` in `PersonaManager.wl`): the `favorites` variable is initialized from this setting on panel creation, favorites are listed before non-favorites in the management grid with a visual divider, and the updated favorites list is saved back to `CurrentChatSettings[$FrontEnd, "PersonaFavorites"]` on panel deinitialization. When the "Personas" preferences are reset (`resetChatPreferences["Personas"]` in `PreferencesContent.wl`), the value is set to `$corePersonaNames`. Listed in `$nonInheritedPersonaValues` (`Settings.wl`), so it retains its value from notebook/cell scope rather than inheriting from persona configurations. Excluded from debug/diagnostic data (listed in `$droppedSettingsKeys` in `Common.wl`). No dependencies in `$autoSettingKeyDependencies`. No model-specific overrides exist in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed to the LLM service). Exposed indirectly in the preferences UI via the Persona Manager panel. |
| `"VisiblePersonas"` | `$corePersonaNames` | List of persona name strings controlling which personas appear in the persona selector UI. The default value is `$corePersonaNames`, defined in `Personas.wl` as `{"CodeAssistant", "CodeWriter", "PlainChat", "RawModel"}`. Always read from and written to `$FrontEnd` scope (global persistent setting). Used primarily in `filterPersonas` (`UI.wl`): when building the persona selector menu, `KeyTake[personas, CurrentChatSettings[$FrontEnd, "VisiblePersonas"]]` filters the full persona data to only include personas in this list; personas not in this list are hidden from the menu. The persona selector then orders visible personas by `PersonaFavorites` (favorites first, then remaining visible personas alphabetically). Lazy initialization occurs in `filterPersonas` (`UI.wl`): if the value does not match `{___String}`, it is set to `DeleteCases[Keys[personas], Alternatives["Birdnardo", "RawModel", "Wolfie"]]`, excluding certain personas from the default visible set. The `CreatePersonaManagerPanel` (`PersonaManager.wl`) sanitizes this setting on initialization by intersecting it with the keys of `$CachedPersonaData` to remove any stale persona names that no longer exist. In the Persona Manager dialog, each persona row includes a checkbox (`addRemovePersonaListingCheckbox`) that toggles membership in this list: checking adds the persona name via `Union`, unchecking removes it via `Complement`. When a new persona is installed from the Prompt Repository (`ResourceInstaller.wl`), `addToVisiblePersonas` automatically appends the new persona name to this list via `Union @ Append[...]`. When the "Personas" preferences are reset (`resetChatPreferences["Personas"]` in `PreferencesContent.wl`), the value is reset to `$corePersonaNames`. Not listed in `$nonInheritedPersonaValues`. Excluded from debug/diagnostic data (listed in `$droppedSettingsKeys` in `Common.wl`). No dependencies in `$autoSettingKeyDependencies`. No model-specific overrides exist in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed to the LLM service). Not listed in `$popOutSettings`. Exposed in the preferences UI via the Persona Manager panel checkboxes. |
| `"ChatDrivenNotebook"` | `False` | **Deprecated.** Whether the entire notebook operates in "chat-driven" mode rather than the default "chat-enabled" mode. When `True`, new cells default to `"ChatInput"` style, the persona selector prioritizes PlainChat/RawModel/CodeWriter/CodeAssistant at the top of the list (`UI.wl` `filterPersonas`), and the cloud toolbar displays "Chat-Driven Notebook" instead of "Chat Notebook" (`CloudToolbar.wl`). Used by `CreateChatDrivenNotebook[]`, which wraps `CreateChatNotebook` with `"ChatDrivenNotebook" -> True`, `"LLMEvaluator" -> "PlainChat"`, and `DefaultNewCellStyle -> "ChatInput"`. This is a non-inherited persona value (listed in `$nonInheritedPersonaValues`). |
| `"InitialChatCell"` | `True` | Whether to create an initial empty chat input cell when opening a new chat notebook. When `True`, `CreateChatNotebook` inserts an empty `"ChatInput"` cell (via `initialChatCells` in `CreateChatNotebook.wl`); for cloud notebooks (`$cloudNotebooks`), an additional selection-mover cell is appended to position the cursor. When `False`, the notebook is created with no initial cells. The value is evaluated via `TrueQ`, so only an explicit `True` creates the cell; `Automatic` or other non-boolean values behave as `False`. This is an **unsaved setting** (listed in `$unsavedSettings` in `CreateChatNotebook.wl`): it is used only at notebook creation time and is explicitly dropped from the notebook's `TaggingRules` by `makeChatNotebookSettings`, so it does not persist in the saved notebook. Defined as an option of `CreateChatNotebook` (inherited from `$defaultChatSettings`). When creating a notebook from a `ChatObject`, `initialChatCells` is locally overridden to return the converted message cells instead. No dependencies on other settings. No model-specific overrides exist in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed to the LLM service). Listed in `$nonInheritedPersonaValues`, so it retains its value from notebook/cell scope rather than inheriting from persona configurations. Not exposed in the preferences UI. |
| `"ChatInputIndicator"` | `Automatic` | Text prefix prepended to `"ChatInput"` cells when serializing notebook content for the LLM. When `Automatic`, resolves to `"\|01f4ac"` (speech balloon emoji). Can be any string (e.g., `"[USER]"`), or `None`/`""` to disable the indicator entirely. Only applied when the content is mixed (i.e., when `mixedContentQ` returns `True` in `ChatMessages.wl`, indicating the conversation includes both chat input cells and other cell types). When the indicator is used, the `"ChatInputIndicator"` base prompt component (`Prompting.wl`) is automatically included in the system prompt to explain the indicator's meaning to the LLM: it tells the model that cells prefixed with this symbol are actual user messages, while other cells are context. The indicator text is distinct from cell dingbats controlled by `"SetCellDingbat"`, which are visual notebook icons. The indicator symbol is set per chat evaluation via `chatIndicatorSymbol` in `SendChat.wl` and stored in the global `$chatIndicatorSymbol` variable (`Common.wl`). No model-specific overrides exist. |
| `"SetCellDingbat"` | `True` | Whether to set cell dingbats (icons) on chat cells. When `True`, chat input cells receive dingbat icons (e.g., `"ChatInputCellDingbat"` / `"ChatInputActiveCellDingbat"` template boxes) and assistant output cells receive persona-based dingbat icons (via `makeActiveOutputDingbat` during streaming and `makeOutputDingbat` after completion in `SendChat.wl`). When `False`, no `CellDingbat` option is set on generated cells. Checked via `TrueQ @ settings["SetCellDingbat"]` for output cells and `settings["SetCellDingbat"]` (truthy) for input cells; additionally guarded by `! TrueQ @ $cloudNotebooks`, so dingbats are never set in cloud notebooks regardless of this setting. The input cell dingbat logic replaces `"ChatInputActiveCellDingbat"` with `"ChatInputCellDingbat"` once evaluation begins. For output cells, the active dingbat includes a `"ChatOutputStopButtonWrapper"` that provides a stop button during streaming; after completion, the dingbat is replaced with a static persona icon. For tabbed output (`"TabbedOutput"`), an `"AssistantIconTabbed"` wrapper is used instead. Also used in `ConvertChatNotebook.wl`: `updateCellDingbats` applies `$evaluatedChatInputDingbat` to input cells and a persona-derived dingbat to output cells when converting messages to notebook format. Overridden to `False` in workspace chat (`$workspaceDefaultSettings` in `StylesheetBuilder.wl` and `WorkspaceChat.nb` stylesheet), notebook assistance workspace settings (`$notebookAssistanceWorkspaceSettings`), and sidebar chat settings (`$notebookAssistanceSidebarSettings`) in `ShowNotebookAssistance.wl`. No dependencies in `$autoSettingKeyDependencies`. No model-specific overrides exist in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed to the LLM service). Not exposed in the preferences UI. |
| `"EnableChatGroupSettings"` | `False` | Whether chat group-level settings are enabled. When `True`, during chat evaluation (`SendChat.wl`), `getChatGroupSettings` is called on the evaluation cell to retrieve prompt text from parent group header cells. The feature walks backward through notebook cells to find parent group headers using cell grouping rules (`"TitleGrouping"` and `"SectionGrouping"`) and collects `"Prompt"` values stored in their `TaggingRules` at the path `"ChatNotebookSettings"` → `"ChatGroupSettings"` → `"Prompt"`. Multiple prompts from different grouping levels are joined with `"\n\n"`. The collected group prompt is stored in the settings as `"ChatGroupSettings"` and incorporated into the system prompt via `buildSystemPrompt` in `ChatMessages.wl`, where it appears as the `"Group"` section in the prompt template (between `"Pre"` and `"Base"` sections). When `False`, no group settings are resolved and the `"Group"` section of the system prompt is omitted. Implementation is in `ChatGroups.wl`. Not exposed in the preferences UI. No model-specific overrides exist. |
| `"AllowSelectionContext"` | `Automatic` | Whether to allow the current selection to be used as context. Resolves to `True` when using workspace chat, inline chat, or sidebar chat. |
| `"CurrentPreferencesTab"` | `"Services"` | Persists the user's last-selected tab in the Chatbook preferences dialog. When the preferences dialog opens, the tab is initialized from this setting (defaulting to `"Services"` if unset); when the dialog closes, the current tab selection is saved back. The `openPreferencesPage` function in `PreferencesContent.wl` also writes to this setting at `$FrontEnd` scope to navigate directly to a specific preferences page. This is a non-inherited persona value (listed in `$nonInheritedPersonaValues`) and is excluded from debug/diagnostic data (listed in `$droppedSettingsKeys` in `Common.wl`). Not included in `$defaultChatSettings`. No model-specific overrides exist. |

## Storage & Conversations

| Setting | Default | Description |
| ------- | ------- | ----------- |
| `"ConversationUUID"` | `None` | UUID identifying the current conversation. `None` means no conversation tracking. When a conversation is saved (via `SaveChat`), `ensureConversationUUID` in `Storage.wl` checks whether the current setting is a valid string; if not, it generates a new UUID via `CreateUUID[]` and writes it back to the notebook or cell's `CurrentChatSettings`. Used as the primary key for persistent conversation storage (`Storage.wl`), chat search indexing (`Search.wl`), and chat history listings (`ChatModes/UI.wl`). The `AutoSaveConversations` setting depends on `ConversationUUID` being a valid string (along with `AppName`). Chat modes may override this: workspace chat and sidebar chat automatically generate a new UUID when starting a new conversation (`ChatModes/UI.wl`) or when initialized via NotebookAssistance settings (`ChatModes/ShowNotebookAssistance.wl`); inline chat does not set a UUID (falls back to `None`). When loading a saved conversation, the stored UUID is restored to `CurrentChatSettings` for the target notebook or cell. |
| `"AutoSaveConversations"` | `Automatic` | Whether to automatically save conversations to persistent storage after chat evaluations. When `Automatic`, resolves to `True` if both `AppName` is a valid string and `ConversationUUID` is a valid string; otherwise resolves to `False`. Depends on `"AppName"` and `"ConversationUUID"`. When `True`, conversations are saved after each chat evaluation, subject to the `"MinimumResponsesToSave"` threshold. Chat modes may override this: workspace chat and sidebar chat set it to `True` (with a new `ConversationUUID`), while inline chat sets it to `False`. |
| `"AppName"` | `Automatic` | Application name used to namespace saved conversations, search indexes, and chat history listings. When `Automatic`, resolves to `$defaultAppName` (`"Default"`). When set to a non-default string value, also establishes a service caller context via `setServiceCaller`. Chat modes may override this (e.g., NotebookAssistance uses `"NotebookAssistance"`). The `AutoSaveConversations` setting depends on `AppName` being a valid string. |
| `"MinimumResponsesToSave"` | `1` | Minimum number of assistant responses required before a conversation is automatically saved. Used by `autoSaveQ` in `Storage.wl` to gate auto-saving: after each chat evaluation, the function counts messages with `"Role" -> "Assistant"` in the conversation and only proceeds with saving if the count is greater than or equal to this value. The value must be a positive integer (`_Integer? Positive`); invalid values trigger an `"MinResponses"` confirmation failure. The default of `1` means that auto-saving occurs as soon as the first assistant response is present. Chat mode overrides: the `"GettingStarted"` alias in `ShowNotebookAssistance.wl` sets this to `2`, requiring at least two assistant responses before saving (preventing the initial getting-started prompt from being saved as a conversation). This setting only takes effect when `"AutoSaveConversations"` resolves to `True` (which itself requires valid `"AppName"` and `"ConversationUUID"` values). No dependencies in `$autoSettingKeyDependencies`. No model-specific overrides exist in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook for conversation storage). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not exposed in the preferences UI. |
| `"TargetCloudObject"` | `Automatic` | Target `CloudObject` location for deploying cloud-based chat notebooks via `CreateChatNotebook`. When `Automatic`, `CloudDeploy` is called without a target location, allowing the Wolfram Cloud to assign a default URL. When set to a `CloudObject`, the notebook is deployed to that specific cloud location. Used exclusively in the cloud notebook creation path (`createCloudChatNotebook` in `CreateChatNotebook.wl`): the value is read via `OptionValue[CreateChatNotebook, validOpts, "TargetCloudObject"]` and passed to `deployCloudNotebook`, which calls `CloudDeploy[nb, obj, CloudObjectURLType -> "Environment"]` if the value matches `$$cloudObject` (`HoldPattern[_CloudObject]`), or `CloudDeploy[nb, CloudObjectURLType -> "Environment"]` otherwise. This is an **unsaved setting** (listed in `$unsavedSettings` in `CreateChatNotebook.wl`): it is used only at notebook creation time and is explicitly dropped from the notebook's `TaggingRules` by `makeChatNotebookSettings`, so it does not persist in the saved notebook. Cloud notebook creation is triggered when `$cloudNotebooks` is `True`. No dependencies in `$autoSettingKeyDependencies`. No model-specific overrides exist in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed to the LLM service). Not listed in `$nonInheritedPersonaValues`. Not exposed in the preferences UI. |

## Advanced / Internal

| Setting | Default | Description |
| ------- | ------- | ----------- |
| `"Tokenizer"` | `Automatic` | Tokenizer function used for token counting throughout the chat pipeline. When `Automatic`, resolved via `getTokenizer` (`ChatMessages.wl`) using a three-step fallback: (1) if an explicit non-`Automatic` tokenizer function is already set, it is used directly; (2) if `"TokenizerName"` is a string, the corresponding cached tokenizer is looked up via `cachedTokenizer`; (3) otherwise, the tokenizer is derived from the `"Model"` setting by extracting the model name and matching it to a known tokenizer. Pre-cached tokenizer functions exist for `"chat-bison"` (UTF-8 byte encoding via `ToCharacterCode`), `"gpt-4-vision"` and `"gpt-4o"` (with special image token counting for `Graphics` content), `"claude-3"` (with Claude-specific image token counting), and `"generic"` (GPT-2 fallback). Additional tokenizers are loaded on demand from `.wxf` files in the `Assets/Tokenizers/` directory, or discovered via `Wolfram`LLMFunctions`Utilities`Tokenization`FindTokenizer`; if no model-specific tokenizer is found, the generic GPT-2 tokenizer is used as a fallback. The resolved tokenizer is applied via `applyTokenizer` in `tokenCount` (`ChatMessages.wl`), which tokenizes message content and returns the token list length. The `"Tokenizer"` value can also be set to a custom function (any expression other than `Automatic`/`$$unspecified`), in which case that function is used directly; if the custom value is a string, it is treated as a tokenizer name and the `"TokenizerName"` key is set to the resolved name while `"Tokenizer"` is reset to `Automatic` during `resolveAutoSettings` (`Settings.wl`). Explicitly dropped from saved notebook settings via `toSmallSettings` (`SendChat.wl`, `KeyDrop[as, {"OpenAIKey", "Tokenizer"}]`) because tokenizer functions cannot be serialized. Serialized to a name-based reference (`<| "_Object" -> "Tokenizer", "Data" -> name |>`) in `Feedback.wl` for diagnostic reporting. Depends on `"TokenizerName"` in `$autoSettingKeyDependencies`, which in turn depends on `"Model"`. No model-specific overrides exist in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed to the LLM service via `LLMConfiguration`; used internally by Chatbook for token counting). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not in `$popOutSettings`. Not exposed in the preferences UI. |
| `"HandlerFunctions"` | `$DefaultChatHandlerFunctions` | Callback functions invoked at various stages of chat processing. The value is an `Association` mapping event name strings to handler functions (or `None` to skip). The default value `$DefaultChatHandlerFunctions` (`Settings.wl`) defines 9 event keys, all defaulting to `None`: `"AppendCitationsStart"`, `"AppendCitationsEnd"`, `"ChatAbort"`, `"ChatPost"`, `"ChatPre"`, `"PromptGeneratorEnd"`, `"PromptGeneratorStart"`, `"ToolRequestReceived"`, and `"ToolResponseGenerated"`. The `"ChatAbort"`, `"ChatPost"`, and `"ChatPre"` entries use `RuleDelayed` (`:>`) pointing to global variables `$ChatAbort`, `$ChatPost`, and `$ChatPre` (all initially `None`), allowing runtime reassignment without modifying the association. A 10th event, `"ToolResponseReceived"`, is also dispatched via `applyHandlerFunction` (`SendChat.wl`) but is not included in the default association (falls back to `None` via `getHandlerFunction`). Custom handler values are merged with defaults during resolution: `resolveHandlers` in `Handlers.wl` creates a new association with `$DefaultChatHandlerFunctions` as the base, overlaid with user-provided handlers (after `replaceCellContext` processing), plus a `"Resolved" -> True` marker to prevent re-resolution. Resolution occurs during `resolveAutoSettings` (`Settings.wl`), where `getHandlerFunctions` is called on the settings and the result replaces the `"HandlerFunctions"` key. Each handler function is invoked via `applyHandlerFunction` (`Handlers.wl`), which constructs an argument association containing: `"EventName"` (the event type string), `"ChatNotebookSettings"` (current settings with `"Data"` and `"OpenAIKey"` keys dropped), and event-specific data. This argument association is accumulated in the `$ChatHandlerData` global variable (publicly exported) via `addHandlerArguments`, which merges new data with existing handler state (supporting nested association merging). The handler receives `$ChatHandlerData` with `"DefaultProcessingFunction"` dropped. Event dispatch locations: `"ChatPre"` is called in `sendChat` (`SendChat.wl`) before chat submission, with `"EvaluationCell"` and `"Messages"` in the arguments; `"ChatPost"` and `"ChatAbort"` are called in `applyChatPost` (`Actions.wl`) after chat completion or abort, with `"ChatObject"` and `"NotebookObject"` in the arguments; `"ToolRequestReceived"` is called after parsing a tool call (`SendChat.wl`), with `"ToolRequest"` in the arguments; `"ToolResponseGenerated"` is called after generating a tool response (`SendChat.wl`), with `"ToolResponse"` and `"ToolResponseString"` in the arguments; `"ToolResponseReceived"` is called after the tool response is formatted and ready to send back (`SendChat.wl`), with `"ToolResponse"` in the arguments; `"PromptGeneratorStart"` and `"PromptGeneratorEnd"` are called in `DefaultPromptGenerators.wl` around each prompt generator execution, with `"PromptGenerator"` in the arguments (and `"PromptGeneratorResult"` added for the end event); `"AppendCitationsStart"` and `"AppendCitationsEnd"` are called in `Citations.wl` around citation generation, with `"Sources"` and `"CitationString"` respectively. In the streaming chat submission path (`chatHandlers` in `SendChat.wl`), the resolved handlers are passed to `LLMServices`ChatSubmit` via the `HandlerFunctions` parameter, but `"ChatPost"`, `"ChatPre"`, and `"Resolved"` keys are dropped (listed in `$chatSubmitDroppedHandlers`). The `chatHandlers` function also wraps custom `"BodyChunkReceived"` and `"TaskFinished"` handlers (if provided) inside Chatbook's own streaming logic, calling the user's handler before Chatbook's processing for each body chunk and after task completion. Not passed through `LLMConfiguration` (not in `$llmConfigPassedKeys`). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. No dependencies in `$autoSettingKeyDependencies`. No model-specific overrides exist in `$modelAutoSettings`. Not exposed in the preferences UI. |
| `"HandlerFunctionsKeys"` | `Automatic` | Keys to include in the handler functions callback data passed to `LLMServices`ChatSubmit` and `URLSubmit`. Controls which fields from the streaming response are available to handler functions. When `Automatic` or unspecified, resolves to `$defaultHandlerKeys` (`SendChat.wl`): `{"Body", "BodyChunk", "BodyChunkProcessed", "StatusCode", "TaskStatus", "EventName"}`. When a list of strings, the user-provided keys are merged with `$defaultHandlerKeys` via `Union` (so the default keys are always included). When a single string, it is treated as a one-element list. Invalid values trigger an `"InvalidHandlerKeys"` warning (`Common.wl`) and fall back to `$defaultHandlerKeys`. Resolution is performed by `chatHandlerFunctionsKeys` (`SendChat.wl`), which is called from `resolveAutoSetting0` (`Settings.wl`). The resolved value is passed directly as the `HandlerFunctionsKeys` parameter to `LLMServices`ChatSubmit` (for LLMServices-based chat) and `URLSubmit` (for legacy HTTP-based chat) in `chatSubmit0` (`SendChat.wl`). Also used in other `URLSubmit` calls outside of chat: `VectorDatabases.wl` uses `{"ByteCountDownloaded", "StatusCode"}` for vector database downloads, and `RelatedWolframAlphaResults.wl` uses `{"StatusCode", "BodyByteArray"}` for Wolfram Alpha result fetching. Not passed through `LLMConfiguration` (not in `$llmConfigPassedKeys`). Depends on `"EnableLLMServices"` for resolution ordering in `$autoSettingKeyDependencies`. Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. No model-specific overrides exist in `$modelAutoSettings`. Not exposed in the preferences UI. |
| `"InheritanceTest"` | N/A | Internal diagnostic flag used by the settings inheritance verification system. Not a user-configurable setting and not included in `$defaultChatSettings`. During `verifyInheritance0` (`Settings.wl`), this flag is set to `True` at the `$FrontEnd` scope via `setCurrentValue[fe, {TaggingRules, "ChatNotebookSettings", "InheritanceTest"}, True]` to mark that the inheritance chain for tagging rules has been properly initialized. Subsequent calls to `verifyInheritance` check this flag via `inheritingQ`, which reads `AbsoluteCurrentValue[obj, {TaggingRules, "ChatNotebookSettings", "InheritanceTest"}]` — if `True` (or if the read fails), the object is considered to have valid inheritance and initialization is skipped. During `repairTaggingRules`, the flag is explicitly removed from child objects (notebooks, cells) so that it only persists at the top-level `FrontEndObject`, preventing it from appearing as an explicit override in child scopes. The `verifyInheritance` function is called by `currentChatSettings0` before reading or writing settings, ensuring the inheritance chain is intact. Listed in `$nonInheritedPersonaValues`, so it retains its value from notebook/cell scope rather than inheriting from persona configurations. No model-specific overrides exist. Not exposed in the preferences UI. |
| `"ProcessingFunctions"` | `$DefaultChatProcessingFunctions` | An `Association` of callback functions that control the chat processing pipeline, allowing complete customization of how cells are converted to messages, how messages are post-processed, how chat requests are submitted, how output is formatted, and how output cells are written. The default value `$DefaultChatProcessingFunctions` (`Settings.wl`) is defined using `RuleDelayed` (`:>`) in `$defaultChatSettings` so it is evaluated lazily. It contains six keys: `"CellToChatMessage" -> CellToChatMessage` (converts individual notebook `Cell` expressions to message Associations with `"Role"` and `"Content"` keys; retrieved via `getCellMessageFunction` in `ChatMessages.wl` and wrapped by `checkedMessageFunction`, which validates that custom functions return valid message results — plain strings are auto-wrapped with the cell's role, and invalid results fall back to the default `CellToChatMessage`); `"ChatMessages" -> (#1 &)` (identity function that post-processes the combined message list after construction and prompt generator augmentation; called via `applyProcessingFunction[settings, "ChatMessages", HoldComplete[combined, $ChatHandlerData]]` in `augmentChatMessages` (`ChatMessages.wl`); the result is validated against `$$validMessageResults` and if invalid, an `"InvalidMessages"` warning is printed and the original messages are used instead); `"ChatSubmit" -> Automatic` (submits the prepared messages to the LLM service; when `Automatic`, resolves to `LLMServices``ChatSubmit` in the modern LLMServices path or `URLSubmit` in the legacy HTTP path, selected via the `"DefaultSubmitFunction"` parameter passed to `applyProcessingFunction`; called in `chatSubmit0` (`SendChat.wl`) with the standardized messages, `LLMConfiguration`, authentication, handler functions, and handler function keys as arguments); `"FormatChatOutput" -> FormatChatOutput` (formats the LLM response text for notebook display; retrieved via `getFormattingFunction` in `SendChat.wl`, which wraps it to set `$ChatHandlerData["EventName"]` to `"FormatChatOutput"` before calling; `FormatChatOutput` (`Formatting.wl`) dispatches on a status Association with a `"Status"` key — `"Streaming"` for live-formatted output during streaming, `"Finished"` for final formatting, `"Waiting"` for a progress indicator — and converts Markdown to formatted notebook expressions via `reformatTextData`); `"FormatToolCall" -> FormatToolCall` (formats tool call data for display in the notebook; retrieved both via `getToolFormatter` in `SendChat.wl` (which wraps it similarly to set `$ChatHandlerData["EventName"]`) and directly from `$ChatHandlerData["ChatNotebookSettings", "ProcessingFunctions", "FormatToolCall"]` in `inlineToolCall` (`Formatting.wl`) for inline tool call rendering; `FormatToolCall` (`Formatting.wl`) takes a raw tool call string and parsed data Association, with an optional info Association containing `"Status"`); `"WriteChatOutputCell" -> WriteChatOutputCell` (writes the formatted output cell to the notebook; called via `applyProcessingFunction[settings, "WriteChatOutputCell", HoldComplete[cell, new, info]]` inside `createTask` in `writeReformattedCell` (`SendChat.wl`); `WriteChatOutputCell` (`SendChat.wl`) has two main definitions: for inline chat (`$InlineChat`), it delegates to `writeInlineChatOutputCell`; for regular chat, it uses `NotebookWrite` to insert the cell, sets cell tags from the `"ExpressionUUID"` in the info Association, attaches the chat output menu via `attachChatOutputMenu`, and handles scrolling via `scrollOutput` based on the `"ScrollOutput"` info key). During `resolveAutoSettings` (`Settings.wl`), the value is resolved via `getProcessingFunctions` (`Handlers.wl`), which calls `resolveFunctions`: this merges user-provided overrides on top of `$DefaultChatProcessingFunctions`, applies `replaceCellContext` to convert `$CellContext` symbols to the global context, and marks the result with `"Resolved" -> True` to prevent re-resolution. Invalid values trigger an `"InvalidFunctions"` warning (`Common.wl`) and fall back to defaults. Each processing function is invoked via `applyProcessingFunction` (`Handlers.wl`), which retrieves the function via `getProcessingFunction` (falling back through the resolved association, then `$DefaultChatProcessingFunctions`), merges parameters with `$ChatHandlerData` (adding `"ChatNotebookSettings"` and `"DefaultProcessingFunction"` to the handler data), applies the function to its held arguments, and logs timing via `LogChatTiming`. The `$DefaultChatProcessingFunctions` variable is publicly exported (`Main.wl`). No dependencies in `$autoSettingKeyDependencies`. No model-specific overrides exist in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed to the LLM service; used internally by Chatbook for pipeline customization). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not listed in `$popOutSettings`. Not listed in `$droppedSettingsKeys`. Not exposed in the preferences UI. |
| `"ConversionRules"` | `None` | Custom transformation rules applied to notebook cells before they are serialized to chat message strings. When `None` (default), cells are passed through unmodified. When set to a list of replacement rules, the rules are compiled into a `Dispatch` table (cached for reuse) and applied to each cell via `ReplaceRepeated` inside `CellToString` (`Serialization.wl`) before string conversion occurs. This enables custom box-level or expression-level transformations of cell content prior to sending it to the LLM. The setting value is read from `CurrentChatSettings` in `makeChatMessages` (`ChatMessages.wl`) and stored in the dynamic variable `$conversionRules`, which `CellToString` picks up as its default `"ConversionRules"` option. Invalid values (neither a rule list nor a valid `Dispatch` table) trigger an `"InvalidConversionRules"` warning and fall back to `None`. |
| `"ExperimentalFeatures"` | `Automatic` | List of enabled experimental feature names, resolved dynamically from other settings. When `Automatic`, the `autoExperimentalFeatures` function (`Settings.wl`) builds a list based on two conditions: `"RelatedWolframAlphaResults"` is included if `"WolframAlphaCAGEnabled"` is `True` or `"RelatedWolframAlphaResults"` is in the `"PromptGenerators"` list; `"RelatedWebSearchResults"` is included if `"WebSearchRAGMethod"` is `"Tavily"` or `"WebSearch"` is in the `"PromptGenerators"` list. The resolved list is stored in the `$experimentalFeatures` global variable (`Settings.wl`) and preserved across handler evaluation via `ChatState.wl`. Individual features are checked at runtime via `featureEnabledQ` (`Settings.wl`), which tests membership in the resolved list. The primary consumer is `resolvePromptGenerators` in `PromptGenerators/Common.wl`, which appends `"RelatedWolframAlphaResults"` and/or `"WebSearch"` to the active prompt generators list based on feature flags. Depends on `"WolframAlphaCAGEnabled"`, `"WebSearchRAGMethod"`, and `"PromptGenerators"` (declared in `$autoSettingKeyDependencies`). No model-specific overrides exist. Not exposed in the preferences UI. |
| `"OpenAIKey"` | `Automatic` | **Deprecated.** OpenAI API key used for direct OpenAI API authentication in the legacy (non-LLMServices) HTTP request path. Only used when `EnableLLMServices` resolves to `False` (i.e., the `Wolfram/LLMFunctions` paclet is not installed or `EnableLLMServices` is explicitly `False`). When `Automatic`, the key is resolved at chat time by `toAPIKey` (`Actions.wl`), which checks in order: (1) `SystemCredential["OPENAI_API_KEY"]`, (2) `Environment["OPENAI_API_KEY"]`, (3) an interactive API key dialog (`apiKeyDialog`); if none produces a valid string, throws a `"NoAPIKey"` failure. If already a string, the value is used directly. The resolved key is assigned back into the settings association (`settings["OpenAIKey"] = key`) in the legacy `sendChat` overload (`SendChat.wl`), then used by `makeHTTPRequest` (`SendChat.wl`) to construct the `"Authorization"` header (`"Bearer " <> key`) for the `HTTPRequest` sent to `OpenAIAPICompletionURL`. The entire legacy `sendChat` overload and `toAPIKey` function are marked with TODO comments indicating they are obsolete once LLMServices is widely available. In the modern code path (`$useLLMServices` is `True`), this setting is completely unused — the `LLMServices` framework handles authentication internally via the `Authentication` setting and service-specific credential management. **Security handling**: the `maskOpenAIKey` function (`Common.wl`) replaces actual key values with `"**********"` in all diagnostic, debug, and error output. The key is explicitly dropped from: saved notebook settings via `toSmallSettings` (`SendChat.wl`, `KeyDrop[as, {"OpenAIKey", "Tokenizer"}]`), handler function callback data via `$settingsDroppedKeys` (`Handlers.wl`), and notebook settings diagnostic output (`Common.wl`). Not resolved during `resolveAutoSettings` — there is no `resolveAutoSetting0` case for `"OpenAIKey"`, so `Automatic` persists until `toAPIKey` is called at chat time. No dependencies in `$autoSettingKeyDependencies`. No model-specific overrides exist in `$modelAutoSettings`. Not in `$llmConfigPassedKeys` (not passed through `LLMConfiguration`). Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not in `$popOutSettings`. Not exposed in the preferences UI. |
| `"OpenAIAPICompletionURL"` | `"https://api.openai.com/v1/chat/completions"` | **Deprecated.** OpenAI API completion endpoint URL, used only in the legacy (non-LLMServices) HTTP request path. The default value is a fixed string (`"https://api.openai.com/v1/chat/completions"`), not `Automatic`. Only used when `$useLLMServices` is `False` (i.e., when the `EnableLLMServices` setting resolves to `False` or the required `Wolfram/LLMFunctions` paclet is not installed). In this legacy path, `makeHTTPRequest` (`SendChat.wl`) reads the value from settings via `Lookup[settings, "OpenAIAPICompletionURL"]`, confirms it is a string, and uses it as the URL for the `HTTPRequest` sent to the OpenAI chat completions API, with the `"OpenAIKey"` setting providing the `"Authorization"` header. The entire `sendChat` overload that uses this setting is marked with a TODO comment: `"this definition is obsolete once LLMServices is widely available"`. When `$useLLMServices` is `True` (the primary/modern code path), this setting is completely unused — the LLMServices framework handles endpoint routing internally. Not included in `$llmConfigPassedKeys` (`SendChat.wl`), so it is NOT passed through `LLMConfiguration` when using the LLMServices framework. No model-specific overrides exist in `$modelAutoSettings`. No dependencies in `$autoSettingKeyDependencies`. Not listed in `$nonInheritedPersonaValues`, so it is inherited from persona configurations. Not listed in `$popOutSettings`. Conditionally exposed in the preferences UI (`PreferencesContent.wl`): the `makeOpenAICompletionURLInput` function returns `Nothing` when `$useLLMServices` is `True` (hiding the control entirely) and renders a string input field in the "Notebooks" tab via `highlightControl` when `$useLLMServices` is `False`, reading and writing `CurrentChatSettings[$preferencesScope, "OpenAIAPICompletionURL"]`. |

---

## Model-Specific Auto Settings

When a setting has a value of `Automatic`, the resolution pipeline checks `$modelAutoSettings` for a model-specific default. Settings are looked up in order of specificity:

1. Service + model name (e.g., `$modelAutoSettings["Anthropic", "Claude4"]`)
2. Service + model ID
3. Service + model family
4. Any service + model name (e.g., `$modelAutoSettings[Automatic, "GPT4Omni"]`)
5. Any service + model ID
6. Any service + model family
7. Service-level default (e.g., `$modelAutoSettings["Anthropic", Automatic]`)
8. Global default (`$modelAutoSettings[Automatic, Automatic]`)

The first match wins. For details on how to add support for new models, see [TODO: How to Add Support for New Models].

### Global Auto Setting Defaults

These are the fallback values from `$modelAutoSettings[Automatic, Automatic]` when no model-specific override exists:

| Setting | Default |
| ------- | ------- |
| `"AppendCitations"` | `False` |
| `"ConvertSystemRoleToUser"` | `False` |
| `"EndToken"` | `"/end"` |
| `"ExcludedBasePrompts"` | `{ParentList}` |
| `"PresencePenalty"` | `0.1` |
| `"ReplaceUnicodeCharacters"` | `False` |
| `"ShowProgressText"` | `True` |
| `"SplitToolResponseMessages"` | `False` |
| `"Temperature"` | `0.7` |
| `"ToolResponseRole"` | `"System"` |

### Non-Inherited Persona Values

The following settings are not inherited from the persona configuration when resolving settings. They retain their value from the notebook/cell scope:

- `"ChatDrivenNotebook"`
- `"CurrentPreferencesTab"`
- `"EnableLLMServices"`
- `"Icon"`
- `"InheritanceTest"`
- `"InitialChatCell"`
- `"LLMEvaluator"`
- `"PersonaFavorites"`
- `"ServiceDefaultModel"`
